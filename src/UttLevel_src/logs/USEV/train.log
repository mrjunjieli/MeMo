started on logs/USEV

Namespace(mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', batch_size=6, max_length=6, num_workers=8, epochs=100, effec_batch_size=8, accu_grad=0, C=2, model_name='USEV', lr=0.001, max_norm=5, log_name='logs/USEV', use_tensorboard=1, continue_from='FALSE', local_rank=0, sample_rate=16000, win=512, hop_length=128, n_mels=80, shift_range='0,1', Self_enroll_amplitude_scaling='0.1,1', teacher_point=50, num_slots='5,5', loss_weight='28', distributed=True, world_size=2, speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800)

Total number of parameters: 15304390 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
)
Start new training
step:1/1667 avg loss:40.004
step:1/1667 avg loss:33.292
step:1001/1667 avg loss:0.415
step:1001/1667 avg loss:0.425
Train Summary | End of Epoch 1 | Time 1285.43s | Current time 2025-01-10 18:44:01.043613 |Train Loss 0.157| 
step:1/417 avg loss:-0.763
step:1/417 avg loss:-0.428
step:301/417 avg loss:-0.323
step:301/417 avg loss:-0.359
Valid Summary | End of Epoch 1 | Time 270.98s | Current time 2025-01-10 18:48:32.031128 |Valid Loss -0.349| 
Found new best model, dict saved
step:1/1667 avg loss:-0.706
step:1/1667 avg loss:-0.304
step:1001/1667 avg loss:-0.591
step:1001/1667 avg loss:-0.576
Train Summary | End of Epoch 2 | Time 1267.12s | Current time 2025-01-10 19:09:39.433900 |Train Loss -0.707| 
step:1/417 avg loss:-0.931
step:1/417 avg loss:0.309
step:301/417 avg loss:-0.951
step:301/417 avg loss:-0.956
Valid Summary | End of Epoch 2 | Time 284.92s | Current time 2025-01-10 19:14:24.449026 |Valid Loss -0.966| 
Found new best model, dict saved
step:1/1667 avg loss:-0.814
step:1/1667 avg loss:-2.046
step:1001/1667 avg loss:-1.244
step:1001/1667 avg loss:-1.186
Train Summary | End of Epoch 3 | Time 1367.34s | Current time 2025-01-10 19:37:12.031899 |Train Loss -1.329| 
step:1/417 avg loss:-2.292
step:1/417 avg loss:-1.157
step:301/417 avg loss:-1.461
step:301/417 avg loss:-1.512
Valid Summary | End of Epoch 3 | Time 270.95s | Current time 2025-01-10 19:41:43.042028 |Valid Loss -1.511| 
Found new best model, dict saved
step:1/1667 avg loss:-0.350
step:1/1667 avg loss:-1.194
step:1001/1667 avg loss:-1.862
step:1001/1667 avg loss:-1.831
Train Summary | End of Epoch 4 | Time 1307.43s | Current time 2025-01-10 20:03:30.819202 |Train Loss -2.041| 
step:1/417 avg loss:-3.440
step:1/417 avg loss:-1.660
step:301/417 avg loss:-2.492
step:301/417 avg loss:-2.530
Valid Summary | End of Epoch 4 | Time 315.70s | Current time 2025-01-10 20:08:46.554415 |Valid Loss -2.522| 
Found new best model, dict saved
step:1/1667 avg loss:-2.148
step:1/1667 avg loss:-2.457
step:1001/1667 avg loss:-2.715
step:1001/1667 avg loss:-2.714
Train Summary | End of Epoch 5 | Time 1504.90s | Current time 2025-01-10 20:33:51.856967 |Train Loss -2.839| 
step:1/417 avg loss:-3.403
step:1/417 avg loss:-2.065
step:301/417 avg loss:-3.020
step:301/417 avg loss:-3.050
Valid Summary | End of Epoch 5 | Time 287.19s | Current time 2025-01-10 20:38:39.094484 |Valid Loss -3.064| 
Found new best model, dict saved
step:1/1667 avg loss:-2.600
step:1/1667 avg loss:-4.394
step:1001/1667 avg loss:-3.368
step:1001/1667 avg loss:-3.325
Train Summary | End of Epoch 6 | Time 1371.11s | Current time 2025-01-10 21:01:30.449370 |Train Loss -3.499| 
step:1/417 avg loss:-2.531
step:1/417 avg loss:-5.114
step:301/417 avg loss:-3.867
step:301/417 avg loss:-3.974
Valid Summary | End of Epoch 6 | Time 329.04s | Current time 2025-01-10 21:06:59.504350 |Valid Loss -3.962| 
Found new best model, dict saved
step:1/1667 avg loss:-4.257
step:1/1667 avg loss:-3.850
step:1001/1667 avg loss:-4.030
step:1001/1667 avg loss:-3.974
Train Summary | End of Epoch 7 | Time 1627.93s | Current time 2025-01-10 21:34:07.725038 |Train Loss -4.118| 
step:1/417 avg loss:-5.534
step:1/417 avg loss:-3.217
step:301/417 avg loss:-4.206
step:301/417 avg loss:-4.328
Valid Summary | End of Epoch 7 | Time 299.04s | Current time 2025-01-10 21:39:06.764381 |Valid Loss -4.309| 
Found new best model, dict saved
step:1/1667 avg loss:-3.834
step:1/1667 avg loss:-4.072
step:1001/1667 avg loss:-4.487
step:1001/1667 avg loss:-4.483
Train Summary | End of Epoch 8 | Time 1419.98s | Current time 2025-01-10 22:02:47.028109 |Train Loss -4.614| 
step:1/417 avg loss:-2.774
step:1/417 avg loss:-5.814
step:301/417 avg loss:-4.427
step:301/417 avg loss:-4.387
Valid Summary | End of Epoch 8 | Time 357.90s | Current time 2025-01-10 22:08:44.932393 |Valid Loss -4.450| 
Found new best model, dict saved
step:1/1667 avg loss:-5.711
step:1/1667 avg loss:-4.392
step:1001/1667 avg loss:-4.962
step:1001/1667 avg loss:-4.931
Train Summary | End of Epoch 9 | Time 1639.12s | Current time 2025-01-10 22:36:05.522701 |Train Loss -5.005| 
step:1/417 avg loss:-3.375
step:1/417 avg loss:-6.320
step:301/417 avg loss:-4.964
step:301/417 avg loss:-5.018
Valid Summary | End of Epoch 9 | Time 309.76s | Current time 2025-01-10 22:41:15.320196 |Valid Loss -5.036| 
Found new best model, dict saved
step:1/1667 avg loss:-4.813
step:1/1667 avg loss:-5.260
step:1001/1667 avg loss:-5.326
step:1001/1667 avg loss:-5.282
Train Summary | End of Epoch 10 | Time 1460.80s | Current time 2025-01-10 23:05:36.363388 |Train Loss -5.331| 
step:1/417 avg loss:-6.681
step:1/417 avg loss:-3.703
step:301/417 avg loss:-5.049
step:301/417 avg loss:-5.000
Valid Summary | End of Epoch 10 | Time 344.88s | Current time 2025-01-10 23:11:21.280810 |Valid Loss -5.061| 
Found new best model, dict saved
step:1/1667 avg loss:-4.188
step:1/1667 avg loss:-5.989
step:1001/1667 avg loss:-5.651
step:1001/1667 avg loss:-5.615
Train Summary | End of Epoch 11 | Time 1643.59s | Current time 2025-01-10 23:38:45.520829 |Train Loss -5.649| 
step:1/417 avg loss:-6.878
step:1/417 avg loss:-3.432
step:301/417 avg loss:-5.569
step:301/417 avg loss:-5.605
Valid Summary | End of Epoch 11 | Time 317.97s | Current time 2025-01-10 23:44:03.502076 |Valid Loss -5.622| 
Found new best model, dict saved
step:1/1667 avg loss:-6.090
step:1/1667 avg loss:-7.595
step:1001/1667 avg loss:-5.916step:1001/1667 avg loss:-5.958

Train Summary | End of Epoch 12 | Time 1505.35s | Current time 2025-01-11 00:09:09.314024 |Train Loss -5.918| 
step:1/417 avg loss:-6.870
step:1/417 avg loss:-3.896
step:301/417 avg loss:-5.827
step:301/417 avg loss:-5.867
Valid Summary | End of Epoch 12 | Time 357.81s | Current time 2025-01-11 00:15:07.129766 |Valid Loss -5.884| 
Found new best model, dict saved
step:1/1667 avg loss:-4.843
step:1/1667 avg loss:-5.946
step:1001/1667 avg loss:-6.176step:1001/1667 avg loss:-6.181

Train Summary | End of Epoch 13 | Time 1649.35s | Current time 2025-01-11 00:42:37.578742 |Train Loss -6.232| 
step:1/417 avg loss:-7.333
step:1/417 avg loss:-4.436
step:301/417 avg loss:-6.081
step:301/417 avg loss:-6.065
Valid Summary | End of Epoch 13 | Time 326.26s | Current time 2025-01-11 00:48:03.839258 |Valid Loss -6.105| 
Found new best model, dict saved
step:1/1667 avg loss:-7.397
step:1/1667 avg loss:-6.576
step:1001/1667 avg loss:-6.511
step:1001/1667 avg loss:-6.498
Train Summary | End of Epoch 14 | Time 1541.02s | Current time 2025-01-11 01:13:45.126027 |Train Loss -6.534| 
step:1/417 avg loss:-6.577
step:1/417 avg loss:-4.667
step:301/417 avg loss:-6.207
step:301/417 avg loss:-6.169
Valid Summary | End of Epoch 14 | Time 344.67s | Current time 2025-01-11 01:19:29.801765 |Valid Loss -6.234| 
Found new best model, dict saved
step:1/1667 avg loss:-8.042
step:1/1667 avg loss:-6.774
step:1001/1667 avg loss:-6.802
step:1001/1667 avg loss:-6.752
Train Summary | End of Epoch 15 | Time 1625.94s | Current time 2025-01-11 01:46:35.990775 |Train Loss -6.773| 
step:1/417 avg loss:-4.972
step:1/417 avg loss:-7.100
step:301/417 avg loss:-6.234
step:301/417 avg loss:-6.338
Valid Summary | End of Epoch 15 | Time 333.67s | Current time 2025-01-11 01:52:09.659493 |Valid Loss -6.331| 
Found new best model, dict saved
step:1/1667 avg loss:-7.447
step:1/1667 avg loss:-6.640
step:1001/1667 avg loss:-6.987step:1001/1667 avg loss:-7.010

Train Summary | End of Epoch 16 | Time 1562.65s | Current time 2025-01-11 02:18:12.556329 |Train Loss -7.004| 
step:1/417 avg loss:-7.341
step:1/417 avg loss:-5.804
step:301/417 avg loss:-6.625
step:301/417 avg loss:-6.605
Valid Summary | End of Epoch 16 | Time 363.19s | Current time 2025-01-11 02:24:15.944390 |Valid Loss -6.665| 
Found new best model, dict saved
step:1/1667 avg loss:-6.862
step:1/1667 avg loss:-8.112
step:1001/1667 avg loss:-7.104step:1001/1667 avg loss:-7.104

Train Summary | End of Epoch 17 | Time 1671.18s | Current time 2025-01-11 02:52:07.439534 |Train Loss -7.104| 
step:1/417 avg loss:-5.091
step:1/417 avg loss:-7.607
step:301/417 avg loss:-6.462
step:301/417 avg loss:-6.495
Valid Summary | End of Epoch 17 | Time 342.53s | Current time 2025-01-11 02:57:49.976290 |Valid Loss -6.502| 
step:1/1667 avg loss:-6.060
step:1/1667 avg loss:-6.329
step:1001/1667 avg loss:-6.956step:1001/1667 avg loss:-6.902

Train Summary | End of Epoch 18 | Time 1613.55s | Current time 2025-01-11 03:24:43.659154 |Train Loss -7.045| 
step:1/417 avg loss:-7.849
step:1/417 avg loss:-5.542
step:301/417 avg loss:-6.797
step:301/417 avg loss:-6.797
Valid Summary | End of Epoch 18 | Time 365.40s | Current time 2025-01-11 03:30:49.066531 |Valid Loss -6.839| 
Found new best model, dict saved
step:1/1667 avg loss:-8.711
step:1/1667 avg loss:-9.790
step:1001/1667 avg loss:-7.520
step:1001/1667 avg loss:-7.489
Train Summary | End of Epoch 19 | Time 1688.63s | Current time 2025-01-11 03:58:57.957690 |Train Loss -7.504| 
step:1/417 avg loss:-8.185
step:1/417 avg loss:-5.626
step:301/417 avg loss:-7.106
step:301/417 avg loss:-7.170
Valid Summary | End of Epoch 19 | Time 343.38s | Current time 2025-01-11 04:04:41.344556 |Valid Loss -7.157| 
Found new best model, dict saved
step:1/1667 avg loss:-7.876
step:1/1667 avg loss:-7.030
step:1001/1667 avg loss:-7.542
step:1001/1667 avg loss:-7.498
Train Summary | End of Epoch 20 | Time 1608.67s | Current time 2025-01-11 04:31:30.276821 |Train Loss -7.539| 
step:1/417 avg loss:-8.115
step:1/417 avg loss:-3.430
step:301/417 avg loss:-7.279
step:301/417 avg loss:-7.180
Valid Summary | End of Epoch 20 | Time 365.21s | Current time 2025-01-11 04:37:35.489749 |Valid Loss -7.271| 
Found new best model, dict saved
step:1/1667 avg loss:-7.414
step:1/1667 avg loss:-6.615
step:1001/1667 avg loss:-7.528
step:1001/1667 avg loss:-7.477
Train Summary | End of Epoch 21 | Time 1695.39s | Current time 2025-01-11 05:05:51.426815 |Train Loss -7.516| 
step:1/417 avg loss:-7.281
step:1/417 avg loss:-5.333
step:301/417 avg loss:-6.950
step:301/417 avg loss:-7.033
Valid Summary | End of Epoch 21 | Time 345.32s | Current time 2025-01-11 05:11:36.765574 |Valid Loss -7.022| 
step:1/1667 avg loss:-7.228
step:1/1667 avg loss:-8.417
step:1001/1667 avg loss:-7.811
step:1001/1667 avg loss:-7.809
Train Summary | End of Epoch 22 | Time 1628.65s | Current time 2025-01-11 05:38:45.599492 |Train Loss -7.804| 
step:1/417 avg loss:-8.519
step:1/417 avg loss:-5.751
step:301/417 avg loss:-7.290
step:301/417 avg loss:-7.183
Valid Summary | End of Epoch 22 | Time 367.39s | Current time 2025-01-11 05:44:53.052284 |Valid Loss -7.291| 
Found new best model, dict saved
step:1/1667 avg loss:-5.555
step:1/1667 avg loss:-9.317
step:1001/1667 avg loss:-7.791
step:1001/1667 avg loss:-7.759
Train Summary | End of Epoch 23 | Time 1710.86s | Current time 2025-01-11 06:13:24.160222 |Train Loss -7.674| 
step:1/417 avg loss:-5.678
step:1/417 avg loss:-8.430
step:301/417 avg loss:-6.979
step:301/417 avg loss:-7.166
Valid Summary | End of Epoch 23 | Time 348.41s | Current time 2025-01-11 06:19:12.570855 |Valid Loss -7.140| 
step:1/1667 avg loss:-9.636
step:1/1667 avg loss:-7.929
step:1001/1667 avg loss:-7.905
step:1001/1667 avg loss:-7.864
Train Summary | End of Epoch 24 | Time 1632.48s | Current time 2025-01-11 06:46:25.179107 |Train Loss -7.906| 
step:1/417 avg loss:-7.712
step:1/417 avg loss:-5.695
step:301/417 avg loss:-6.995
step:301/417 avg loss:-6.910
Valid Summary | End of Epoch 24 | Time 371.58s | Current time 2025-01-11 06:52:36.762303 |Valid Loss -6.979| 
step:1/1667 avg loss:-9.436
step:1/1667 avg loss:-8.533
step:1001/1667 avg loss:-7.952
step:1001/1667 avg loss:-7.978
Train Summary | End of Epoch 25 | Time 1706.68s | Current time 2025-01-11 07:21:03.610837 |Train Loss -8.035| 
step:1/417 avg loss:-8.608
step:1/417 avg loss:-6.318
step:301/417 avg loss:-7.439
step:301/417 avg loss:-7.558
Valid Summary | End of Epoch 25 | Time 350.48s | Current time 2025-01-11 07:26:54.094363 |Valid Loss -7.549| 
Found new best model, dict saved
step:1/1667 avg loss:-9.774
step:1/1667 avg loss:-6.698
step:1001/1667 avg loss:-8.147
step:1001/1667 avg loss:-8.127
Train Summary | End of Epoch 26 | Time 1641.63s | Current time 2025-01-11 07:54:16.161612 |Train Loss -8.117| 
step:1/417 avg loss:-8.394
step:1/417 avg loss:-5.708
step:301/417 avg loss:-7.273
step:301/417 avg loss:-7.226
Valid Summary | End of Epoch 26 | Time 370.59s | Current time 2025-01-11 08:00:26.788518 |Valid Loss -7.279| 
step:1/1667 avg loss:-8.281
step:1/1667 avg loss:-8.934
step:1001/1667 avg loss:-8.357
step:1001/1667 avg loss:-8.389
Train Summary | End of Epoch 27 | Time 1691.03s | Current time 2025-01-11 08:28:37.954419 |Train Loss -8.301| 
step:1/417 avg loss:-5.922
step:1/417 avg loss:-8.944
step:301/417 avg loss:-7.701
step:301/417 avg loss:-7.783
Valid Summary | End of Epoch 27 | Time 350.24s | Current time 2025-01-11 08:34:28.194888 |Valid Loss -7.787| 
Found new best model, dict saved
step:1/1667 avg loss:-6.351
step:1/1667 avg loss:-7.632
step:1001/1667 avg loss:-8.293
step:1001/1667 avg loss:-8.281
Train Summary | End of Epoch 28 | Time 1622.43s | Current time 2025-01-11 09:01:31.371582 |Train Loss -8.323| 
step:1/417 avg loss:-9.117
step:1/417 avg loss:-6.669
step:301/417 avg loss:-7.845
step:301/417 avg loss:-7.704
Valid Summary | End of Epoch 28 | Time 351.69s | Current time 2025-01-11 09:07:23.100754 |Valid Loss -7.829| 
Found new best model, dict saved
step:1/1667 avg loss:-7.252
step:1/1667 avg loss:-7.116
step:1001/1667 avg loss:-8.452
step:1001/1667 avg loss:-8.494
Train Summary | End of Epoch 29 | Time 1634.83s | Current time 2025-01-11 09:34:38.212960 |Train Loss -8.442| 
step:1/417 avg loss:-5.605
step:1/417 avg loss:-8.736
step:301/417 avg loss:-7.703
step:301/417 avg loss:-7.494
Valid Summary | End of Epoch 29 | Time 368.57s | Current time 2025-01-11 09:40:46.817132 |Valid Loss -7.643| 
step:1/1667 avg loss:-9.993
step:1/1667 avg loss:-9.155
step:1001/1667 avg loss:-8.534step:1001/1667 avg loss:-8.552

Train Summary | End of Epoch 30 | Time 1708.30s | Current time 2025-01-11 10:09:15.286769 |Train Loss -8.567| 
step:1/417 avg loss:-8.712
step:1/417 avg loss:-5.903
step:301/417 avg loss:-7.791
step:301/417 avg loss:-7.744
Valid Summary | End of Epoch 30 | Time 371.89s | Current time 2025-01-11 10:15:27.221580 |Valid Loss -7.793| 
step:1/1667 avg loss:-8.578
step:1/1667 avg loss:-7.980
step:1001/1667 avg loss:-8.516
step:1001/1667 avg loss:-8.507
Train Summary | End of Epoch 31 | Time 1693.59s | Current time 2025-01-11 10:43:41.092170 |Train Loss -8.535| 
step:1/417 avg loss:-5.752
step:1/417 avg loss:-9.167
step:301/417 avg loss:-8.014
step:301/417 avg loss:-8.135
Valid Summary | End of Epoch 31 | Time 355.73s | Current time 2025-01-11 10:49:36.858058 |Valid Loss -8.109| 
Found new best model, dict saved
step:1/1667 avg loss:-6.638
step:1/1667 avg loss:-9.451
step:1001/1667 avg loss:-8.703
step:1001/1667 avg loss:-8.598
Train Summary | End of Epoch 32 | Time 1653.63s | Current time 2025-01-11 11:17:10.764349 |Train Loss -8.575| 
step:1/417 avg loss:-9.015
step:1/417 avg loss:-7.161
step:301/417 avg loss:-7.771
step:301/417 avg loss:-7.510
Valid Summary | End of Epoch 32 | Time 358.65s | Current time 2025-01-11 11:23:09.454613 |Valid Loss -7.683| 
step:1/1667 avg loss:-8.287
step:1/1667 avg loss:-9.543
step:1001/1667 avg loss:-8.666
step:1001/1667 avg loss:-8.565
Train Summary | End of Epoch 33 | Time 1651.71s | Current time 2025-01-11 11:50:41.301320 |Train Loss -8.724| 
step:1/417 avg loss:-9.320
step:1/417 avg loss:-6.257
step:301/417 avg loss:-8.189
step:301/417 avg loss:-8.322
Valid Summary | End of Epoch 33 | Time 356.86s | Current time 2025-01-11 11:56:38.163423 |Valid Loss -8.293| 
Found new best model, dict saved
step:1/1667 avg loss:-9.009
step:1/1667 avg loss:-8.752
step:1001/1667 avg loss:-8.857step:1001/1667 avg loss:-8.844

Train Summary | End of Epoch 34 | Time 1672.79s | Current time 2025-01-11 12:24:31.249885 |Train Loss -8.791| 
step:1/417 avg loss:-9.217
step:1/417 avg loss:-6.124
step:301/417 avg loss:-8.100
step:301/417 avg loss:-7.953
Valid Summary | End of Epoch 34 | Time 351.14s | Current time 2025-01-11 12:30:22.429758 |Valid Loss -8.074| 
step:1/1667 avg loss:-7.748
step:1/1667 avg loss:-7.792
step:1001/1667 avg loss:-8.787
step:1001/1667 avg loss:-8.731
Train Summary | End of Epoch 35 | Time 1619.64s | Current time 2025-01-11 12:57:22.262673 |Train Loss -8.804| 
step:1/417 avg loss:-6.108
step:1/417 avg loss:-9.218
step:301/417 avg loss:-7.977
step:301/417 avg loss:-8.021
Valid Summary | End of Epoch 35 | Time 359.06s | Current time 2025-01-11 13:03:21.364149 |Valid Loss -8.040| 
step:1/1667 avg loss:-8.973
step:1/1667 avg loss:-9.919
step:1001/1667 avg loss:-8.921
step:1001/1667 avg loss:-8.916
Train Summary | End of Epoch 36 | Time 1636.18s | Current time 2025-01-11 13:30:37.761773 |Train Loss -8.878| 
step:1/417 avg loss:-6.326
step:1/417 avg loss:-9.239
step:301/417 avg loss:-8.065
step:301/417 avg loss:-8.063
Valid Summary | End of Epoch 36 | Time 358.78s | Current time 2025-01-11 13:36:36.582066 |Valid Loss -8.110| 
step:1/1667 avg loss:-10.658
step:1/1667 avg loss:-6.644
step:1001/1667 avg loss:-8.885
step:1001/1667 avg loss:-8.896
Train Summary | End of Epoch 37 | Time 1662.66s | Current time 2025-01-11 14:04:19.369694 |Train Loss -8.791| 
step:1/417 avg loss:-5.768
step:1/417 avg loss:-9.053
step:301/417 avg loss:-7.407
step:301/417 avg loss:-7.492
Valid Summary | End of Epoch 37 | Time 359.04s | Current time 2025-01-11 14:10:18.453655 |Valid Loss -7.532| 
step:1/1667 avg loss:-8.878
step:1/1667 avg loss:-7.624
step:1001/1667 avg loss:-8.916
step:1001/1667 avg loss:-8.908
Train Summary | End of Epoch 38 | Time 1662.53s | Current time 2025-01-11 14:38:01.130544 |Train Loss -8.928| 
step:1/417 avg loss:-9.354
step:1/417 avg loss:-6.354
step:301/417 avg loss:-8.217
step:301/417 avg loss:-8.061
Valid Summary | End of Epoch 38 | Time 357.27s | Current time 2025-01-11 14:43:58.445501 |Valid Loss -8.193| 
step:1/1667 avg loss:-8.978
step:1/1667 avg loss:-8.952
step:1001/1667 avg loss:-8.935
step:1001/1667 avg loss:-8.886
Train Summary | End of Epoch 39 | Time 1658.80s | Current time 2025-01-11 15:11:37.429983 |Train Loss -8.985| 
step:1/417 avg loss:-9.076
step:1/417 avg loss:-4.662
step:301/417 avg loss:-8.170
step:301/417 avg loss:-8.044
Valid Summary | End of Epoch 39 | Time 369.29s | Current time 2025-01-11 15:17:46.764932 |Valid Loss -8.162| 
Learning rate adjusted to: 0.000500
step:1/1667 avg loss:-6.727
step:1/1667 avg loss:-8.213
step:1001/1667 avg loss:-9.480
step:1001/1667 avg loss:-9.448
Train Summary | End of Epoch 40 | Time 1682.55s | Current time 2025-01-11 15:45:49.448066 |Train Loss -9.516| 
step:1/417 avg loss:-9.881
step:1/417 avg loss:-6.599
step:301/417 avg loss:-8.707
step:301/417 avg loss:-8.556
Valid Summary | End of Epoch 40 | Time 353.33s | Current time 2025-01-11 15:51:42.813803 |Valid Loss -8.664| 
Found new best model, dict saved
step:1/1667 avg loss:-11.064
step:1/1667 avg loss:-11.282
step:1001/1667 avg loss:-9.754
step:1001/1667 avg loss:-9.727
Train Summary | End of Epoch 41 | Time 1635.81s | Current time 2025-01-11 16:18:59.185962 |Train Loss -9.724| 
step:1/417 avg loss:-9.798
step:1/417 avg loss:-6.207
step:301/417 avg loss:-8.791
step:301/417 avg loss:-8.700
Valid Summary | End of Epoch 41 | Time 365.43s | Current time 2025-01-11 16:25:04.639376 |Valid Loss -8.803| 
Found new best model, dict saved
step:1/1667 avg loss:-9.285
step:1/1667 avg loss:-10.471
step:1001/1667 avg loss:-9.829step:1001/1667 avg loss:-9.890

Train Summary | End of Epoch 42 | Time 1680.41s | Current time 2025-01-11 16:53:05.398032 |Train Loss -9.795| 
step:1/417 avg loss:-9.823
step:1/417 avg loss:-6.170
step:301/417 avg loss:-8.512
step:301/417 avg loss:-8.383
Valid Summary | End of Epoch 42 | Time 338.05s | Current time 2025-01-11 16:58:43.491830 |Valid Loss -8.523| 
step:1/1667 avg loss:-7.557
step:1/1667 avg loss:-8.527
step:1001/1667 avg loss:-9.929
step:1001/1667 avg loss:-9.901
Train Summary | End of Epoch 43 | Time 1598.51s | Current time 2025-01-11 17:25:22.160085 |Train Loss -9.901| 
step:1/417 avg loss:-10.123
step:1/417 avg loss:-6.331
step:301/417 avg loss:-8.916
step:301/417 avg loss:-8.775
Valid Summary | End of Epoch 43 | Time 363.61s | Current time 2025-01-11 17:31:25.816727 |Valid Loss -8.898| 
Found new best model, dict saved
step:1/1667 avg loss:-8.747
step:1/1667 avg loss:-9.230
step:1001/1667 avg loss:-9.945step:1001/1667 avg loss:-9.935

Train Summary | End of Epoch 44 | Time 1660.88s | Current time 2025-01-11 17:59:06.996113 |Train Loss -9.974| 
step:1/417 avg loss:-10.274
step:1/417 avg loss:-3.244
step:301/417 avg loss:-8.760
step:301/417 avg loss:-8.616
Valid Summary | End of Epoch 44 | Time 339.97s | Current time 2025-01-11 18:04:46.970226 |Valid Loss -8.742| 
step:1/1667 avg loss:-8.503
step:1/1667 avg loss:-9.366
step:1001/1667 avg loss:-10.066
step:1001/1667 avg loss:-10.053
Train Summary | End of Epoch 45 | Time 1569.07s | Current time 2025-01-11 18:30:56.206483 |Train Loss -9.967| 
step:1/417 avg loss:-6.610
step:1/417 avg loss:-10.197
step:301/417 avg loss:-8.689
step:301/417 avg loss:-8.878
Valid Summary | End of Epoch 45 | Time 341.92s | Current time 2025-01-11 18:36:38.170196 |Valid Loss -8.849| 
step:1/1667 avg loss:-8.838
step:1/1667 avg loss:-11.405
step:1001/1667 avg loss:-10.107
step:1001/1667 avg loss:-10.120
Train Summary | End of Epoch 46 | Time 1588.30s | Current time 2025-01-11 19:03:06.637038 |Train Loss -10.106| 
step:1/417 avg loss:-10.207
step:1/417 avg loss:-6.319
step:301/417 avg loss:-8.932
step:301/417 avg loss:-8.673
Valid Summary | End of Epoch 46 | Time 335.60s | Current time 2025-01-11 19:08:42.236130 |Valid Loss -8.863| 
step:1/1667 avg loss:-10.574
step:1/1667 avg loss:-9.527
step:1001/1667 avg loss:-10.152
step:1001/1667 avg loss:-10.107
Train Summary | End of Epoch 47 | Time 1569.64s | Current time 2025-01-11 19:34:52.031925 |Train Loss -10.151| 
step:1/417 avg loss:-10.284
step:1/417 avg loss:-6.303
step:301/417 avg loss:-8.793
step:301/417 avg loss:-8.716
Valid Summary | End of Epoch 47 | Time 366.18s | Current time 2025-01-11 19:40:58.252747 |Valid Loss -8.833| 
step:1/1667 avg loss:-10.546
step:1/1667 avg loss:-9.855
step:1001/1667 avg loss:-10.206
step:1001/1667 avg loss:-10.165
Train Summary | End of Epoch 48 | Time 1739.31s | Current time 2025-01-11 20:09:57.690792 |Train Loss -10.126| 
step:1/417 avg loss:-6.298
step:1/417 avg loss:-10.236
step:301/417 avg loss:-8.948
step:301/417 avg loss:-8.769
Valid Summary | End of Epoch 48 | Time 357.40s | Current time 2025-01-11 20:15:55.095111 |Valid Loss -8.910| 
Found new best model, dict saved
step:1/1667 avg loss:-9.882
step:1/1667 avg loss:-10.697
step:1001/1667 avg loss:-10.122
step:1001/1667 avg loss:-10.165
Train Summary | End of Epoch 49 | Time 1581.37s | Current time 2025-01-11 20:42:16.746543 |Train Loss -10.126| 
step:1/417 avg loss:-6.497
step:1/417 avg loss:-10.272
step:301/417 avg loss:-8.966
step:301/417 avg loss:-9.006
Valid Summary | End of Epoch 49 | Time 378.70s | Current time 2025-01-11 20:48:35.459257 |Valid Loss -9.026| 
Found new best model, dict saved
step:1/1667 avg loss:-10.682
step:1/1667 avg loss:-10.900
step:1001/1667 avg loss:-10.264
step:1001/1667 avg loss:-10.239
Train Summary | End of Epoch 50 | Time 1721.18s | Current time 2025-01-11 21:17:16.934274 |Train Loss -10.212| 
step:1/417 avg loss:-10.254
step:1/417 avg loss:-6.742
step:301/417 avg loss:-9.168
step:301/417 avg loss:-9.043
Valid Summary | End of Epoch 50 | Time 353.72s | Current time 2025-01-11 21:23:10.694368 |Valid Loss -9.141| 
Found new best model, dict saved
step:1/1667 avg loss:-9.971
step:1/1667 avg loss:-10.976
step:1001/1667 avg loss:-10.254
step:1001/1667 avg loss:-10.298
Train Summary | End of Epoch 51 | Time 1537.08s | Current time 2025-01-11 21:48:48.179142 |Train Loss -10.276| 
step:1/417 avg loss:-6.793
step:1/417 avg loss:-10.410
step:301/417 avg loss:-8.936
step:301/417 avg loss:-9.169
Valid Summary | End of Epoch 51 | Time 368.30s | Current time 2025-01-11 21:54:56.487921 |Valid Loss -9.103| 
step:1/1667 avg loss:-12.003
step:1/1667 avg loss:-10.689
step:1001/1667 avg loss:-10.369
step:1001/1667 avg loss:-10.314
Train Summary | End of Epoch 52 | Time 1677.47s | Current time 2025-01-11 22:22:54.088452 |Train Loss -10.303| 
step:1/417 avg loss:-10.368
step:1/417 avg loss:-6.694
step:301/417 avg loss:-9.098
step:301/417 avg loss:-8.967
Valid Summary | End of Epoch 52 | Time 337.94s | Current time 2025-01-11 22:28:32.029504 |Valid Loss -9.066| 
step:1/1667 avg loss:-10.000
step:1/1667 avg loss:-10.552
step:1001/1667 avg loss:-10.382
step:1001/1667 avg loss:-10.391
Train Summary | End of Epoch 53 | Time 1498.35s | Current time 2025-01-11 22:53:30.544327 |Train Loss -10.345| 
step:1/417 avg loss:-10.336
step:1/417 avg loss:-6.884
step:301/417 avg loss:-9.128
step:301/417 avg loss:-9.058
Valid Summary | End of Epoch 53 | Time 363.86s | Current time 2025-01-11 22:59:34.408078 |Valid Loss -9.150| 
Found new best model, dict saved
step:1/1667 avg loss:-10.211
step:1/1667 avg loss:-10.245
step:1001/1667 avg loss:-10.332
step:1001/1667 avg loss:-10.321
Train Summary | End of Epoch 54 | Time 1661.95s | Current time 2025-01-11 23:27:16.653016 |Train Loss -10.347| 
step:1/417 avg loss:-10.454
step:1/417 avg loss:-6.828
step:301/417 avg loss:-9.245
step:301/417 avg loss:-9.101
Valid Summary | End of Epoch 54 | Time 315.55s | Current time 2025-01-11 23:32:32.204277 |Valid Loss -9.213| 
Found new best model, dict saved
step:1/1667 avg loss:-9.358
step:1/1667 avg loss:-11.145
step:1001/1667 avg loss:-10.451
step:1001/1667 avg loss:-10.450
Train Summary | End of Epoch 55 | Time 1496.15s | Current time 2025-01-11 23:57:28.589631 |Train Loss -10.394| 
step:1/417 avg loss:-6.852
step:1/417 avg loss:-10.419
step:301/417 avg loss:-9.262
step:301/417 avg loss:-9.069
Valid Summary | End of Epoch 55 | Time 357.24s | Current time 2025-01-12 00:03:25.877858 |Valid Loss -9.200| 
step:1/1667 avg loss:-10.146
step:1/1667 avg loss:-11.336
step:1001/1667 avg loss:-10.484
step:1001/1667 avg loss:-10.461
Train Summary | End of Epoch 56 | Time 1642.17s | Current time 2025-01-12 00:30:48.308557 |Train Loss -10.490| 
step:1/417 avg loss:-10.623
step:1/417 avg loss:-6.937
step:301/417 avg loss:-9.350
step:301/417 avg loss:-9.199
Valid Summary | End of Epoch 56 | Time 317.68s | Current time 2025-01-12 00:36:06.033516 |Valid Loss -9.303| 
Found new best model, dict saved
step:1/1667 avg loss:-10.333
step:1/1667 avg loss:-9.944
step:1001/1667 avg loss:-10.543
step:1001/1667 avg loss:-10.531
Train Summary | End of Epoch 57 | Time 1479.48s | Current time 2025-01-12 01:00:45.828754 |Train Loss -10.498| 
step:1/417 avg loss:-10.536
step:1/417 avg loss:-6.592
step:301/417 avg loss:-9.305
step:301/417 avg loss:-9.091
Valid Summary | End of Epoch 57 | Time 351.31s | Current time 2025-01-12 01:06:37.175999 |Valid Loss -9.243| 
step:1/1667 avg loss:-11.043
step:1/1667 avg loss:-10.421
step:1001/1667 avg loss:-10.516
step:1001/1667 avg loss:-10.531
Train Summary | End of Epoch 58 | Time 1611.05s | Current time 2025-01-12 01:33:28.388072 |Train Loss -10.477| 
step:1/417 avg loss:-10.577
step:1/417 avg loss:-6.978
step:301/417 avg loss:-9.302
step:301/417 avg loss:-9.194
Valid Summary | End of Epoch 58 | Time 314.71s | Current time 2025-01-12 01:38:43.113282 |Valid Loss -9.282| 
step:1/1667 avg loss:-12.519
step:1/1667 avg loss:-10.406
step:1001/1667 avg loss:-10.574
step:1001/1667 avg loss:-10.551
Train Summary | End of Epoch 59 | Time 1479.14s | Current time 2025-01-12 02:03:22.416227 |Train Loss -10.616| 
step:1/417 avg loss:-10.690
step:1/417 avg loss:-7.103
step:301/417 avg loss:-9.435
step:301/417 avg loss:-9.325
Valid Summary | End of Epoch 59 | Time 331.85s | Current time 2025-01-12 02:08:54.308845 |Valid Loss -9.436| 
Found new best model, dict saved
step:1/1667 avg loss:-11.194
step:1/1667 avg loss:-10.955
step:1001/1667 avg loss:-10.482
step:1001/1667 avg loss:-10.475
Train Summary | End of Epoch 60 | Time 1586.24s | Current time 2025-01-12 02:35:20.782344 |Train Loss -10.560| 
step:1/417 avg loss:-10.641
step:1/417 avg loss:-6.696
step:301/417 avg loss:-9.497
step:301/417 avg loss:-9.306
Valid Summary | End of Epoch 60 | Time 308.79s | Current time 2025-01-12 02:40:29.613844 |Valid Loss -9.418| 
step:1/1667 avg loss:-11.412
step:1/1667 avg loss:-10.701
step:1001/1667 avg loss:-10.731
step:1001/1667 avg loss:-10.705
