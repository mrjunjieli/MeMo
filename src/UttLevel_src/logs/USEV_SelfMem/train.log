started on logs/USEV_SelfMem

Namespace(mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', batch_size=6, max_length=6, num_workers=8, epochs=100, effec_batch_size=8, accu_grad=0, C=2, model_name='USEV_SelfMem', lr=0.001, max_norm=5, log_name='logs/USEV_SelfMem', use_tensorboard=1, continue_from='FALSE', local_rank=0, sample_rate=16000, win=512, hop_length=128, n_mels=80, shift_range='0,1', Self_enroll_amplitude_scaling='0.1,1', teacher_point=50, num_slots='5,5', loss_weight='28', distributed=True, world_size=2, speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800)

Total number of parameters: 16421830 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Start new training
step:1/1667 avg loss:28.761, stage_1_loss:28.503, stage_2_loss:28.825
step:1/1667 avg loss:28.945, stage_1_loss:28.586, stage_2_loss:29.034
step:1001/1667 avg loss:-0.334, stage_1_loss:0.073, stage_2_loss:-0.436step:1001/1667 avg loss:-0.329, stage_1_loss:0.074, stage_2_loss:-0.429

Train Summary | End of Epoch 1 | Time 1753.44s | Current time 2025-01-10 16:13:10.449637 |Train Loss -0.954| 
step:1/417 avg loss:-0.444, stage_1_loss:-0.409, stage_2_loss:-0.453
step:1/417 avg loss:-0.005, stage_1_loss:-0.297, stage_2_loss:0.068
step:301/417 avg loss:0.073, stage_1_loss:-0.176, stage_2_loss:0.135
step:301/417 avg loss:0.031, stage_1_loss:-0.192, stage_2_loss:0.087
Valid Summary | End of Epoch 1 | Time 291.99s | Current time 2025-01-10 16:18:02.542754 |Valid Loss 0.055| 
Found new best model, dict saved
step:1/1667 avg loss:-3.058, stage_1_loss:0.332, stage_2_loss:-3.906
step:1/1667 avg loss:-2.733, stage_1_loss:-0.192, stage_2_loss:-3.368
step:1001/1667 avg loss:-3.110, stage_1_loss:-0.644, stage_2_loss:-3.727step:1001/1667 avg loss:-3.099, stage_1_loss:-0.656, stage_2_loss:-3.710

Train Summary | End of Epoch 2 | Time 1723.73s | Current time 2025-01-10 16:46:46.864009 |Train Loss -3.487| 
step:1/417 avg loss:-1.651, stage_1_loss:-1.896, stage_2_loss:-1.590
step:1/417 avg loss:-1.073, stage_1_loss:-1.252, stage_2_loss:-1.029
step:301/417 avg loss:-1.672, stage_1_loss:-1.869, stage_2_loss:-1.623
step:301/417 avg loss:-1.704, stage_1_loss:-1.905, stage_2_loss:-1.654
Valid Summary | End of Epoch 2 | Time 287.54s | Current time 2025-01-10 16:51:34.553777 |Valid Loss -1.700| 
Found new best model, dict saved
step:1/1667 avg loss:-3.107, stage_1_loss:-1.112, stage_2_loss:-3.606
step:1/1667 avg loss:-4.293, stage_1_loss:-2.688, stage_2_loss:-4.695
step:1001/1667 avg loss:-4.877, stage_1_loss:-1.704, stage_2_loss:-5.671step:1001/1667 avg loss:-4.860, stage_1_loss:-1.695, stage_2_loss:-5.651

Train Summary | End of Epoch 3 | Time 1729.63s | Current time 2025-01-10 17:20:24.651583 |Train Loss -5.089| 
step:1/417 avg loss:-4.115, stage_1_loss:-4.035, stage_2_loss:-4.135
step:1/417 avg loss:-2.001, stage_1_loss:-2.191, stage_2_loss:-1.954
step:301/417 avg loss:-2.901, stage_1_loss:-2.986, stage_2_loss:-2.880
step:301/417 avg loss:-2.813, stage_1_loss:-2.933, stage_2_loss:-2.783
Valid Summary | End of Epoch 3 | Time 287.77s | Current time 2025-01-10 17:25:12.445634 |Valid Loss -2.874| 
Found new best model, dict saved
step:1/1667 avg loss:-6.014, stage_1_loss:-1.966, stage_2_loss:-7.026
step:1/1667 avg loss:-5.935, stage_1_loss:-1.406, stage_2_loss:-7.067
step:1001/1667 avg loss:-6.021, stage_1_loss:-2.884, stage_2_loss:-6.805step:1001/1667 avg loss:-6.008, stage_1_loss:-2.831, stage_2_loss:-6.803

Train Summary | End of Epoch 4 | Time 1727.25s | Current time 2025-01-10 17:54:00.214273 |Train Loss -6.197| 
step:1/417 avg loss:-3.648, stage_1_loss:-4.193, stage_2_loss:-3.512
step:1/417 avg loss:-2.075, stage_1_loss:-2.741, stage_2_loss:-1.908
step:301/417 avg loss:-2.331, stage_1_loss:-2.845, stage_2_loss:-2.203
step:301/417 avg loss:-2.423, stage_1_loss:-2.914, stage_2_loss:-2.300
Valid Summary | End of Epoch 4 | Time 287.30s | Current time 2025-01-10 17:58:47.747543 |Valid Loss -2.403| 
step:1/1667 avg loss:-6.508, stage_1_loss:-1.248, stage_2_loss:-7.823
step:1/1667 avg loss:-5.396, stage_1_loss:-2.106, stage_2_loss:-6.218
step:1001/1667 avg loss:-6.897, stage_1_loss:-3.942, stage_2_loss:-7.635
step:1001/1667 avg loss:-6.892, stage_1_loss:-3.968, stage_2_loss:-7.623
Train Summary | End of Epoch 5 | Time 1730.58s | Current time 2025-01-10 18:27:38.561102 |Train Loss -7.043| 
step:1/417 avg loss:-5.961, stage_1_loss:-5.838, stage_2_loss:-5.991
step:1/417 avg loss:-2.948, stage_1_loss:-3.598, stage_2_loss:-2.786
step:301/417 avg loss:-4.994, stage_1_loss:-4.984, stage_2_loss:-4.997
step:301/417 avg loss:-5.029, stage_1_loss:-5.034, stage_2_loss:-5.028
Valid Summary | End of Epoch 5 | Time 288.03s | Current time 2025-01-10 18:32:26.595997 |Valid Loss -5.020| 
Found new best model, dict saved
step:1/1667 avg loss:-7.338, stage_1_loss:-4.367, stage_2_loss:-8.081
step:1/1667 avg loss:-8.745, stage_1_loss:-6.823, stage_2_loss:-9.226
step:1001/1667 avg loss:-7.587, stage_1_loss:-4.859, stage_2_loss:-8.269
step:1001/1667 avg loss:-7.556, stage_1_loss:-4.775, stage_2_loss:-8.251
Train Summary | End of Epoch 6 | Time 1724.91s | Current time 2025-01-10 19:01:12.041456 |Train Loss -7.672| 
step:1/417 avg loss:-6.240, stage_1_loss:-6.339, stage_2_loss:-6.216
step:1/417 avg loss:-3.559, stage_1_loss:-4.122, stage_2_loss:-3.418
step:301/417 avg loss:-5.568, stage_1_loss:-5.654, stage_2_loss:-5.547
step:301/417 avg loss:-5.541, stage_1_loss:-5.666, stage_2_loss:-5.510
Valid Summary | End of Epoch 6 | Time 287.52s | Current time 2025-01-10 19:05:59.584167 |Valid Loss -5.552| 
Found new best model, dict saved
step:1/1667 avg loss:-8.319, stage_1_loss:-5.971, stage_2_loss:-8.906
step:1/1667 avg loss:-6.747, stage_1_loss:-4.926, stage_2_loss:-7.202
step:1001/1667 avg loss:-8.166, stage_1_loss:-5.523, stage_2_loss:-8.827
step:1001/1667 avg loss:-8.146, stage_1_loss:-5.531, stage_2_loss:-8.800
Train Summary | End of Epoch 7 | Time 1729.61s | Current time 2025-01-10 19:34:49.833079 |Train Loss -8.194| 
step:1/417 avg loss:-7.829, stage_1_loss:-7.228, stage_2_loss:-7.980
step:1/417 avg loss:-4.186, stage_1_loss:-4.565, stage_2_loss:-4.091
step:301/417 avg loss:-6.100, stage_1_loss:-5.927, stage_2_loss:-6.143
step:301/417 avg loss:-6.095, stage_1_loss:-5.938, stage_2_loss:-6.134
Valid Summary | End of Epoch 7 | Time 287.03s | Current time 2025-01-10 19:39:36.868548 |Valid Loss -6.118| 
Found new best model, dict saved
step:1/1667 avg loss:-8.059, stage_1_loss:-6.384, stage_2_loss:-8.478
step:1/1667 avg loss:-8.013, stage_1_loss:-5.836, stage_2_loss:-8.557
step:1001/1667 avg loss:-8.069, stage_1_loss:-5.757, stage_2_loss:-8.647
step:1001/1667 avg loss:-8.063, stage_1_loss:-5.743, stage_2_loss:-8.643
Train Summary | End of Epoch 8 | Time 1725.66s | Current time 2025-01-10 20:08:23.225012 |Train Loss -8.350| 
step:1/417 avg loss:-7.068, stage_1_loss:-7.067, stage_2_loss:-7.068
step:1/417 avg loss:-4.165, stage_1_loss:-4.408, stage_2_loss:-4.104
step:301/417 avg loss:-6.763, stage_1_loss:-6.697, stage_2_loss:-6.780
step:301/417 avg loss:-6.710, stage_1_loss:-6.664, stage_2_loss:-6.721
Valid Summary | End of Epoch 8 | Time 288.41s | Current time 2025-01-10 20:13:11.638915 |Valid Loss -6.736| 
Found new best model, dict saved
step:1/1667 avg loss:-9.772, stage_1_loss:-7.850, stage_2_loss:-10.253
step:1/1667 avg loss:-8.907, stage_1_loss:-6.892, stage_2_loss:-9.410
step:1001/1667 avg loss:-9.086, stage_1_loss:-6.654, stage_2_loss:-9.693
step:1001/1667 avg loss:-9.043, stage_1_loss:-6.601, stage_2_loss:-9.653
Train Summary | End of Epoch 9 | Time 1728.52s | Current time 2025-01-10 20:42:00.566134 |Train Loss -8.948| 
step:1/417 avg loss:-8.093, stage_1_loss:-8.061, stage_2_loss:-8.101
step:1/417 avg loss:-7.069, stage_1_loss:-7.232, stage_2_loss:-7.028
step:301/417 avg loss:-7.244, stage_1_loss:-7.500, stage_2_loss:-7.180
step:301/417 avg loss:-7.224, stage_1_loss:-7.486, stage_2_loss:-7.159
Valid Summary | End of Epoch 9 | Time 287.51s | Current time 2025-01-10 20:46:48.152522 |Valid Loss -7.272| 
Found new best model, dict saved
step:1/1667 avg loss:-9.544, stage_1_loss:-5.366, stage_2_loss:-10.588
step:1/1667 avg loss:-9.648, stage_1_loss:-7.107, stage_2_loss:-10.283
step:1001/1667 avg loss:-9.438, stage_1_loss:-7.270, stage_2_loss:-9.979
step:1001/1667 avg loss:-9.403, stage_1_loss:-7.172, stage_2_loss:-9.960
Train Summary | End of Epoch 10 | Time 1730.13s | Current time 2025-01-10 21:15:38.777063 |Train Loss -9.442| 
step:1/417 avg loss:-8.337, stage_1_loss:-8.464, stage_2_loss:-8.305
step:1/417 avg loss:-5.481, stage_1_loss:-6.113, stage_2_loss:-5.324
step:301/417 avg loss:-8.045, stage_1_loss:-8.071, stage_2_loss:-8.039
step:301/417 avg loss:-8.033, stage_1_loss:-8.062, stage_2_loss:-8.026
Valid Summary | End of Epoch 10 | Time 289.00s | Current time 2025-01-10 21:20:27.817635 |Valid Loss -8.081| 
Found new best model, dict saved
step:1/1667 avg loss:-10.006, stage_1_loss:-8.044, stage_2_loss:-10.496
step:1/1667 avg loss:-10.178, stage_1_loss:-5.204, stage_2_loss:-11.422
step:1001/1667 avg loss:-9.707, stage_1_loss:-7.612, stage_2_loss:-10.231
step:1001/1667 avg loss:-9.708, stage_1_loss:-7.630, stage_2_loss:-10.228
Train Summary | End of Epoch 11 | Time 1733.83s | Current time 2025-01-10 21:49:22.459915 |Train Loss -9.696| 
step:1/417 avg loss:-8.610, stage_1_loss:-8.667, stage_2_loss:-8.596
step:1/417 avg loss:-6.192, stage_1_loss:-6.647, stage_2_loss:-6.078
step:301/417 avg loss:-8.071, stage_1_loss:-8.221, stage_2_loss:-8.033
step:301/417 avg loss:-7.997, stage_1_loss:-8.163, stage_2_loss:-7.956
Valid Summary | End of Epoch 11 | Time 290.95s | Current time 2025-01-10 21:54:13.423420 |Valid Loss -8.078| 
step:1/1667 avg loss:-11.489, stage_1_loss:-10.188, stage_2_loss:-11.814
step:1/1667 avg loss:-10.506, stage_1_loss:-8.288, stage_2_loss:-11.060
step:1001/1667 avg loss:-9.502, stage_1_loss:-7.406, stage_2_loss:-10.026step:1001/1667 avg loss:-9.538, stage_1_loss:-7.517, stage_2_loss:-10.043

Train Summary | End of Epoch 12 | Time 1738.88s | Current time 2025-01-10 22:23:12.527815 |Train Loss -9.632| 
step:1/417 avg loss:-8.980, stage_1_loss:-8.856, stage_2_loss:-9.011
step:1/417 avg loss:-5.522, stage_1_loss:-6.078, stage_2_loss:-5.383
step:301/417 avg loss:-8.258, stage_1_loss:-8.401, stage_2_loss:-8.222
step:301/417 avg loss:-8.189, stage_1_loss:-8.351, stage_2_loss:-8.148
Valid Summary | End of Epoch 12 | Time 290.56s | Current time 2025-01-10 22:28:03.125293 |Valid Loss -8.267| 
Found new best model, dict saved
step:1/1667 avg loss:-8.784, stage_1_loss:-6.091, stage_2_loss:-9.457
step:1/1667 avg loss:-9.692, stage_1_loss:-8.704, stage_2_loss:-9.939
step:1001/1667 avg loss:-10.053, stage_1_loss:-8.147, stage_2_loss:-10.530
step:1001/1667 avg loss:-10.020, stage_1_loss:-8.068, stage_2_loss:-10.508
Train Summary | End of Epoch 13 | Time 1738.49s | Current time 2025-01-10 22:57:02.040205 |Train Loss -10.005| 
step:1/417 avg loss:-8.012, stage_1_loss:-7.812, stage_2_loss:-8.062
step:1/417 avg loss:-4.927, stage_1_loss:-5.644, stage_2_loss:-4.747
step:301/417 avg loss:-7.688, stage_1_loss:-7.733, stage_2_loss:-7.676
step:301/417 avg loss:-7.637, stage_1_loss:-7.696, stage_2_loss:-7.622
Valid Summary | End of Epoch 13 | Time 290.96s | Current time 2025-01-10 23:01:53.035829 |Valid Loss -7.676| 
step:1/1667 avg loss:-8.797, stage_1_loss:-4.636, stage_2_loss:-9.838
step:1/1667 avg loss:-8.778, stage_1_loss:-6.787, stage_2_loss:-9.276
step:1001/1667 avg loss:-10.041, stage_1_loss:-8.194, stage_2_loss:-10.502step:1001/1667 avg loss:-10.061, stage_1_loss:-8.206, stage_2_loss:-10.524

Train Summary | End of Epoch 14 | Time 1733.96s | Current time 2025-01-10 23:30:47.141100 |Train Loss -10.096| 
step:1/417 avg loss:-9.068, stage_1_loss:-9.141, stage_2_loss:-9.050
step:1/417 avg loss:-5.799, stage_1_loss:-6.276, stage_2_loss:-5.680
step:301/417 avg loss:-8.580, stage_1_loss:-8.696, stage_2_loss:-8.551
step:301/417 avg loss:-8.464, stage_1_loss:-8.622, stage_2_loss:-8.425
Valid Summary | End of Epoch 14 | Time 293.42s | Current time 2025-01-10 23:35:40.585449 |Valid Loss -8.534| 
Found new best model, dict saved
step:1/1667 avg loss:-9.807, stage_1_loss:-8.864, stage_2_loss:-10.042
step:1/1667 avg loss:-9.513, stage_1_loss:-8.262, stage_2_loss:-9.825
step:1001/1667 avg loss:-10.167, stage_1_loss:-8.461, stage_2_loss:-10.593
step:1001/1667 avg loss:-10.140, stage_1_loss:-8.417, stage_2_loss:-10.571
Train Summary | End of Epoch 15 | Time 1735.30s | Current time 2025-01-11 00:04:36.732455 |Train Loss -10.160| 
step:1/417 avg loss:-8.192, stage_1_loss:-8.317, stage_2_loss:-8.161
step:1/417 avg loss:-5.235, stage_1_loss:-5.813, stage_2_loss:-5.091
step:301/417 avg loss:-8.049, stage_1_loss:-8.098, stage_2_loss:-8.036
step:301/417 avg loss:-7.980, stage_1_loss:-8.023, stage_2_loss:-7.969
Valid Summary | End of Epoch 15 | Time 289.18s | Current time 2025-01-11 00:09:25.937163 |Valid Loss -8.042| 
step:1/1667 avg loss:-9.693, stage_1_loss:-8.758, stage_2_loss:-9.927
step:1/1667 avg loss:-9.382, stage_1_loss:-7.027, stage_2_loss:-9.971
step:1001/1667 avg loss:-9.804, stage_1_loss:-8.385, stage_2_loss:-10.159step:1001/1667 avg loss:-9.834, stage_1_loss:-8.408, stage_2_loss:-10.191

Train Summary | End of Epoch 16 | Time 1736.72s | Current time 2025-01-11 00:38:22.828412 |Train Loss -9.992| 
step:1/417 avg loss:-8.188, stage_1_loss:-8.681, stage_2_loss:-8.065
step:1/417 avg loss:-6.875, stage_1_loss:-7.333, stage_2_loss:-6.761
step:301/417 avg loss:-8.676, stage_1_loss:-8.818, stage_2_loss:-8.640
step:301/417 avg loss:-8.571, stage_1_loss:-8.716, stage_2_loss:-8.534
Valid Summary | End of Epoch 16 | Time 291.08s | Current time 2025-01-11 00:43:13.925155 |Valid Loss -8.619| 
Found new best model, dict saved
step:1/1667 avg loss:-9.442, stage_1_loss:-8.878, stage_2_loss:-9.584
step:1/1667 avg loss:-10.534, stage_1_loss:-9.564, stage_2_loss:-10.776
step:1001/1667 avg loss:-9.855, stage_1_loss:-8.404, stage_2_loss:-10.218
step:1001/1667 avg loss:-9.859, stage_1_loss:-8.412, stage_2_loss:-10.220
Train Summary | End of Epoch 17 | Time 1761.27s | Current time 2025-01-11 01:12:36.245129 |Train Loss -9.989| 
step:1/417 avg loss:-8.666, stage_1_loss:-8.754, stage_2_loss:-8.644
step:1/417 avg loss:-5.965, stage_1_loss:-6.709, stage_2_loss:-5.779
step:301/417 avg loss:-8.556, stage_1_loss:-8.644, stage_2_loss:-8.534
step:301/417 avg loss:-8.437, stage_1_loss:-8.543, stage_2_loss:-8.410
Valid Summary | End of Epoch 17 | Time 290.42s | Current time 2025-01-11 01:17:26.717407 |Valid Loss -8.531| 
step:1/1667 avg loss:-8.862, stage_1_loss:-7.949, stage_2_loss:-9.090
step:1/1667 avg loss:-9.541, stage_1_loss:-8.632, stage_2_loss:-9.769
step:1001/1667 avg loss:-9.869, stage_1_loss:-8.669, stage_2_loss:-10.169step:1001/1667 avg loss:-9.871, stage_1_loss:-8.664, stage_2_loss:-10.173

Train Summary | End of Epoch 18 | Time 1738.64s | Current time 2025-01-11 01:46:26.180499 |Train Loss -9.736| 
step:1/417 avg loss:-9.337, stage_1_loss:-9.576, stage_2_loss:-9.277
step:1/417 avg loss:-6.030, stage_1_loss:-6.936, stage_2_loss:-5.803
step:301/417 avg loss:-8.869, stage_1_loss:-9.087, stage_2_loss:-8.814
step:301/417 avg loss:-8.826, stage_1_loss:-9.036, stage_2_loss:-8.774
Valid Summary | End of Epoch 18 | Time 298.36s | Current time 2025-01-11 01:51:24.539075 |Valid Loss -8.856| 
Found new best model, dict saved
step:1/1667 avg loss:-11.040, stage_1_loss:-9.730, stage_2_loss:-11.367
step:1/1667 avg loss:-11.580, stage_1_loss:-10.393, stage_2_loss:-11.876
step:1001/1667 avg loss:-9.692, stage_1_loss:-8.760, stage_2_loss:-9.925step:1001/1667 avg loss:-9.675, stage_1_loss:-8.680, stage_2_loss:-9.923

Train Summary | End of Epoch 19 | Time 1734.86s | Current time 2025-01-11 02:20:19.864301 |Train Loss -9.558| 
step:1/417 avg loss:-9.067, stage_1_loss:-9.065, stage_2_loss:-9.068
step:1/417 avg loss:-5.085, stage_1_loss:-6.411, stage_2_loss:-4.754
step:301/417 avg loss:-8.474, stage_1_loss:-8.648, stage_2_loss:-8.431
step:301/417 avg loss:-8.356, stage_1_loss:-8.534, stage_2_loss:-8.312
Valid Summary | End of Epoch 19 | Time 288.47s | Current time 2025-01-11 02:25:08.335823 |Valid Loss -8.457| 
step:1/1667 avg loss:-9.508, stage_1_loss:-8.649, stage_2_loss:-9.723
step:1/1667 avg loss:-7.642, stage_1_loss:-7.814, stage_2_loss:-7.599
step:1001/1667 avg loss:-9.327, stage_1_loss:-8.672, stage_2_loss:-9.491step:1001/1667 avg loss:-9.308, stage_1_loss:-8.675, stage_2_loss:-9.466

Train Summary | End of Epoch 20 | Time 1739.60s | Current time 2025-01-11 02:54:08.116600 |Train Loss -9.367| 
step:1/417 avg loss:-7.334, stage_1_loss:-8.393, stage_2_loss:-7.070
step:1/417 avg loss:-6.611, stage_1_loss:-6.962, stage_2_loss:-6.523
step:301/417 avg loss:-8.004, stage_1_loss:-8.706, stage_2_loss:-7.828
step:301/417 avg loss:-8.031, stage_1_loss:-8.699, stage_2_loss:-7.864
Valid Summary | End of Epoch 20 | Time 288.99s | Current time 2025-01-11 02:58:57.164907 |Valid Loss -8.047| 
step:1/1667 avg loss:-8.993, stage_1_loss:-8.764, stage_2_loss:-9.050
step:1/1667 avg loss:-7.410, stage_1_loss:-7.322, stage_2_loss:-7.432
step:1001/1667 avg loss:-8.956, stage_1_loss:-8.551, stage_2_loss:-9.057
step:1001/1667 avg loss:-8.894, stage_1_loss:-8.505, stage_2_loss:-8.991
Train Summary | End of Epoch 21 | Time 1725.83s | Current time 2025-01-11 03:27:44.210770 |Train Loss -9.074| 
step:1/417 avg loss:-6.119, stage_1_loss:-8.488, stage_2_loss:-5.527
step:1/417 avg loss:-3.674, stage_1_loss:-5.817, stage_2_loss:-3.138
step:301/417 avg loss:-6.323, stage_1_loss:-8.283, stage_2_loss:-5.833
step:301/417 avg loss:-6.359, stage_1_loss:-8.193, stage_2_loss:-5.900
Valid Summary | End of Epoch 21 | Time 289.33s | Current time 2025-01-11 03:32:33.578660 |Valid Loss -6.403| 
step:1/1667 avg loss:-8.578, stage_1_loss:-7.958, stage_2_loss:-8.733
step:1/1667 avg loss:-9.792, stage_1_loss:-9.393, stage_2_loss:-9.891
step:1001/1667 avg loss:-9.218, stage_1_loss:-8.736, stage_2_loss:-9.338
step:1001/1667 avg loss:-9.234, stage_1_loss:-8.755, stage_2_loss:-9.354
Train Summary | End of Epoch 22 | Time 1731.96s | Current time 2025-01-11 04:01:26.049852 |Train Loss -9.117| 
step:1/417 avg loss:-2.996, stage_1_loss:-9.199, stage_2_loss:-1.445
step:1/417 avg loss:-3.115, stage_1_loss:-6.053, stage_2_loss:-2.380
step:301/417 avg loss:-5.191, stage_1_loss:-8.719, stage_2_loss:-4.309
step:301/417 avg loss:-5.364, stage_1_loss:-8.661, stage_2_loss:-4.540
Valid Summary | End of Epoch 22 | Time 287.47s | Current time 2025-01-11 04:06:13.556196 |Valid Loss -5.312| 
step:1/1667 avg loss:-8.702, stage_1_loss:-6.375, stage_2_loss:-9.284
step:1/1667 avg loss:-10.114, stage_1_loss:-10.210, stage_2_loss:-10.089
step:1001/1667 avg loss:-9.068, stage_1_loss:-8.667, stage_2_loss:-9.168step:1001/1667 avg loss:-9.069, stage_1_loss:-8.733, stage_2_loss:-9.153

Train Summary | End of Epoch 23 | Time 1728.84s | Current time 2025-01-11 04:35:02.700329 |Train Loss -9.126| 
step:1/417 avg loss:-1.815, stage_1_loss:-8.942, stage_2_loss:-0.033
step:1/417 avg loss:-0.298, stage_1_loss:-6.675, stage_2_loss:1.297
step:301/417 avg loss:-2.535, stage_1_loss:-8.627, stage_2_loss:-1.012
step:301/417 avg loss:-2.820, stage_1_loss:-8.640, stage_2_loss:-1.365
Valid Summary | End of Epoch 23 | Time 291.77s | Current time 2025-01-11 04:39:54.474495 |Valid Loss -2.770| 
step:1/1667 avg loss:-10.432, stage_1_loss:-9.551, stage_2_loss:-10.652
step:1/1667 avg loss:-8.886, stage_1_loss:-8.402, stage_2_loss:-9.008
step:1001/1667 avg loss:-9.312, stage_1_loss:-8.933, stage_2_loss:-9.407step:1001/1667 avg loss:-9.238, stage_1_loss:-8.868, stage_2_loss:-9.331

Train Summary | End of Epoch 24 | Time 1729.59s | Current time 2025-01-11 05:08:44.252914 |Train Loss -9.232| 
step:1/417 avg loss:-1.716, stage_1_loss:-9.462, stage_2_loss:0.221
step:1/417 avg loss:0.642, stage_1_loss:-6.447, stage_2_loss:2.414
step:301/417 avg loss:-2.741, stage_1_loss:-9.282, stage_2_loss:-1.106
step:301/417 avg loss:-3.038, stage_1_loss:-9.121, stage_2_loss:-1.517
Valid Summary | End of Epoch 24 | Time 287.70s | Current time 2025-01-11 05:13:31.951577 |Valid Loss -2.982| 
Learning rate adjusted to: 0.000500
step:1/1667 avg loss:-11.210, stage_1_loss:-11.377, stage_2_loss:-11.168
step:1/1667 avg loss:-10.120, stage_1_loss:-10.162, stage_2_loss:-10.109
step:1001/1667 avg loss:-9.830, stage_1_loss:-9.530, stage_2_loss:-9.905
step:1001/1667 avg loss:-9.861, stage_1_loss:-9.537, stage_2_loss:-9.942
Train Summary | End of Epoch 25 | Time 1723.07s | Current time 2025-01-11 05:42:15.201252 |Train Loss -9.896| 
step:1/417 avg loss:-3.747, stage_1_loss:-9.766, stage_2_loss:-2.243
step:1/417 avg loss:-0.780, stage_1_loss:-7.354, stage_2_loss:0.863
step:301/417 avg loss:-5.364, stage_1_loss:-9.620, stage_2_loss:-4.300
step:301/417 avg loss:-5.680, stage_1_loss:-9.555, stage_2_loss:-4.712
Valid Summary | End of Epoch 25 | Time 289.11s | Current time 2025-01-11 05:47:04.315453 |Valid Loss -5.590| 
step:1/1667 avg loss:-11.477, stage_1_loss:-11.376, stage_2_loss:-11.502
step:1/1667 avg loss:-8.242, stage_1_loss:-8.006, stage_2_loss:-8.301
step:1001/1667 avg loss:-9.937, stage_1_loss:-9.605, stage_2_loss:-10.020
step:1001/1667 avg loss:-9.886, stage_1_loss:-9.589, stage_2_loss:-9.960
Train Summary | End of Epoch 26 | Time 1727.51s | Current time 2025-01-11 06:15:51.978384 |Train Loss -10.010| 
step:1/417 avg loss:-6.872, stage_1_loss:-9.807, stage_2_loss:-6.138
step:1/417 avg loss:-2.036, stage_1_loss:-6.399, stage_2_loss:-0.945
step:301/417 avg loss:-6.191, stage_1_loss:-9.673, stage_2_loss:-5.321
step:301/417 avg loss:-6.354, stage_1_loss:-9.514, stage_2_loss:-5.564
Valid Summary | End of Epoch 26 | Time 289.12s | Current time 2025-01-11 06:20:41.104429 |Valid Loss -6.301| 
step:1/1667 avg loss:-10.217, stage_1_loss:-10.233, stage_2_loss:-10.213
step:1/1667 avg loss:-10.572, stage_1_loss:-10.314, stage_2_loss:-10.637
step:1001/1667 avg loss:-10.287, stage_1_loss:-10.000, stage_2_loss:-10.359step:1001/1667 avg loss:-10.284, stage_1_loss:-10.008, stage_2_loss:-10.353

Train Summary | End of Epoch 27 | Time 1728.35s | Current time 2025-01-11 06:49:29.606821 |Train Loss -10.279| 
step:1/417 avg loss:-9.315, stage_1_loss:-9.709, stage_2_loss:-9.217
step:1/417 avg loss:-5.537, stage_1_loss:-7.218, stage_2_loss:-5.117
step:301/417 avg loss:-7.845, stage_1_loss:-9.839, stage_2_loss:-7.346
step:301/417 avg loss:-7.871, stage_1_loss:-9.728, stage_2_loss:-7.406
Valid Summary | End of Epoch 27 | Time 287.42s | Current time 2025-01-11 06:54:17.067116 |Valid Loss -7.928| 
step:1/1667 avg loss:-8.837, stage_1_loss:-8.680, stage_2_loss:-8.876
step:1/1667 avg loss:-8.957, stage_1_loss:-8.767, stage_2_loss:-9.005
step:1001/1667 avg loss:-10.404, stage_1_loss:-10.130, stage_2_loss:-10.473step:1001/1667 avg loss:-10.327, stage_1_loss:-10.047, stage_2_loss:-10.397

Train Summary | End of Epoch 28 | Time 1727.54s | Current time 2025-01-11 07:23:04.844356 |Train Loss -10.267| 
step:1/417 avg loss:-10.136, stage_1_loss:-10.204, stage_2_loss:-10.119
step:1/417 avg loss:-6.740, stage_1_loss:-6.675, stage_2_loss:-6.757
step:301/417 avg loss:-9.318, stage_1_loss:-9.861, stage_2_loss:-9.182
step:301/417 avg loss:-9.207, stage_1_loss:-9.766, stage_2_loss:-9.067
Valid Summary | End of Epoch 28 | Time 287.22s | Current time 2025-01-11 07:27:52.104389 |Valid Loss -9.297| 
Found new best model, dict saved
step:1/1667 avg loss:-9.668, stage_1_loss:-9.461, stage_2_loss:-9.719
step:1/1667 avg loss:-9.367, stage_1_loss:-9.169, stage_2_loss:-9.417
step:1001/1667 avg loss:-10.335, stage_1_loss:-10.103, stage_2_loss:-10.392step:1001/1667 avg loss:-10.428, stage_1_loss:-10.176, stage_2_loss:-10.491

Train Summary | End of Epoch 29 | Time 1731.78s | Current time 2025-01-11 07:56:44.431561 |Train Loss -10.366| 
step:1/417 avg loss:-9.408, stage_1_loss:-9.756, stage_2_loss:-9.321
step:1/417 avg loss:-6.861, stage_1_loss:-7.094, stage_2_loss:-6.802
step:301/417 avg loss:-9.658, stage_1_loss:-9.933, stage_2_loss:-9.589
step:301/417 avg loss:-9.570, stage_1_loss:-9.825, stage_2_loss:-9.506
Valid Summary | End of Epoch 29 | Time 287.98s | Current time 2025-01-11 08:01:32.460379 |Valid Loss -9.642| 
Found new best model, dict saved
step:1/1667 avg loss:-12.210, stage_1_loss:-12.153, stage_2_loss:-12.225
step:1/1667 avg loss:-11.410, stage_1_loss:-11.186, stage_2_loss:-11.466
step:1001/1667 avg loss:-10.376, stage_1_loss:-10.150, stage_2_loss:-10.433
step:1001/1667 avg loss:-10.401, stage_1_loss:-10.174, stage_2_loss:-10.458
Train Summary | End of Epoch 30 | Time 1721.78s | Current time 2025-01-11 08:30:14.685607 |Train Loss -10.395| 
step:1/417 avg loss:-10.264, stage_1_loss:-10.423, stage_2_loss:-10.224
step:1/417 avg loss:-8.107, stage_1_loss:-7.687, stage_2_loss:-8.212
step:301/417 avg loss:-9.418, stage_1_loss:-9.970, stage_2_loss:-9.280
step:301/417 avg loss:-9.371, stage_1_loss:-9.902, stage_2_loss:-9.239
Valid Summary | End of Epoch 30 | Time 290.77s | Current time 2025-01-11 08:35:05.530382 |Valid Loss -9.434| 
step:1/1667 avg loss:-10.460, stage_1_loss:-10.148, stage_2_loss:-10.538
step:1/1667 avg loss:-9.916, stage_1_loss:-9.570, stage_2_loss:-10.003
step:1001/1667 avg loss:-10.493, stage_1_loss:-10.302, stage_2_loss:-10.541
step:1001/1667 avg loss:-10.498, stage_1_loss:-10.281, stage_2_loss:-10.552
Train Summary | End of Epoch 31 | Time 1728.30s | Current time 2025-01-11 09:03:54.265895 |Train Loss -10.517| 
step:1/417 avg loss:-10.436, stage_1_loss:-10.009, stage_2_loss:-10.543
step:1/417 avg loss:-8.169, stage_1_loss:-7.229, stage_2_loss:-8.404
step:301/417 avg loss:-9.925, stage_1_loss:-10.023, stage_2_loss:-9.901
step:301/417 avg loss:-9.790, stage_1_loss:-9.907, stage_2_loss:-9.761
Valid Summary | End of Epoch 31 | Time 288.88s | Current time 2025-01-11 09:08:43.196198 |Valid Loss -9.893| 
Found new best model, dict saved
step:1/1667 avg loss:-10.751, stage_1_loss:-10.620, stage_2_loss:-10.784
step:1/1667 avg loss:-9.036, stage_1_loss:-8.485, stage_2_loss:-9.174
step:1001/1667 avg loss:-10.584, stage_1_loss:-10.401, stage_2_loss:-10.630step:1001/1667 avg loss:-10.580, stage_1_loss:-10.375, stage_2_loss:-10.631

Train Summary | End of Epoch 32 | Time 1729.04s | Current time 2025-01-11 09:37:32.655240 |Train Loss -10.601| 
step:1/417 avg loss:-10.538, stage_1_loss:-10.278, stage_2_loss:-10.602
step:1/417 avg loss:-7.915, stage_1_loss:-7.463, stage_2_loss:-8.028
step:301/417 avg loss:-9.999, stage_1_loss:-10.112, stage_2_loss:-9.970
step:301/417 avg loss:-9.880, stage_1_loss:-10.005, stage_2_loss:-9.848
Valid Summary | End of Epoch 32 | Time 289.76s | Current time 2025-01-11 09:42:22.451757 |Valid Loss -9.966| 
Found new best model, dict saved
step:1/1667 avg loss:-10.287, stage_1_loss:-10.036, stage_2_loss:-10.349
step:1/1667 avg loss:-11.449, stage_1_loss:-11.324, stage_2_loss:-11.480
step:1001/1667 avg loss:-10.573, stage_1_loss:-10.399, stage_2_loss:-10.616step:1001/1667 avg loss:-10.507, stage_1_loss:-10.329, stage_2_loss:-10.551

Train Summary | End of Epoch 33 | Time 1727.72s | Current time 2025-01-11 10:11:10.782347 |Train Loss -10.565| 
step:1/417 avg loss:-10.404, stage_1_loss:-10.153, stage_2_loss:-10.467
step:1/417 avg loss:-6.190, stage_1_loss:-6.878, stage_2_loss:-6.019
step:301/417 avg loss:-10.069, stage_1_loss:-10.087, stage_2_loss:-10.064
step:301/417 avg loss:-10.019, stage_1_loss:-10.014, stage_2_loss:-10.021
Valid Summary | End of Epoch 33 | Time 288.64s | Current time 2025-01-11 10:15:59.465450 |Valid Loss -10.099| 
Found new best model, dict saved
step:1/1667 avg loss:-10.727, stage_1_loss:-10.644, stage_2_loss:-10.747
step:1/1667 avg loss:-7.552, stage_1_loss:-7.386, stage_2_loss:-7.594
step:1001/1667 avg loss:-10.679, stage_1_loss:-10.538, stage_2_loss:-10.715
step:1001/1667 avg loss:-10.658, stage_1_loss:-10.510, stage_2_loss:-10.695
Train Summary | End of Epoch 34 | Time 1728.06s | Current time 2025-01-11 10:44:48.028555 |Train Loss -10.620| 
step:1/417 avg loss:-10.804, stage_1_loss:-10.664, stage_2_loss:-10.839
step:1/417 avg loss:-6.559, stage_1_loss:-7.057, stage_2_loss:-6.434
step:301/417 avg loss:-10.076, stage_1_loss:-10.132, stage_2_loss:-10.062
step:301/417 avg loss:-9.956, stage_1_loss:-9.996, stage_2_loss:-9.946
Valid Summary | End of Epoch 34 | Time 287.73s | Current time 2025-01-11 10:49:35.760109 |Valid Loss -10.035| 
step:1/1667 avg loss:-9.259, stage_1_loss:-9.177, stage_2_loss:-9.280
step:1/1667 avg loss:-9.511, stage_1_loss:-9.355, stage_2_loss:-9.550
step:1001/1667 avg loss:-10.677, stage_1_loss:-10.522, stage_2_loss:-10.715step:1001/1667 avg loss:-10.659, stage_1_loss:-10.528, stage_2_loss:-10.691

Train Summary | End of Epoch 35 | Time 1730.60s | Current time 2025-01-11 11:18:26.577674 |Train Loss -10.689| 
step:1/417 avg loss:-10.334, stage_1_loss:-10.099, stage_2_loss:-10.393
step:1/417 avg loss:-6.815, stage_1_loss:-6.616, stage_2_loss:-6.864
step:301/417 avg loss:-9.895, stage_1_loss:-10.022, stage_2_loss:-9.863
step:301/417 avg loss:-9.671, stage_1_loss:-9.767, stage_2_loss:-9.647
Valid Summary | End of Epoch 35 | Time 291.73s | Current time 2025-01-11 11:23:18.310657 |Valid Loss -9.813| 
step:1/1667 avg loss:-10.191, stage_1_loss:-10.131, stage_2_loss:-10.206
step:1/1667 avg loss:-11.121, stage_1_loss:-11.471, stage_2_loss:-11.033
step:1001/1667 avg loss:-10.534, stage_1_loss:-10.449, stage_2_loss:-10.556step:1001/1667 avg loss:-10.449, stage_1_loss:-10.390, stage_2_loss:-10.463

Train Summary | End of Epoch 36 | Time 1727.65s | Current time 2025-01-11 11:52:06.138736 |Train Loss -10.477| 
step:1/417 avg loss:-10.569, stage_1_loss:-10.502, stage_2_loss:-10.586
step:1/417 avg loss:-7.375, stage_1_loss:-7.682, stage_2_loss:-7.298
step:301/417 avg loss:-10.254, stage_1_loss:-10.222, stage_2_loss:-10.262
step:301/417 avg loss:-10.160, stage_1_loss:-10.107, stage_2_loss:-10.173
Valid Summary | End of Epoch 36 | Time 290.09s | Current time 2025-01-11 11:56:56.282060 |Valid Loss -10.242| 
Found new best model, dict saved
step:1/1667 avg loss:-12.264, stage_1_loss:-12.198, stage_2_loss:-12.280
step:1/1667 avg loss:-7.977, stage_1_loss:-7.931, stage_2_loss:-7.989
step:1001/1667 avg loss:-10.681, stage_1_loss:-10.621, stage_2_loss:-10.696
step:1001/1667 avg loss:-10.684, stage_1_loss:-10.625, stage_2_loss:-10.699
Train Summary | End of Epoch 37 | Time 1728.48s | Current time 2025-01-11 12:25:45.210657 |Train Loss -10.591| 
step:1/417 avg loss:-10.734, stage_1_loss:-10.630, stage_2_loss:-10.759
step:1/417 avg loss:-7.196, stage_1_loss:-7.414, stage_2_loss:-7.142
step:301/417 avg loss:-10.089, stage_1_loss:-10.153, stage_2_loss:-10.074
step:301/417 avg loss:-9.966, stage_1_loss:-10.027, stage_2_loss:-9.951
Valid Summary | End of Epoch 37 | Time 288.63s | Current time 2025-01-11 12:30:33.846226 |Valid Loss -10.051| 
step:1/1667 avg loss:-11.365, stage_1_loss:-11.280, stage_2_loss:-11.386
step:1/1667 avg loss:-9.384, stage_1_loss:-9.308, stage_2_loss:-9.403
step:1001/1667 avg loss:-10.505, stage_1_loss:-10.456, stage_2_loss:-10.517
step:1001/1667 avg loss:-10.500, stage_1_loss:-10.448, stage_2_loss:-10.513
Train Summary | End of Epoch 38 | Time 1728.00s | Current time 2025-01-11 12:59:22.056462 |Train Loss -10.588| 
step:1/417 avg loss:-11.048, stage_1_loss:-10.881, stage_2_loss:-11.089
step:1/417 avg loss:-7.744, stage_1_loss:-8.089, stage_2_loss:-7.658
step:301/417 avg loss:-10.274, stage_1_loss:-10.274, stage_2_loss:-10.274
step:301/417 avg loss:-10.181, stage_1_loss:-10.169, stage_2_loss:-10.183
Valid Summary | End of Epoch 38 | Time 287.93s | Current time 2025-01-11 13:04:10.025662 |Valid Loss -10.265| 
Found new best model, dict saved
step:1/1667 avg loss:-7.714, stage_1_loss:-7.755, stage_2_loss:-7.703
step:1/1667 avg loss:-11.043, stage_1_loss:-10.976, stage_2_loss:-11.060
step:1001/1667 avg loss:-10.820, stage_1_loss:-10.781, stage_2_loss:-10.830
step:1001/1667 avg loss:-10.802, stage_1_loss:-10.746, stage_2_loss:-10.816
Train Summary | End of Epoch 39 | Time 1728.18s | Current time 2025-01-11 13:32:58.638383 |Train Loss -10.759| 
step:1/417 avg loss:-10.694, stage_1_loss:-10.598, stage_2_loss:-10.718
step:1/417 avg loss:-7.448, stage_1_loss:-7.727, stage_2_loss:-7.378
step:301/417 avg loss:-10.240, stage_1_loss:-10.221, stage_2_loss:-10.245
step:301/417 avg loss:-10.106, stage_1_loss:-10.076, stage_2_loss:-10.113
Valid Summary | End of Epoch 39 | Time 288.90s | Current time 2025-01-11 13:37:47.542772 |Valid Loss -10.195| 
step:1/1667 avg loss:-8.716, stage_1_loss:-8.535, stage_2_loss:-8.761
step:1/1667 avg loss:-10.039, stage_1_loss:-10.059, stage_2_loss:-10.034
step:1001/1667 avg loss:-11.005, stage_1_loss:-10.929, stage_2_loss:-11.024
step:1001/1667 avg loss:-11.001, stage_1_loss:-10.915, stage_2_loss:-11.022
Train Summary | End of Epoch 40 | Time 1732.53s | Current time 2025-01-11 14:06:40.236671 |Train Loss -10.871| 
step:1/417 avg loss:-10.773, stage_1_loss:-10.237, stage_2_loss:-10.907
step:1/417 avg loss:-7.644, stage_1_loss:-7.912, stage_2_loss:-7.577
step:301/417 avg loss:-10.302, stage_1_loss:-10.283, stage_2_loss:-10.306
step:301/417 avg loss:-10.189, stage_1_loss:-10.143, stage_2_loss:-10.200
Valid Summary | End of Epoch 40 | Time 289.27s | Current time 2025-01-11 14:11:29.509950 |Valid Loss -10.270| 
Found new best model, dict saved
step:1/1667 avg loss:-12.215, stage_1_loss:-12.164, stage_2_loss:-12.227
step:1/1667 avg loss:-12.566, stage_1_loss:-12.537, stage_2_loss:-12.573
step:1001/1667 avg loss:-10.860, stage_1_loss:-10.792, stage_2_loss:-10.877
step:1001/1667 avg loss:-10.873, stage_1_loss:-10.793, stage_2_loss:-10.893
Train Summary | End of Epoch 41 | Time 1729.36s | Current time 2025-01-11 14:40:19.802677 |Train Loss -10.842| 
step:1/417 avg loss:-11.016, stage_1_loss:-10.887, stage_2_loss:-11.048
step:1/417 avg loss:-7.202, stage_1_loss:-7.187, stage_2_loss:-7.206
step:301/417 avg loss:-10.312, stage_1_loss:-10.322, stage_2_loss:-10.310
step:301/417 avg loss:-10.178, stage_1_loss:-10.166, stage_2_loss:-10.182
Valid Summary | End of Epoch 41 | Time 289.08s | Current time 2025-01-11 14:45:08.928680 |Valid Loss -10.271| 
Found new best model, dict saved
step:1/1667 avg loss:-10.369, stage_1_loss:-10.265, stage_2_loss:-10.395
step:1/1667 avg loss:-11.094, stage_1_loss:-10.902, stage_2_loss:-11.143
step:1001/1667 avg loss:-10.792, stage_1_loss:-10.747, stage_2_loss:-10.803step:1001/1667 avg loss:-10.797, stage_1_loss:-10.722, stage_2_loss:-10.815

Train Summary | End of Epoch 42 | Time 1724.37s | Current time 2025-01-11 15:13:53.714138 |Train Loss -10.754| 
step:1/417 avg loss:-11.143, stage_1_loss:-10.938, stage_2_loss:-11.194
step:1/417 avg loss:-7.360, stage_1_loss:-8.008, stage_2_loss:-7.198
step:301/417 avg loss:-10.437, stage_1_loss:-10.401, stage_2_loss:-10.446
step:301/417 avg loss:-10.332, stage_1_loss:-10.307, stage_2_loss:-10.338
Valid Summary | End of Epoch 42 | Time 285.96s | Current time 2025-01-11 15:18:39.685235 |Valid Loss -10.411| 
Found new best model, dict saved
step:1/1667 avg loss:-8.698, stage_1_loss:-8.699, stage_2_loss:-8.698
step:1/1667 avg loss:-9.851, stage_1_loss:-9.817, stage_2_loss:-9.859
step:1001/1667 avg loss:-11.027, stage_1_loss:-10.961, stage_2_loss:-11.044step:1001/1667 avg loss:-11.002, stage_1_loss:-10.928, stage_2_loss:-11.021

Train Summary | End of Epoch 43 | Time 1728.94s | Current time 2025-01-11 15:47:29.087161 |Train Loss -10.871| 
step:1/417 avg loss:-10.277, stage_1_loss:-10.229, stage_2_loss:-10.289
step:1/417 avg loss:-7.001, stage_1_loss:-7.368, stage_2_loss:-6.909
step:301/417 avg loss:-10.116, stage_1_loss:-10.172, stage_2_loss:-10.102
step:301/417 avg loss:-10.095, stage_1_loss:-10.138, stage_2_loss:-10.085
Valid Summary | End of Epoch 43 | Time 288.06s | Current time 2025-01-11 15:52:17.153153 |Valid Loss -10.132| 
step:1/1667 avg loss:-9.222, stage_1_loss:-9.202, stage_2_loss:-9.227
step:1/1667 avg loss:-10.285, stage_1_loss:-10.235, stage_2_loss:-10.298
step:1001/1667 avg loss:-10.737, stage_1_loss:-10.667, stage_2_loss:-10.754step:1001/1667 avg loss:-10.774, stage_1_loss:-10.688, stage_2_loss:-10.795

Train Summary | End of Epoch 44 | Time 1728.25s | Current time 2025-01-11 16:21:05.578497 |Train Loss -10.832| 
step:1/417 avg loss:-11.063, stage_1_loss:-10.940, stage_2_loss:-11.094
step:1/417 avg loss:-7.559, stage_1_loss:-7.885, stage_2_loss:-7.477
step:301/417 avg loss:-10.483, stage_1_loss:-10.432, stage_2_loss:-10.495
step:301/417 avg loss:-10.400, stage_1_loss:-10.344, stage_2_loss:-10.414
Valid Summary | End of Epoch 44 | Time 289.66s | Current time 2025-01-11 16:25:55.310740 |Valid Loss -10.467| 
Found new best model, dict saved
step:1/1667 avg loss:-10.016, stage_1_loss:-9.969, stage_2_loss:-10.027
step:1/1667 avg loss:-10.676, stage_1_loss:-10.501, stage_2_loss:-10.719
step:1001/1667 avg loss:-11.161, stage_1_loss:-11.074, stage_2_loss:-11.183step:1001/1667 avg loss:-11.165, stage_1_loss:-11.071, stage_2_loss:-11.188

Train Summary | End of Epoch 45 | Time 1728.35s | Current time 2025-01-11 16:54:44.162620 |Train Loss -11.095| 
step:1/417 avg loss:-10.755, stage_1_loss:-10.454, stage_2_loss:-10.830
step:1/417 avg loss:-7.105, stage_1_loss:-7.316, stage_2_loss:-7.052
step:301/417 avg loss:-10.167, stage_1_loss:-10.127, stage_2_loss:-10.177
step:301/417 avg loss:-10.043, stage_1_loss:-10.024, stage_2_loss:-10.048
Valid Summary | End of Epoch 45 | Time 287.36s | Current time 2025-01-11 16:59:31.560238 |Valid Loss -10.144| 
step:1/1667 avg loss:-9.005, stage_1_loss:-9.367, stage_2_loss:-8.914
step:1/1667 avg loss:-12.244, stage_1_loss:-12.295, stage_2_loss:-12.231
step:1001/1667 avg loss:-11.105, stage_1_loss:-11.005, stage_2_loss:-11.130step:1001/1667 avg loss:-11.080, stage_1_loss:-10.981, stage_2_loss:-11.105

Train Summary | End of Epoch 46 | Time 1727.44s | Current time 2025-01-11 17:28:19.173538 |Train Loss -11.120| 
step:1/417 avg loss:-11.057, stage_1_loss:-10.918, stage_2_loss:-11.092
step:1/417 avg loss:-7.720, stage_1_loss:-8.017, stage_2_loss:-7.646
step:301/417 avg loss:-10.287, stage_1_loss:-10.228, stage_2_loss:-10.301
step:301/417 avg loss:-10.287, stage_1_loss:-10.205, stage_2_loss:-10.308
Valid Summary | End of Epoch 46 | Time 288.01s | Current time 2025-01-11 17:33:07.196548 |Valid Loss -10.320| 
step:1/1667 avg loss:-11.707, stage_1_loss:-11.724, stage_2_loss:-11.703
step:1/1667 avg loss:-10.576, stage_1_loss:-10.280, stage_2_loss:-10.650
step:1001/1667 avg loss:-10.839, stage_1_loss:-10.746, stage_2_loss:-10.862step:1001/1667 avg loss:-10.829, stage_1_loss:-10.733, stage_2_loss:-10.853

Train Summary | End of Epoch 47 | Time 1727.16s | Current time 2025-01-11 18:01:54.568000 |Train Loss -10.919| 
step:1/417 avg loss:-10.782, stage_1_loss:-10.753, stage_2_loss:-10.789
step:1/417 avg loss:-7.167, stage_1_loss:-7.643, stage_2_loss:-7.048
step:301/417 avg loss:-10.183, stage_1_loss:-10.144, stage_2_loss:-10.193
step:301/417 avg loss:-10.125, stage_1_loss:-10.067, stage_2_loss:-10.139
Valid Summary | End of Epoch 47 | Time 287.30s | Current time 2025-01-11 18:06:41.870139 |Valid Loss -10.185| 
step:1/1667 avg loss:-11.129, stage_1_loss:-11.049, stage_2_loss:-11.149
step:1/1667 avg loss:-11.498, stage_1_loss:-11.445, stage_2_loss:-11.511
step:1001/1667 avg loss:-10.986, stage_1_loss:-10.904, stage_2_loss:-11.007step:1001/1667 avg loss:-10.995, stage_1_loss:-10.903, stage_2_loss:-11.018

Train Summary | End of Epoch 48 | Time 1728.03s | Current time 2025-01-11 18:35:30.120169 |Train Loss -11.093| 
step:1/417 avg loss:-11.108, stage_1_loss:-11.002, stage_2_loss:-11.134
step:1/417 avg loss:-7.454, stage_1_loss:-7.938, stage_2_loss:-7.332
step:301/417 avg loss:-10.542, stage_1_loss:-10.509, stage_2_loss:-10.551
step:301/417 avg loss:-10.404, stage_1_loss:-10.357, stage_2_loss:-10.415
Valid Summary | End of Epoch 48 | Time 287.99s | Current time 2025-01-11 18:40:18.157586 |Valid Loss -10.508| 
Found new best model, dict saved
step:1/1667 avg loss:-11.096, stage_1_loss:-11.083, stage_2_loss:-11.099
step:1/1667 avg loss:-12.090, stage_1_loss:-12.041, stage_2_loss:-12.102
step:1001/1667 avg loss:-11.306, stage_1_loss:-11.226, stage_2_loss:-11.326step:1001/1667 avg loss:-11.274, stage_1_loss:-11.197, stage_2_loss:-11.294

Train Summary | End of Epoch 49 | Time 1728.35s | Current time 2025-01-11 19:09:07.072969 |Train Loss -11.161| 
step:1/417 avg loss:-10.634, stage_1_loss:-10.532, stage_2_loss:-10.659
step:1/417 avg loss:-8.702, stage_1_loss:-9.008, stage_2_loss:-8.625
step:301/417 avg loss:-10.490, stage_1_loss:-10.443, stage_2_loss:-10.501
step:301/417 avg loss:-10.432, stage_1_loss:-10.373, stage_2_loss:-10.447
Valid Summary | End of Epoch 49 | Time 287.84s | Current time 2025-01-11 19:13:54.956906 |Valid Loss -10.484| 
step:1/1667 avg loss:-11.387, stage_1_loss:-11.374, stage_2_loss:-11.390
step:1/1667 avg loss:-11.964, stage_1_loss:-11.944, stage_2_loss:-11.969
step:1001/1667 avg loss:-10.939, stage_1_loss:-10.854, stage_2_loss:-10.960step:1001/1667 avg loss:-10.934, stage_1_loss:-10.836, stage_2_loss:-10.958

Train Summary | End of Epoch 50 | Time 1724.18s | Current time 2025-01-11 19:42:39.314556 |Train Loss -11.020| 
step:1/417 avg loss:-11.169, stage_1_loss:-11.015, stage_2_loss:-11.207
step:1/417 avg loss:-8.706, stage_1_loss:-9.273, stage_2_loss:-8.564
step:301/417 avg loss:-10.551, stage_1_loss:-10.507, stage_2_loss:-10.562
step:301/417 avg loss:-10.444, stage_1_loss:-10.398, stage_2_loss:-10.456
Valid Summary | End of Epoch 50 | Time 286.51s | Current time 2025-01-11 19:47:25.875906 |Valid Loss -10.530| 
Found new best model, dict saved
step:1/1667 avg loss:-11.191, stage_1_loss:-11.016, stage_2_loss:-11.234
step:1/1667 avg loss:-11.528, stage_1_loss:-10.847, stage_2_loss:-11.699
step:1001/1667 avg loss:-11.273, stage_1_loss:-11.165, stage_2_loss:-11.300
step:1001/1667 avg loss:-11.293, stage_1_loss:-11.190, stage_2_loss:-11.318
Train Summary | End of Epoch 51 | Time 1727.17s | Current time 2025-01-11 20:16:13.706513 |Train Loss -11.261| 
step:1/417 avg loss:-8.808, stage_1_loss:-9.493, stage_2_loss:-8.636
step:1/417 avg loss:-7.443, stage_1_loss:-7.619, stage_2_loss:-7.399
step:301/417 avg loss:-10.479, stage_1_loss:-10.446, stage_2_loss:-10.487
step:301/417 avg loss:-10.337, stage_1_loss:-10.301, stage_2_loss:-10.346
Valid Summary | End of Epoch 51 | Time 288.16s | Current time 2025-01-11 20:21:01.873965 |Valid Loss -10.435| 
step:1/1667 avg loss:-12.852, stage_1_loss:-12.763, stage_2_loss:-12.874
step:1/1667 avg loss:-11.843, stage_1_loss:-11.686, stage_2_loss:-11.882
step:1001/1667 avg loss:-11.182, stage_1_loss:-11.067, stage_2_loss:-11.211
step:1001/1667 avg loss:-11.103, stage_1_loss:-10.984, stage_2_loss:-11.133
Train Summary | End of Epoch 52 | Time 1727.50s | Current time 2025-01-11 20:49:49.566419 |Train Loss -11.161| 
step:1/417 avg loss:-11.436, stage_1_loss:-11.318, stage_2_loss:-11.465
step:1/417 avg loss:-7.772, stage_1_loss:-8.039, stage_2_loss:-7.705
step:301/417 avg loss:-10.727, stage_1_loss:-10.651, stage_2_loss:-10.746
step:301/417 avg loss:-10.643, stage_1_loss:-10.556, stage_2_loss:-10.664
Valid Summary | End of Epoch 52 | Time 288.98s | Current time 2025-01-11 20:54:38.585567 |Valid Loss -10.705| 
Found new best model, dict saved
step:1/1667 avg loss:-10.886, stage_1_loss:-10.721, stage_2_loss:-10.928
step:1/1667 avg loss:-11.719, stage_1_loss:-11.517, stage_2_loss:-11.769
step:1001/1667 avg loss:-11.379, stage_1_loss:-11.254, stage_2_loss:-11.410
step:1001/1667 avg loss:-11.373, stage_1_loss:-11.234, stage_2_loss:-11.408
Train Summary | End of Epoch 53 | Time 1724.14s | Current time 2025-01-11 21:23:23.142612 |Train Loss -11.361| 
step:1/417 avg loss:-11.150, stage_1_loss:-11.110, stage_2_loss:-11.160
step:1/417 avg loss:-9.632, stage_1_loss:-9.441, stage_2_loss:-9.680
step:301/417 avg loss:-10.688, stage_1_loss:-10.589, stage_2_loss:-10.713
step:301/417 avg loss:-10.624, stage_1_loss:-10.524, stage_2_loss:-10.649
Valid Summary | End of Epoch 53 | Time 288.30s | Current time 2025-01-11 21:28:11.481401 |Valid Loss -10.679| 
step:1/1667 avg loss:-11.183, stage_1_loss:-11.161, stage_2_loss:-11.188
step:1/1667 avg loss:-10.931, stage_1_loss:-10.871, stage_2_loss:-10.946
step:1001/1667 avg loss:-11.226, stage_1_loss:-11.100, stage_2_loss:-11.257
step:1001/1667 avg loss:-11.191, stage_1_loss:-11.062, stage_2_loss:-11.223
Train Summary | End of Epoch 54 | Time 1726.28s | Current time 2025-01-11 21:56:58.034577 |Train Loss -11.102| 
step:1/417 avg loss:-11.295, stage_1_loss:-11.131, stage_2_loss:-11.337
step:1/417 avg loss:-10.243, stage_1_loss:-9.722, stage_2_loss:-10.373
step:301/417 avg loss:-10.643, stage_1_loss:-10.530, stage_2_loss:-10.671
step:301/417 avg loss:-10.486, stage_1_loss:-10.381, stage_2_loss:-10.513
Valid Summary | End of Epoch 54 | Time 288.39s | Current time 2025-01-11 22:01:46.426450 |Valid Loss -10.600| 
step:1/1667 avg loss:-8.858, stage_1_loss:-9.176, stage_2_loss:-8.778
step:1/1667 avg loss:-11.965, stage_1_loss:-11.741, stage_2_loss:-12.021
step:1001/1667 avg loss:-11.168, stage_1_loss:-11.045, stage_2_loss:-11.199
step:1001/1667 avg loss:-11.176, stage_1_loss:-11.045, stage_2_loss:-11.209
Train Summary | End of Epoch 55 | Time 1729.38s | Current time 2025-01-11 22:30:35.952316 |Train Loss -11.205| 
step:1/417 avg loss:-11.227, stage_1_loss:-11.069, stage_2_loss:-11.267
step:1/417 avg loss:-8.186, stage_1_loss:-9.002, stage_2_loss:-7.982
step:301/417 avg loss:-10.691, stage_1_loss:-10.598, stage_2_loss:-10.714
step:301/417 avg loss:-10.543, stage_1_loss:-10.443, stage_2_loss:-10.568
Valid Summary | End of Epoch 55 | Time 289.44s | Current time 2025-01-11 22:35:25.408079 |Valid Loss -10.644| 
step:1/1667 avg loss:-10.888, stage_1_loss:-10.789, stage_2_loss:-10.913
step:1/1667 avg loss:-12.289, stage_1_loss:-12.191, stage_2_loss:-12.314
step:1001/1667 avg loss:-11.024, stage_1_loss:-10.901, stage_2_loss:-11.055step:1001/1667 avg loss:-11.005, stage_1_loss:-10.879, stage_2_loss:-11.036

Train Summary | End of Epoch 56 | Time 1730.57s | Current time 2025-01-11 23:04:16.161118 |Train Loss -11.144| 
step:1/417 avg loss:-11.468, stage_1_loss:-11.347, stage_2_loss:-11.499
step:1/417 avg loss:-7.873, stage_1_loss:-8.219, stage_2_loss:-7.786
step:301/417 avg loss:-10.691, stage_1_loss:-10.614, stage_2_loss:-10.711
step:301/417 avg loss:-10.612, stage_1_loss:-10.531, stage_2_loss:-10.632
Valid Summary | End of Epoch 56 | Time 288.09s | Current time 2025-01-11 23:09:04.300316 |Valid Loss -10.676| 
step:1/1667 avg loss:-10.610, stage_1_loss:-10.436, stage_2_loss:-10.654
step:1/1667 avg loss:-11.247, stage_1_loss:-11.075, stage_2_loss:-11.290
step:1001/1667 avg loss:-11.351, stage_1_loss:-11.222, stage_2_loss:-11.384step:1001/1667 avg loss:-11.348, stage_1_loss:-11.217, stage_2_loss:-11.381

Train Summary | End of Epoch 57 | Time 1729.81s | Current time 2025-01-11 23:37:54.282957 |Train Loss -11.364| 
step:1/417 avg loss:-10.919, stage_1_loss:-10.685, stage_2_loss:-10.978
step:1/417 avg loss:-7.567, stage_1_loss:-8.014, stage_2_loss:-7.455
step:301/417 avg loss:-10.764, stage_1_loss:-10.649, stage_2_loss:-10.793
step:301/417 avg loss:-10.695, stage_1_loss:-10.579, stage_2_loss:-10.724
Valid Summary | End of Epoch 57 | Time 288.72s | Current time 2025-01-11 23:42:43.006447 |Valid Loss -10.747| 
Found new best model, dict saved
step:1/1667 avg loss:-11.204, stage_1_loss:-10.981, stage_2_loss:-11.259
step:1/1667 avg loss:-10.987, stage_1_loss:-10.879, stage_2_loss:-11.014
step:1001/1667 avg loss:-11.448, stage_1_loss:-11.304, stage_2_loss:-11.484
step:1001/1667 avg loss:-11.477, stage_1_loss:-11.325, stage_2_loss:-11.515
Train Summary | End of Epoch 58 | Time 1733.43s | Current time 2025-01-12 00:11:36.881178 |Train Loss -11.445| 
step:1/417 avg loss:-11.333, stage_1_loss:-11.176, stage_2_loss:-11.373
step:1/417 avg loss:-8.380, stage_1_loss:-9.011, stage_2_loss:-8.223
step:301/417 avg loss:-10.757, stage_1_loss:-10.649, stage_2_loss:-10.784
step:301/417 avg loss:-10.649, stage_1_loss:-10.542, stage_2_loss:-10.676
Valid Summary | End of Epoch 58 | Time 287.79s | Current time 2025-01-12 00:16:24.710088 |Valid Loss -10.746| 
step:1/1667 avg loss:-13.446, stage_1_loss:-13.358, stage_2_loss:-13.468
step:1/1667 avg loss:-11.086, stage_1_loss:-10.978, stage_2_loss:-11.112
step:1001/1667 avg loss:-11.099, stage_1_loss:-10.972, stage_2_loss:-11.131step:1001/1667 avg loss:-11.058, stage_1_loss:-10.921, stage_2_loss:-11.092

Train Summary | End of Epoch 59 | Time 1727.71s | Current time 2025-01-12 00:45:12.691475 |Train Loss -11.081| 
step:1/417 avg loss:-11.235, stage_1_loss:-11.035, stage_2_loss:-11.285
step:1/417 avg loss:-8.154, stage_1_loss:-8.801, stage_2_loss:-7.992
step:301/417 avg loss:-10.674, stage_1_loss:-10.588, stage_2_loss:-10.695
step:301/417 avg loss:-10.628, stage_1_loss:-10.534, stage_2_loss:-10.652
Valid Summary | End of Epoch 59 | Time 286.61s | Current time 2025-01-12 00:49:59.309745 |Valid Loss -10.696| 
step:1/1667 avg loss:-11.635, stage_1_loss:-11.539, stage_2_loss:-11.659
step:1/1667 avg loss:-12.004, stage_1_loss:-11.901, stage_2_loss:-12.030
step:1001/1667 avg loss:-11.490, stage_1_loss:-11.342, stage_2_loss:-11.527step:1001/1667 avg loss:-11.504, stage_1_loss:-11.351, stage_2_loss:-11.542

Train Summary | End of Epoch 60 | Time 1727.76s | Current time 2025-01-12 01:18:47.544387 |Train Loss -11.290| 
step:1/417 avg loss:-10.644, stage_1_loss:-10.441, stage_2_loss:-10.695
step:1/417 avg loss:-9.541, stage_1_loss:-9.650, stage_2_loss:-9.514
step:301/417 avg loss:-10.398, stage_1_loss:-10.350, stage_2_loss:-10.410
step:301/417 avg loss:-10.317, stage_1_loss:-10.260, stage_2_loss:-10.331
Valid Summary | End of Epoch 60 | Time 288.73s | Current time 2025-01-12 01:23:36.275082 |Valid Loss -10.362| 
step:1/1667 avg loss:-10.893, stage_1_loss:-10.951, stage_2_loss:-10.878
step:1/1667 avg loss:-11.807, stage_1_loss:-11.517, stage_2_loss:-11.879
step:1001/1667 avg loss:-11.564, stage_1_loss:-11.435, stage_2_loss:-11.596step:1001/1667 avg loss:-11.556, stage_1_loss:-11.418, stage_2_loss:-11.591

Train Summary | End of Epoch 61 | Time 1730.15s | Current time 2025-01-12 01:52:26.949494 |Train Loss -11.556| 
step:1/417 avg loss:-11.519, stage_1_loss:-11.288, stage_2_loss:-11.577
step:1/417 avg loss:-8.426, stage_1_loss:-8.912, stage_2_loss:-8.305
step:301/417 avg loss:-10.790, stage_1_loss:-10.687, stage_2_loss:-10.815
step:301/417 avg loss:-10.644, stage_1_loss:-10.550, stage_2_loss:-10.668
Valid Summary | End of Epoch 61 | Time 289.44s | Current time 2025-01-12 01:57:16.440015 |Valid Loss -10.735| 
step:1/1667 avg loss:-12.041, stage_1_loss:-11.888, stage_2_loss:-12.079
step:1/1667 avg loss:-12.502, stage_1_loss:-12.540, stage_2_loss:-12.492
step:1001/1667 avg loss:-11.460, stage_1_loss:-11.315, stage_2_loss:-11.496
step:1001/1667 avg loss:-11.437, stage_1_loss:-11.287, stage_2_loss:-11.474
Train Summary | End of Epoch 62 | Time 1728.23s | Current time 2025-01-12 02:26:04.876645 |Train Loss -11.468| 
step:1/417 avg loss:-11.272, stage_1_loss:-11.219, stage_2_loss:-11.285
step:1/417 avg loss:-10.617, stage_1_loss:-10.506, stage_2_loss:-10.645
step:301/417 avg loss:-10.684, stage_1_loss:-10.569, stage_2_loss:-10.713
step:301/417 avg loss:-10.634, stage_1_loss:-10.506, stage_2_loss:-10.666
Valid Summary | End of Epoch 62 | Time 287.05s | Current time 2025-01-12 02:30:52.808832 |Valid Loss -10.698| 
step:1/1667 avg loss:-12.704, stage_1_loss:-12.562, stage_2_loss:-12.740
step:1/1667 avg loss:-11.134, stage_1_loss:-10.921, stage_2_loss:-11.187
step:1001/1667 avg loss:-11.521, stage_1_loss:-11.381, stage_2_loss:-11.556step:1001/1667 avg loss:-11.463, stage_1_loss:-11.314, stage_2_loss:-11.500

Train Summary | End of Epoch 63 | Time 1747.84s | Current time 2025-01-12 03:00:00.930603 |Train Loss -11.478| 
started on logs/USEV_SelfMem

Namespace(mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', audio_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/lrs3.lst', noise_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/wham.lst', batch_size=6, max_length=6, num_workers=8, epochs=100, effec_batch_size=8, accu_grad=0, C=2, model_name='USEV_SelfMem', lr=0.001, max_norm=5, log_name='logs/USEV_SelfMem', use_tensorboard=1, continue_from='logs/USEV_SelfMem', local_rank=0, sample_rate=16000, win=512, hop_length=128, n_mels=80, shift_range='0,1', Self_enroll_amplitude_scaling='0.1,1', teacher_point=50, num_slots='5,5', loss_weight='28', distributed=True, world_size=2, speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800)

Total number of parameters: 16421830 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
wav_list_length: 118513
noise_list_length: 68000
wav_list_length: 118513
noise_list_length: 68000
Resume training from epoch: 63
step:1/988 avg loss:-5.630, stage_1_loss:-5.787, stage_2_loss:-5.591
step:1/988 avg loss:-11.079, stage_1_loss:-9.539, stage_2_loss:-11.464
started on logs/USEV_SelfMem

Namespace(mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', audio_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/lrs3.lst', noise_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/wham.lst', batch_size=6, max_length=6, num_workers=8, epochs=100, effec_batch_size=8, accu_grad=0, C=2, model_name='USEV_SelfMem', lr=0.001, max_norm=5, log_name='logs/USEV_SelfMem', use_tensorboard=1, continue_from='logs/USEV_SelfMem', local_rank=0, sample_rate=16000, win=512, hop_length=128, n_mels=80, shift_range='0,1', Self_enroll_amplitude_scaling='0.1,1', teacher_point=50, num_slots='5,5', loss_weight='28', distributed=True, world_size=2, speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800)

Total number of parameters: 16421830 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 63
step:1/1667 avg loss:-11.130, stage_1_loss:-10.862, stage_2_loss:-11.197
step:1/1667 avg loss:-12.692, stage_1_loss:-12.529, stage_2_loss:-12.732
started on logs/USEV_SelfMem

Namespace(C=2, Self_enroll_amplitude_scaling='0.1,1', accu_grad=0, audio_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/lrs3.lst', batch_size=6, continue_from='logs/USEV_SelfMem', distributed=False, effec_batch_size=8, epochs=100, hop_length=128, local_rank=0, log_name='logs/USEV_SelfMem', loss_weight='28', lr=0.001, max_length=6, max_norm=5, mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', model_name='USEV_SelfMem', n_mels=80, noise_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/wham.lst', num_slots='5,5', num_workers=8, obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', sample_rate=16000, shift_range='0,1', speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800, teacher_point=50, use_tensorboard=1, visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', win=512, world_size=1)

Total number of parameters: 16421830 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 63
started on logs/USEV_SelfMem

Namespace(C=2, Self_enroll_amplitude_scaling='0.1,1', accu_grad=0, audio_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/lrs3.lst', batch_size=6, continue_from='logs/USEV_SelfMem', distributed=False, effec_batch_size=8, epochs=100, hop_length=128, local_rank=0, log_name='logs/USEV_SelfMem', loss_weight='28', lr=0.001, max_length=6, max_norm=5, mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', model_name='USEV_SelfMem', n_mels=80, noise_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/wham.lst', num_slots='5,5', num_workers=8, obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', sample_rate=16000, shift_range='0,1', speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800, teacher_point=50, use_tensorboard=1, visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', win=512, world_size=1)

Total number of parameters: 16421830 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 63
started on logs/USEV_SelfMem

Namespace(C=2, Self_enroll_amplitude_scaling='0.1,1', accu_grad=0, audio_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/lrs3.lst', batch_size=6, continue_from='logs/USEV_SelfMem', distributed=False, effec_batch_size=8, epochs=100, hop_length=128, local_rank=0, log_name='logs/USEV_SelfMem', loss_weight='28', lr=0.001, max_length=6, max_norm=5, mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', mixture_direc='/data/junjie/data/mixture/', model_name='USEV_SelfMem', n_mels=80, noise_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/wham.lst', num_slots='5,5', num_workers=8, obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', sample_rate=16000, shift_range='0,1', speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800, teacher_point=50, use_tensorboard=1, visual_direc='/data/junjie/data/mp4/', win=512, world_size=1)

Total number of parameters: 16421830 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 63
step:1/3334 avg loss:-12.345, stage_1_loss:-12.206, stage_2_loss:-12.380
started on logs/USEV_SelfMem

Namespace(C=2, Self_enroll_amplitude_scaling='0.1,1', accu_grad=0, audio_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/lrs3.lst', batch_size=2, continue_from='logs/USEV_SelfMem', distributed=False, effec_batch_size=8, epochs=100, hop_length=128, local_rank=0, log_name='logs/USEV_SelfMem', loss_weight='28', lr=0.001, max_length=6, max_norm=5, mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', mixture_direc='/data/junjie/data/mixture/', model_name='USEV_SelfMem', n_mels=80, noise_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/wham.lst', num_slots='5,5', num_workers=8, obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', sample_rate=16000, shift_range='0,1', speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800, teacher_point=50, use_tensorboard=1, visual_direc='/data/junjie/data/mp4/', win=512, world_size=1)

Total number of parameters: 16421830 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 63
step:1/10000 avg loss:-12.369, stage_1_loss:-12.551, stage_2_loss:-12.324
started on logs/USEV_SelfMem

Namespace(C=2, Self_enroll_amplitude_scaling='0.1,1', accu_grad=0, audio_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/lrs3.lst', batch_size=3, continue_from='logs/USEV_SelfMem', distributed=True, effec_batch_size=8, epochs=100, hop_length=128, local_rank=0, log_name='logs/USEV_SelfMem', loss_weight='28', lr=0.001, max_length=6, max_norm=5, mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', mixture_direc='/data/junjie/data/mixture/', model_name='USEV_SelfMem', n_mels=80, noise_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/wham.lst', num_slots='5,5', num_workers=8, obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', sample_rate=16000, shift_range='0,1', speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800, teacher_point=50, use_tensorboard=1, visual_direc='/data/junjie/data/mp4/', win=512, world_size=2)

Total number of parameters: 16421830 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
started on logs/USEV_SelfMem

Namespace(C=2, Self_enroll_amplitude_scaling='0.1,1', accu_grad=0, audio_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/lrs3.lst', batch_size=3, continue_from='logs/USEV_SelfMem', distributed=True, effec_batch_size=8, epochs=100, hop_length=128, local_rank=0, log_name='logs/USEV_SelfMem', loss_weight='28', lr=0.001, max_length=6, max_norm=5, mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', mixture_direc='/data/junjie/data/mixture/', model_name='USEV_SelfMem', n_mels=80, noise_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/wham.lst', num_slots='5,5', num_workers=8, obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', sample_rate=16000, shift_range='0,1', speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800, teacher_point=50, use_tensorboard=1, visual_direc='/data/junjie/data/mp4/', win=512, world_size=2)

Total number of parameters: 16421830 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 63
step:1/3334 avg loss:-9.561, stage_1_loss:-9.482, stage_2_loss:-9.580
step:1/3334 avg loss:-13.441, stage_1_loss:-13.362, stage_2_loss:-13.461
step:1001/3334 avg loss:-10.895, stage_1_loss:-10.748, stage_2_loss:-10.931
step:1001/3334 avg loss:-11.006, stage_1_loss:-10.859, stage_2_loss:-11.043
step:2001/3334 avg loss:-10.918, stage_1_loss:-10.764, stage_2_loss:-10.957
step:2001/3334 avg loss:-11.029, stage_1_loss:-10.870, stage_2_loss:-11.068
step:3001/3334 avg loss:-10.886, stage_1_loss:-10.726, stage_2_loss:-10.926
step:3001/3334 avg loss:-10.976, stage_1_loss:-10.822, stage_2_loss:-11.014
Train Summary | End of Epoch 63 | Time 9116.63s | Current time 2025-01-16 13:54:36.238259 |Train Loss -10.929| 
step:1/834 avg loss:-12.709, stage_1_loss:-12.710, stage_2_loss:-12.709
step:1/834 avg loss:-10.990, stage_1_loss:-10.797, stage_2_loss:-11.038
step:301/834 avg loss:-10.170, stage_1_loss:-10.084, stage_2_loss:-10.191
step:301/834 avg loss:-10.170, stage_1_loss:-10.099, stage_2_loss:-10.187
step:601/834 avg loss:-10.165, stage_1_loss:-10.077, stage_2_loss:-10.187
step:601/834 avg loss:-10.165, stage_1_loss:-10.096, stage_2_loss:-10.183
Valid Summary | End of Epoch 63 | Time 1655.53s | Current time 2025-01-16 14:22:11.772997 |Valid Loss -10.163| 
Learning rate adjusted to: 0.000250
step:1/3334 avg loss:-11.689, stage_1_loss:-11.557, stage_2_loss:-11.721
step:1/3334 avg loss:-9.936, stage_1_loss:-9.515, stage_2_loss:-10.041
step:1001/3334 avg loss:-11.260, stage_1_loss:-11.123, stage_2_loss:-11.294
step:1001/3334 avg loss:-11.343, stage_1_loss:-11.199, stage_2_loss:-11.379
step:2001/3334 avg loss:-11.365, stage_1_loss:-11.231, stage_2_loss:-11.399
step:2001/3334 avg loss:-11.440, stage_1_loss:-11.302, stage_2_loss:-11.474
step:3001/3334 avg loss:-11.381, stage_1_loss:-11.245, stage_2_loss:-11.415
step:3001/3334 avg loss:-11.460, stage_1_loss:-11.326, stage_2_loss:-11.493
Train Summary | End of Epoch 64 | Time 8619.76s | Current time 2025-01-16 16:45:53.650990 |Train Loss -11.419| 
step:1/834 avg loss:-13.087, stage_1_loss:-13.096, stage_2_loss:-13.085
step:1/834 avg loss:-11.721, stage_1_loss:-11.463, stage_2_loss:-11.785
step:301/834 avg loss:-10.990, stage_1_loss:-10.896, stage_2_loss:-11.013
step:301/834 avg loss:-11.006, stage_1_loss:-10.893, stage_2_loss:-11.034
step:601/834 avg loss:-10.918, stage_1_loss:-10.825, stage_2_loss:-10.942
step:601/834 avg loss:-10.926, stage_1_loss:-10.825, stage_2_loss:-10.951
Valid Summary | End of Epoch 64 | Time 1661.74s | Current time 2025-01-16 17:13:35.394891 |Valid Loss -10.912| 
Found new best model, dict saved
step:1/3334 avg loss:-12.615, stage_1_loss:-12.531, stage_2_loss:-12.636
step:1/3334 avg loss:-10.882, stage_1_loss:-10.652, stage_2_loss:-10.939
step:1001/3334 avg loss:-11.612, stage_1_loss:-11.486, stage_2_loss:-11.644
step:1001/3334 avg loss:-11.709, stage_1_loss:-11.576, stage_2_loss:-11.742
step:2001/3334 avg loss:-11.584, stage_1_loss:-11.453, stage_2_loss:-11.616
step:2001/3334 avg loss:-11.650, stage_1_loss:-11.514, stage_2_loss:-11.683
step:3001/3334 avg loss:-11.617, stage_1_loss:-11.485, stage_2_loss:-11.650
step:3001/3334 avg loss:-11.676, stage_1_loss:-11.540, stage_2_loss:-11.710
Train Summary | End of Epoch 65 | Time 8541.02s | Current time 2025-01-16 19:36:00.739359 |Train Loss -11.583| 
step:1/834 avg loss:-11.082, stage_1_loss:-10.929, stage_2_loss:-11.120
step:1/834 avg loss:-12.866, stage_1_loss:-12.832, stage_2_loss:-12.875
step:301/834 avg loss:-10.789, stage_1_loss:-10.697, stage_2_loss:-10.812
step:301/834 avg loss:-10.780, stage_1_loss:-10.690, stage_2_loss:-10.802
step:601/834 avg loss:-10.763, stage_1_loss:-10.666, stage_2_loss:-10.787
step:601/834 avg loss:-10.763, stage_1_loss:-10.671, stage_2_loss:-10.786
Valid Summary | End of Epoch 65 | Time 1612.25s | Current time 2025-01-16 20:02:52.993449 |Valid Loss -10.754| 
step:1/3334 avg loss:-10.921, stage_1_loss:-10.468, stage_2_loss:-11.034
step:1/3334 avg loss:-10.120, stage_1_loss:-9.841, stage_2_loss:-10.189
step:1001/3334 avg loss:-11.597, stage_1_loss:-11.449, stage_2_loss:-11.634
step:1001/3334 avg loss:-11.474, stage_1_loss:-11.322, stage_2_loss:-11.512
step:2001/3334 avg loss:-11.690, stage_1_loss:-11.549, stage_2_loss:-11.725
step:2001/3334 avg loss:-11.646, stage_1_loss:-11.502, stage_2_loss:-11.682
step:3001/3334 avg loss:-11.713, stage_1_loss:-11.576, stage_2_loss:-11.748
step:3001/3334 avg loss:-11.679, stage_1_loss:-11.536, stage_2_loss:-11.715
Train Summary | End of Epoch 66 | Time 8796.05s | Current time 2025-01-16 22:29:30.624016 |Train Loss -11.690| 
step:1/834 avg loss:-13.342, stage_1_loss:-13.352, stage_2_loss:-13.339
step:1/834 avg loss:-11.052, stage_1_loss:-10.964, stage_2_loss:-11.074
step:301/834 avg loss:-11.075, stage_1_loss:-10.974, stage_2_loss:-11.101
step:301/834 avg loss:-11.100, stage_1_loss:-10.986, stage_2_loss:-11.129
step:601/834 avg loss:-10.995, stage_1_loss:-10.897, stage_2_loss:-11.019
step:601/834 avg loss:-11.058, stage_1_loss:-10.948, stage_2_loss:-11.086
Valid Summary | End of Epoch 66 | Time 1751.57s | Current time 2025-01-16 22:58:42.196035 |Valid Loss -11.022| 
Found new best model, dict saved
step:1/3334 avg loss:-11.233, stage_1_loss:-11.128, stage_2_loss:-11.260
step:1/3334 avg loss:-10.961, stage_1_loss:-10.891, stage_2_loss:-10.978
step:1001/3334 avg loss:-11.694, stage_1_loss:-11.535, stage_2_loss:-11.734
step:1001/3334 avg loss:-11.775, stage_1_loss:-11.620, stage_2_loss:-11.813
step:2001/3334 avg loss:-11.795, stage_1_loss:-11.650, stage_2_loss:-11.831
step:2001/3334 avg loss:-11.712, stage_1_loss:-11.563, stage_2_loss:-11.750
step:3001/3334 avg loss:-11.667, stage_1_loss:-11.511, stage_2_loss:-11.706
step:3001/3334 avg loss:-11.742, stage_1_loss:-11.593, stage_2_loss:-11.780
Train Summary | End of Epoch 67 | Time 9512.63s | Current time 2025-01-17 01:37:19.402983 |Train Loss -11.684| 
step:1/834 avg loss:-13.244, stage_1_loss:-13.165, stage_2_loss:-13.264
step:1/834 avg loss:-11.269, stage_1_loss:-11.193, stage_2_loss:-11.288
step:301/834 avg loss:-10.999, stage_1_loss:-10.888, stage_2_loss:-11.026
step:301/834 avg loss:-11.016, stage_1_loss:-10.911, stage_2_loss:-11.042
step:601/834 avg loss:-10.961, stage_1_loss:-10.857, stage_2_loss:-10.987
step:601/834 avg loss:-10.966, stage_1_loss:-10.851, stage_2_loss:-10.994
Valid Summary | End of Epoch 67 | Time 1797.98s | Current time 2025-01-17 02:07:17.380287 |Valid Loss -10.954| 
step:1/3334 avg loss:-14.187, stage_1_loss:-13.932, stage_2_loss:-14.250
step:1/3334 avg loss:-11.407, stage_1_loss:-11.216, stage_2_loss:-11.454
step:1001/3334 avg loss:-11.784, stage_1_loss:-11.638, stage_2_loss:-11.821
step:1001/3334 avg loss:-11.738, stage_1_loss:-11.578, stage_2_loss:-11.778
step:2001/3334 avg loss:-11.734, stage_1_loss:-11.581, stage_2_loss:-11.772
step:2001/3334 avg loss:-11.794, stage_1_loss:-11.653, stage_2_loss:-11.829
step:3001/3334 avg loss:-11.706, stage_1_loss:-11.555, stage_2_loss:-11.744
step:3001/3334 avg loss:-11.769, stage_1_loss:-11.625, stage_2_loss:-11.804
Train Summary | End of Epoch 68 | Time 10732.80s | Current time 2025-01-17 05:06:11.773262 |Train Loss -11.734| 
step:1/834 avg loss:-13.179, stage_1_loss:-13.131, stage_2_loss:-13.191
step:1/834 avg loss:-11.647, stage_1_loss:-11.386, stage_2_loss:-11.712
step:301/834 avg loss:-11.050, stage_1_loss:-10.925, stage_2_loss:-11.081
step:301/834 avg loss:-11.101, stage_1_loss:-10.965, stage_2_loss:-11.136
step:601/834 avg loss:-11.075, stage_1_loss:-10.941, stage_2_loss:-11.108
step:601/834 avg loss:-11.029, stage_1_loss:-10.905, stage_2_loss:-11.061
Valid Summary | End of Epoch 68 | Time 1782.86s | Current time 2025-01-17 05:35:54.641464 |Valid Loss -11.038| 
Found new best model, dict saved
step:1/3334 avg loss:-11.673, stage_1_loss:-11.707, stage_2_loss:-11.664
step:1/3334 avg loss:-12.529, stage_1_loss:-12.580, stage_2_loss:-12.517
step:1001/3334 avg loss:-11.866, stage_1_loss:-11.710, stage_2_loss:-11.905
step:1001/3334 avg loss:-11.804, stage_1_loss:-11.651, stage_2_loss:-11.842
step:2001/3334 avg loss:-11.622, stage_1_loss:-11.464, stage_2_loss:-11.662
step:2001/3334 avg loss:-11.701, stage_1_loss:-11.542, stage_2_loss:-11.741
step:3001/3334 avg loss:-11.747, stage_1_loss:-11.591, stage_2_loss:-11.786
step:3001/3334 avg loss:-11.678, stage_1_loss:-11.518, stage_2_loss:-11.717
Train Summary | End of Epoch 69 | Time 9442.54s | Current time 2025-01-17 08:13:20.975559 |Train Loss -11.717| 
step:1/834 avg loss:-13.362, stage_1_loss:-13.310, stage_2_loss:-13.375
step:1/834 avg loss:-11.360, stage_1_loss:-11.329, stage_2_loss:-11.368
step:301/834 avg loss:-11.122, stage_1_loss:-10.997, stage_2_loss:-11.154
step:301/834 avg loss:-11.110, stage_1_loss:-10.985, stage_2_loss:-11.141
step:601/834 avg loss:-11.101, stage_1_loss:-10.980, stage_2_loss:-11.131
step:601/834 avg loss:-11.072, stage_1_loss:-10.957, stage_2_loss:-11.101
Valid Summary | End of Epoch 69 | Time 1639.07s | Current time 2025-01-17 08:40:40.047649 |Valid Loss -11.057| 
Found new best model, dict saved
step:1/3334 avg loss:-10.257, stage_1_loss:-10.195, stage_2_loss:-10.272
step:1/3334 avg loss:-12.954, stage_1_loss:-12.902, stage_2_loss:-12.967
step:1001/3334 avg loss:-11.848, stage_1_loss:-11.701, stage_2_loss:-11.885
step:1001/3334 avg loss:-11.807, stage_1_loss:-11.656, stage_2_loss:-11.844
step:2001/3334 avg loss:-11.880, stage_1_loss:-11.737, stage_2_loss:-11.916
step:2001/3334 avg loss:-11.842, stage_1_loss:-11.692, stage_2_loss:-11.879
step:3001/3334 avg loss:-11.819, stage_1_loss:-11.669, stage_2_loss:-11.856
step:3001/3334 avg loss:-11.853, stage_1_loss:-11.709, stage_2_loss:-11.889
Train Summary | End of Epoch 70 | Time 9213.94s | Current time 2025-01-17 11:14:18.138319 |Train Loss -11.815| 
step:1/834 avg loss:-13.242, stage_1_loss:-13.200, stage_2_loss:-13.253
step:1/834 avg loss:-10.683, stage_1_loss:-10.722, stage_2_loss:-10.673
step:301/834 avg loss:-11.052, stage_1_loss:-10.934, stage_2_loss:-11.082
step:301/834 avg loss:-11.103, stage_1_loss:-10.976, stage_2_loss:-11.135
step:601/834 avg loss:-11.008, stage_1_loss:-10.904, stage_2_loss:-11.034
step:601/834 avg loss:-11.060, stage_1_loss:-10.937, stage_2_loss:-11.090
Valid Summary | End of Epoch 70 | Time 1636.84s | Current time 2025-01-17 11:41:34.980151 |Valid Loss -11.029| 
step:1/3334 avg loss:-12.370, stage_1_loss:-11.964, stage_2_loss:-12.471
step:1/3334 avg loss:-13.504, stage_1_loss:-13.433, stage_2_loss:-13.522
step:1001/3334 avg loss:-11.949, stage_1_loss:-11.800, stage_2_loss:-11.986
step:1001/3334 avg loss:-11.771, stage_1_loss:-11.626, stage_2_loss:-11.807
step:2001/3334 avg loss:-11.868, stage_1_loss:-11.713, stage_2_loss:-11.906
step:2001/3334 avg loss:-11.744, stage_1_loss:-11.595, stage_2_loss:-11.782
step:3001/3334 avg loss:-11.771, stage_1_loss:-11.622, stage_2_loss:-11.808
step:3001/3334 avg loss:-11.860, stage_1_loss:-11.706, stage_2_loss:-11.899
Train Summary | End of Epoch 71 | Time 8466.96s | Current time 2025-01-17 14:02:46.228964 |Train Loss -11.816| 
step:1/834 avg loss:-11.365, stage_1_loss:-11.443, stage_2_loss:-11.345
step:1/834 avg loss:-13.307, stage_1_loss:-13.249, stage_2_loss:-13.321
step:301/834 avg loss:-11.195, stage_1_loss:-11.071, stage_2_loss:-11.226
step:301/834 avg loss:-11.186, stage_1_loss:-11.055, stage_2_loss:-11.219
step:601/834 avg loss:-11.147, stage_1_loss:-11.019, stage_2_loss:-11.179
step:601/834 avg loss:-11.148, stage_1_loss:-11.024, stage_2_loss:-11.179
Valid Summary | End of Epoch 71 | Time 1621.51s | Current time 2025-01-17 14:29:47.743913 |Valid Loss -11.130| 
Found new best model, dict saved
step:1/3334 avg loss:-13.461, stage_1_loss:-13.289, stage_2_loss:-13.504
step:1/3334 avg loss:-11.605, stage_1_loss:-11.589, stage_2_loss:-11.609
started on logs/USEV_SelfMem

Namespace(C=2, Self_enroll_amplitude_scaling='0.1,1', accu_grad=0, audio_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/lrs3.lst', batch_size=3, continue_from='logs/USEV_SelfMem', distributed=True, effec_batch_size=8, epochs=100, hop_length=128, local_rank=0, log_name='logs/USEV_SelfMem', loss_weight='28', lr=0.001, max_length=6, max_norm=5, mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', mixture_direc='/data/junjie/data/mixture/', model_name='USEV_SelfMem', n_mels=80, noise_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/wham.lst', num_slots='5,5', num_workers=8, obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', sample_rate=16000, shift_range='0,1', speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800, teacher_point=50, use_tensorboard=1, visual_direc='/data/junjie/data/mp4/', win=512, world_size=2)

Total number of parameters: 16421830 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 72
step:1/3334 avg loss:-13.462, stage_1_loss:-13.280, stage_2_loss:-13.507
step:1/3334 avg loss:-11.535, stage_1_loss:-11.594, stage_2_loss:-11.521
step:1001/3334 avg loss:-11.790, stage_1_loss:-11.641, stage_2_loss:-11.828
step:1001/3334 avg loss:-11.858, stage_1_loss:-11.717, stage_2_loss:-11.893
step:2001/3334 avg loss:-11.546, stage_1_loss:-11.389, stage_2_loss:-11.585
step:2001/3334 avg loss:-11.585, stage_1_loss:-11.438, stage_2_loss:-11.621
step:3001/3334 avg loss:-11.577, stage_1_loss:-11.418, stage_2_loss:-11.617
step:3001/3334 avg loss:-11.649, stage_1_loss:-11.497, stage_2_loss:-11.687
Train Summary | End of Epoch 72 | Time 8776.64s | Current time 2025-01-21 00:23:29.493936 |Train Loss -11.617| 
step:1/834 avg loss:-11.300, stage_1_loss:-11.345, stage_2_loss:-11.289
step:1/834 avg loss:-13.260, stage_1_loss:-13.230, stage_2_loss:-13.267
step:301/834 avg loss:-11.107, stage_1_loss:-10.978, stage_2_loss:-11.140
step:301/834 avg loss:-11.106, stage_1_loss:-10.987, stage_2_loss:-11.135
started on logs/USEV_SelfMem

Namespace(C=2, Self_enroll_amplitude_scaling='0.1,1', accu_grad=0, audio_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/lrs3.lst', batch_size=3, continue_from='logs/USEV_SelfMem', distributed=True, effec_batch_size=8, epochs=100, hop_length=128, local_rank=0, log_name='logs/USEV_SelfMem', loss_weight='28', lr=0.001, max_length=4, max_norm=5, mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', mixture_direc='/data/junjie/data/mixture/', model_name='USEV_SelfMem', n_mels=80, noise_wav_list='/mntcephfs/lab_data/lijunjie/AVTSE_momentum/data_preparation/dynamic_mixing_list/wham.lst', num_slots='5,5', num_workers=6, obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', sample_rate=16000, shift_range='0,1', speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800, teacher_point=50, use_tensorboard=1, visual_direc='/data/junjie/data/mp4/', win=512, world_size=2)

Total number of parameters: 16421830 

usev(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): rnn(
    (layer_norm): GroupNorm(1, 256, eps=1e-08, affine=True)
    (bottleneck_conv1x1): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
    (dual_rnn): ModuleList(
      (0-5): 6 x Dual_RNN_Block(
        (intra_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (inter_rnn): LSTM(64, 128, batch_first=True, bidirectional=True)
        (intra_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (inter_norm): GroupNorm(1, 64, eps=1e-08, affine=True)
        (intra_linear): Linear(in_features=256, out_features=64, bias=True)
        (inter_linear): Linear(in_features=256, out_features=64, bias=True)
      )
    )
    (prelu): PReLU(num_parameters=1)
    (mask_conv1x1): Conv1d(64, 256, kernel_size=(1,), stride=(1,), bias=False)
    (v_ds): Linear(in_features=512, out_features=256, bias=False)
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): GlobalLayerNorm()
          (2): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
          (3): ReLU()
          (4): GlobalLayerNorm()
          (5): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (6): PReLU(num_parameters=1)
          (7): GlobalLayerNorm()
          (8): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (av_conv): Conv1d(320, 64, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 72
step:1/1667 avg loss:-11.740, stage_1_loss:-11.511, stage_2_loss:-11.797
step:1/1667 avg loss:-12.711, stage_1_loss:-12.140, stage_2_loss:-12.854
step:1001/1667 avg loss:-11.479, stage_1_loss:-11.306, stage_2_loss:-11.523
step:1001/1667 avg loss:-11.417, stage_1_loss:-11.233, stage_2_loss:-11.463
Train Summary | End of Epoch 72 | Time 4599.36s | Current time 2025-01-21 18:10:05.332198 |Train Loss -11.527| 
step:1/417 avg loss:-9.641, stage_1_loss:-9.530, stage_2_loss:-9.668
step:1/417 avg loss:-13.206, stage_1_loss:-13.028, stage_2_loss:-13.250
step:301/417 avg loss:-11.035, stage_1_loss:-10.901, stage_2_loss:-11.069
step:301/417 avg loss:-11.141, stage_1_loss:-11.025, stage_2_loss:-11.170
Valid Summary | End of Epoch 72 | Time 805.96s | Current time 2025-01-21 18:23:31.296703 |Valid Loss -11.062| 
step:1/1667 avg loss:-13.527, stage_1_loss:-13.427, stage_2_loss:-13.553
step:1/1667 avg loss:-12.543, stage_1_loss:-12.170, stage_2_loss:-12.637
step:1001/1667 avg loss:-11.783, stage_1_loss:-11.623, stage_2_loss:-11.822
step:1001/1667 avg loss:-11.884, stage_1_loss:-11.728, stage_2_loss:-11.923
Train Summary | End of Epoch 73 | Time 4260.69s | Current time 2025-01-21 19:34:34.745657 |Train Loss -11.815| 
step:1/417 avg loss:-10.254, stage_1_loss:-9.858, stage_2_loss:-10.353
step:1/417 avg loss:-13.420, stage_1_loss:-13.317, stage_2_loss:-13.446
step:301/417 avg loss:-11.190, stage_1_loss:-11.064, stage_2_loss:-11.221
step:301/417 avg loss:-11.090, stage_1_loss:-10.972, stage_2_loss:-11.120
Valid Summary | End of Epoch 73 | Time 800.31s | Current time 2025-01-21 19:47:55.057974 |Valid Loss -11.091| 
step:1/1667 avg loss:-12.572, stage_1_loss:-11.955, stage_2_loss:-12.726
step:1/1667 avg loss:-13.022, stage_1_loss:-12.836, stage_2_loss:-13.069
step:1001/1667 avg loss:-11.774, stage_1_loss:-11.614, stage_2_loss:-11.815
step:1001/1667 avg loss:-11.711, stage_1_loss:-11.531, stage_2_loss:-11.757
Train Summary | End of Epoch 74 | Time 4213.52s | Current time 2025-01-21 20:58:11.115540 |Train Loss -11.669| 
step:1/417 avg loss:-10.087, stage_1_loss:-9.601, stage_2_loss:-10.209
step:1/417 avg loss:-13.369, stage_1_loss:-13.128, stage_2_loss:-13.429
step:301/417 avg loss:-10.960, stage_1_loss:-10.838, stage_2_loss:-10.991
step:301/417 avg loss:-11.008, stage_1_loss:-10.900, stage_2_loss:-11.035
Valid Summary | End of Epoch 74 | Time 788.27s | Current time 2025-01-21 21:11:19.383870 |Valid Loss -10.933| 
step:1/1667 avg loss:-11.738, stage_1_loss:-11.451, stage_2_loss:-11.810
step:1/1667 avg loss:-9.964, stage_1_loss:-9.722, stage_2_loss:-10.024
step:1001/1667 avg loss:-11.722, stage_1_loss:-11.554, stage_2_loss:-11.764
step:1001/1667 avg loss:-11.639, stage_1_loss:-11.478, stage_2_loss:-11.679
Train Summary | End of Epoch 75 | Time 4358.69s | Current time 2025-01-21 22:24:00.123127 |Train Loss -11.708| 
step:1/417 avg loss:-9.573, stage_1_loss:-10.203, stage_2_loss:-9.416
step:1/417 avg loss:-9.976, stage_1_loss:-9.357, stage_2_loss:-10.130
step:301/417 avg loss:-10.516, stage_1_loss:-10.405, stage_2_loss:-10.544
step:301/417 avg loss:-10.545, stage_1_loss:-10.454, stage_2_loss:-10.567
Valid Summary | End of Epoch 75 | Time 791.04s | Current time 2025-01-21 22:37:11.164147 |Valid Loss -10.451| 
step:1/1667 avg loss:-11.304, stage_1_loss:-11.149, stage_2_loss:-11.342
step:1/1667 avg loss:-9.236, stage_1_loss:-9.379, stage_2_loss:-9.201
step:1001/1667 avg loss:-11.723, stage_1_loss:-11.538, stage_2_loss:-11.770
step:1001/1667 avg loss:-11.731, stage_1_loss:-11.564, stage_2_loss:-11.772
Train Summary | End of Epoch 76 | Time 4132.26s | Current time 2025-01-21 23:46:05.273367 |Train Loss -11.788| 
step:1/417 avg loss:-9.146, stage_1_loss:-9.161, stage_2_loss:-9.142
step:1/417 avg loss:-13.250, stage_1_loss:-13.081, stage_2_loss:-13.292
step:301/417 avg loss:-11.103, stage_1_loss:-11.000, stage_2_loss:-11.129
step:301/417 avg loss:-11.080, stage_1_loss:-10.941, stage_2_loss:-11.115
Valid Summary | End of Epoch 76 | Time 793.14s | Current time 2025-01-21 23:59:18.414383 |Valid Loss -11.057| 
step:1/1667 avg loss:-12.927, stage_1_loss:-12.845, stage_2_loss:-12.948
step:1/1667 avg loss:-10.482, stage_1_loss:-10.436, stage_2_loss:-10.494
step:1001/1667 avg loss:-12.002, stage_1_loss:-11.860, stage_2_loss:-12.037
step:1001/1667 avg loss:-11.937, stage_1_loss:-11.776, stage_2_loss:-11.977
Train Summary | End of Epoch 77 | Time 4136.51s | Current time 2025-01-22 01:08:16.782587 |Train Loss -11.894| 
step:1/417 avg loss:-9.834, stage_1_loss:-9.569, stage_2_loss:-9.901
step:1/417 avg loss:-12.488, stage_1_loss:-11.835, stage_2_loss:-12.652
step:301/417 avg loss:-10.252, stage_1_loss:-10.155, stage_2_loss:-10.276
step:301/417 avg loss:-10.441, stage_1_loss:-10.340, stage_2_loss:-10.466
Valid Summary | End of Epoch 77 | Time 790.31s | Current time 2025-01-22 01:21:27.109171 |Valid Loss -10.303| 
Learning rate adjusted to: 0.000125
step:1/1667 avg loss:-11.850, stage_1_loss:-11.856, stage_2_loss:-11.848
step:1/1667 avg loss:-11.115, stage_1_loss:-11.074, stage_2_loss:-11.126
step:1001/1667 avg loss:-11.870, stage_1_loss:-11.715, stage_2_loss:-11.909
step:1001/1667 avg loss:-11.792, stage_1_loss:-11.633, stage_2_loss:-11.832
Train Summary | End of Epoch 78 | Time 4152.36s | Current time 2025-01-22 02:30:42.383919 |Train Loss -11.906| 
step:1/417 avg loss:-10.365, stage_1_loss:-9.679, stage_2_loss:-10.537
step:1/417 avg loss:-13.387, stage_1_loss:-13.212, stage_2_loss:-13.430
step:301/417 avg loss:-11.108, stage_1_loss:-11.005, stage_2_loss:-11.134
step:301/417 avg loss:-11.219, stage_1_loss:-11.122, stage_2_loss:-11.243
Valid Summary | End of Epoch 78 | Time 792.94s | Current time 2025-01-22 02:43:55.323264 |Valid Loss -11.115| 
step:1/1667 avg loss:-10.508, stage_1_loss:-10.486, stage_2_loss:-10.514
step:1/1667 avg loss:-11.106, stage_1_loss:-10.977, stage_2_loss:-11.139
step:1001/1667 avg loss:-12.061, stage_1_loss:-11.915, stage_2_loss:-12.098
step:1001/1667 avg loss:-12.170, stage_1_loss:-12.030, stage_2_loss:-12.205
Train Summary | End of Epoch 79 | Time 4143.35s | Current time 2025-01-22 03:53:01.678332 |Train Loss -12.096| 
step:1/417 avg loss:-10.355, stage_1_loss:-9.671, stage_2_loss:-10.526
step:1/417 avg loss:-13.559, stage_1_loss:-13.509, stage_2_loss:-13.571
step:301/417 avg loss:-11.170, stage_1_loss:-11.054, stage_2_loss:-11.199
step:301/417 avg loss:-11.262, stage_1_loss:-11.146, stage_2_loss:-11.290
Valid Summary | End of Epoch 79 | Time 794.23s | Current time 2025-01-22 04:06:15.913398 |Valid Loss -11.155| 
Found new best model, dict saved
step:1/1667 avg loss:-11.834, stage_1_loss:-11.646, stage_2_loss:-11.881
step:1/1667 avg loss:-13.288, stage_1_loss:-13.190, stage_2_loss:-13.312
step:1001/1667 avg loss:-12.159, stage_1_loss:-12.008, stage_2_loss:-12.197
step:1001/1667 avg loss:-12.018, stage_1_loss:-11.867, stage_2_loss:-12.056
Train Summary | End of Epoch 80 | Time 4177.81s | Current time 2025-01-22 05:15:59.457506 |Train Loss -12.100| 
step:1/417 avg loss:-9.686, stage_1_loss:-9.530, stage_2_loss:-9.725
step:1/417 avg loss:-13.568, stage_1_loss:-13.465, stage_2_loss:-13.594
step:301/417 avg loss:-11.136, stage_1_loss:-11.003, stage_2_loss:-11.170
step:301/417 avg loss:-11.269, stage_1_loss:-11.150, stage_2_loss:-11.299
Valid Summary | End of Epoch 80 | Time 794.65s | Current time 2025-01-22 05:29:14.108686 |Valid Loss -11.150| 
step:1/1667 avg loss:-11.967, stage_1_loss:-11.805, stage_2_loss:-12.007
step:1/1667 avg loss:-13.531, stage_1_loss:-13.400, stage_2_loss:-13.564
step:1001/1667 avg loss:-12.014, stage_1_loss:-11.849, stage_2_loss:-12.055
step:1001/1667 avg loss:-12.195, stage_1_loss:-12.038, stage_2_loss:-12.234
Train Summary | End of Epoch 81 | Time 4319.96s | Current time 2025-01-22 06:41:19.495677 |Train Loss -12.102| 
step:1/417 avg loss:-10.096, stage_1_loss:-9.914, stage_2_loss:-10.142
step:1/417 avg loss:-13.564, stage_1_loss:-13.463, stage_2_loss:-13.589
step:301/417 avg loss:-11.195, stage_1_loss:-11.074, stage_2_loss:-11.225
step:301/417 avg loss:-11.275, stage_1_loss:-11.152, stage_2_loss:-11.305
Valid Summary | End of Epoch 81 | Time 792.28s | Current time 2025-01-22 06:54:31.772821 |Valid Loss -11.180| 
Found new best model, dict saved
step:1/1667 avg loss:-11.162, stage_1_loss:-11.016, stage_2_loss:-11.198
step:1/1667 avg loss:-11.189, stage_1_loss:-10.839, stage_2_loss:-11.277
step:1001/1667 avg loss:-12.198, stage_1_loss:-12.054, stage_2_loss:-12.234
step:1001/1667 avg loss:-12.061, stage_1_loss:-11.908, stage_2_loss:-12.100
Train Summary | End of Epoch 82 | Time 4321.72s | Current time 2025-01-22 08:06:39.179245 |Train Loss -12.156| 
step:1/417 avg loss:-9.722, stage_1_loss:-9.507, stage_2_loss:-9.775
step:1/417 avg loss:-13.540, stage_1_loss:-13.564, stage_2_loss:-13.534
step:301/417 avg loss:-11.167, stage_1_loss:-11.058, stage_2_loss:-11.194
step:301/417 avg loss:-11.289, stage_1_loss:-11.173, stage_2_loss:-11.317
Valid Summary | End of Epoch 82 | Time 789.12s | Current time 2025-01-22 08:19:48.297898 |Valid Loss -11.173| 
step:1/1667 avg loss:-11.619, stage_1_loss:-11.518, stage_2_loss:-11.644
step:1/1667 avg loss:-12.465, stage_1_loss:-12.346, stage_2_loss:-12.494
step:1001/1667 avg loss:-12.092, stage_1_loss:-11.939, stage_2_loss:-12.130
step:1001/1667 avg loss:-12.233, stage_1_loss:-12.093, stage_2_loss:-12.268
Train Summary | End of Epoch 83 | Time 4123.38s | Current time 2025-01-22 09:28:34.503315 |Train Loss -12.171| 
step:1/417 avg loss:-9.742, stage_1_loss:-9.623, stage_2_loss:-9.772
step:1/417 avg loss:-13.489, stage_1_loss:-13.482, stage_2_loss:-13.491
step:301/417 avg loss:-11.242, stage_1_loss:-11.126, stage_2_loss:-11.271
step:301/417 avg loss:-11.310, stage_1_loss:-11.191, stage_2_loss:-11.339
Valid Summary | End of Epoch 83 | Time 793.84s | Current time 2025-01-22 09:41:48.342214 |Valid Loss -11.222| 
Found new best model, dict saved
step:1/1667 avg loss:-12.190, stage_1_loss:-12.215, stage_2_loss:-12.184
step:1/1667 avg loss:-12.387, stage_1_loss:-12.277, stage_2_loss:-12.414
step:1001/1667 avg loss:-12.260, stage_1_loss:-12.122, stage_2_loss:-12.294
step:1001/1667 avg loss:-12.089, stage_1_loss:-11.946, stage_2_loss:-12.125
Train Summary | End of Epoch 84 | Time 4110.30s | Current time 2025-01-22 10:50:23.167103 |Train Loss -12.168| 
step:1/417 avg loss:-9.697, stage_1_loss:-9.566, stage_2_loss:-9.730
step:1/417 avg loss:-13.508, stage_1_loss:-13.451, stage_2_loss:-13.522
step:301/417 avg loss:-11.222, stage_1_loss:-11.101, stage_2_loss:-11.252
step:301/417 avg loss:-11.324, stage_1_loss:-11.196, stage_2_loss:-11.356
Valid Summary | End of Epoch 84 | Time 784.74s | Current time 2025-01-22 11:03:27.907157 |Valid Loss -11.210| 
step:1/1667 avg loss:-11.560, stage_1_loss:-11.459, stage_2_loss:-11.586
step:1/1667 avg loss:-11.854, stage_1_loss:-11.701, stage_2_loss:-11.892
step:1001/1667 avg loss:-12.279, stage_1_loss:-12.133, stage_2_loss:-12.315
step:1001/1667 avg loss:-12.155, stage_1_loss:-12.007, stage_2_loss:-12.193
Train Summary | End of Epoch 85 | Time 4150.20s | Current time 2025-01-22 12:12:39.471669 |Train Loss -12.207| 
step:1/417 avg loss:-9.859, stage_1_loss:-9.694, stage_2_loss:-9.900
step:1/417 avg loss:-13.505, stage_1_loss:-13.533, stage_2_loss:-13.498
step:301/417 avg loss:-11.198, stage_1_loss:-11.076, stage_2_loss:-11.228
step:301/417 avg loss:-11.309, stage_1_loss:-11.181, stage_2_loss:-11.341
Valid Summary | End of Epoch 85 | Time 795.01s | Current time 2025-01-22 12:25:54.484220 |Valid Loss -11.179| 
step:1/1667 avg loss:-13.643, stage_1_loss:-13.534, stage_2_loss:-13.670
step:1/1667 avg loss:-11.771, stage_1_loss:-11.710, stage_2_loss:-11.786
step:1001/1667 avg loss:-12.209, stage_1_loss:-12.069, stage_2_loss:-12.244
step:1001/1667 avg loss:-12.071, stage_1_loss:-11.925, stage_2_loss:-12.108
Train Summary | End of Epoch 86 | Time 4127.27s | Current time 2025-01-22 13:34:43.168533 |Train Loss -12.171| 
step:1/417 avg loss:-10.102, stage_1_loss:-9.816, stage_2_loss:-10.174
step:1/417 avg loss:-13.530, stage_1_loss:-13.439, stage_2_loss:-13.553
step:301/417 avg loss:-11.210, stage_1_loss:-11.094, stage_2_loss:-11.239
step:301/417 avg loss:-11.347, stage_1_loss:-11.212, stage_2_loss:-11.381
Valid Summary | End of Epoch 86 | Time 791.09s | Current time 2025-01-22 13:47:54.264931 |Valid Loss -11.203| 
step:1/1667 avg loss:-12.753, stage_1_loss:-12.398, stage_2_loss:-12.841
step:1/1667 avg loss:-10.859, stage_1_loss:-10.742, stage_2_loss:-10.888
step:1001/1667 avg loss:-12.299, stage_1_loss:-12.159, stage_2_loss:-12.334
step:1001/1667 avg loss:-12.196, stage_1_loss:-12.050, stage_2_loss:-12.232
Train Summary | End of Epoch 87 | Time 4119.66s | Current time 2025-01-22 14:56:36.900755 |Train Loss -12.235| 
step:1/417 avg loss:-13.492, stage_1_loss:-13.416, stage_2_loss:-13.511
step:1/417 avg loss:-9.792, stage_1_loss:-9.458, stage_2_loss:-9.875
step:301/417 avg loss:-11.254, stage_1_loss:-11.122, stage_2_loss:-11.287
step:301/417 avg loss:-11.331, stage_1_loss:-11.210, stage_2_loss:-11.361
Valid Summary | End of Epoch 87 | Time 796.11s | Current time 2025-01-22 15:09:53.010616 |Valid Loss -11.242| 
Found new best model, dict saved
step:1/1667 avg loss:-11.053, stage_1_loss:-11.011, stage_2_loss:-11.064
step:1/1667 avg loss:-13.880, stage_1_loss:-13.850, stage_2_loss:-13.887
step:1001/1667 avg loss:-12.158, stage_1_loss:-12.004, stage_2_loss:-12.196
step:1001/1667 avg loss:-12.321, stage_1_loss:-12.176, stage_2_loss:-12.357
Train Summary | End of Epoch 88 | Time 4111.61s | Current time 2025-01-22 16:18:28.628677 |Train Loss -12.240| 
step:1/417 avg loss:-9.823, stage_1_loss:-9.575, stage_2_loss:-9.885
step:1/417 avg loss:-13.511, stage_1_loss:-13.473, stage_2_loss:-13.520
step:301/417 avg loss:-11.331, stage_1_loss:-11.197, stage_2_loss:-11.365
step:301/417 avg loss:-11.266, stage_1_loss:-11.140, stage_2_loss:-11.298
Valid Summary | End of Epoch 88 | Time 798.54s | Current time 2025-01-22 16:31:47.166240 |Valid Loss -11.229| 
step:1/1667 avg loss:-10.438, stage_1_loss:-10.359, stage_2_loss:-10.458
step:1/1667 avg loss:-12.032, stage_1_loss:-12.021, stage_2_loss:-12.035
step:1001/1667 avg loss:-12.314, stage_1_loss:-12.168, stage_2_loss:-12.351
step:1001/1667 avg loss:-12.244, stage_1_loss:-12.085, stage_2_loss:-12.283
Train Summary | End of Epoch 89 | Time 4121.23s | Current time 2025-01-22 17:40:29.776646 |Train Loss -12.235| 
step:1/417 avg loss:-9.851, stage_1_loss:-9.672, stage_2_loss:-9.896
step:1/417 avg loss:-13.526, stage_1_loss:-13.513, stage_2_loss:-13.530
step:301/417 avg loss:-11.222, stage_1_loss:-11.087, stage_2_loss:-11.256
step:301/417 avg loss:-11.295, stage_1_loss:-11.176, stage_2_loss:-11.324
Valid Summary | End of Epoch 89 | Time 794.09s | Current time 2025-01-22 17:53:43.864676 |Valid Loss -11.207| 
step:1/1667 avg loss:-11.204, stage_1_loss:-11.092, stage_2_loss:-11.231
step:1/1667 avg loss:-13.250, stage_1_loss:-13.042, stage_2_loss:-13.303
step:1001/1667 avg loss:-12.189, stage_1_loss:-12.038, stage_2_loss:-12.227
step:1001/1667 avg loss:-12.302, stage_1_loss:-12.166, stage_2_loss:-12.336
Train Summary | End of Epoch 90 | Time 4181.75s | Current time 2025-01-22 19:03:27.095209 |Train Loss -12.191| 
step:1/417 avg loss:-9.938, stage_1_loss:-9.852, stage_2_loss:-9.959
step:1/417 avg loss:-13.442, stage_1_loss:-13.262, stage_2_loss:-13.487
step:301/417 avg loss:-11.022, stage_1_loss:-10.924, stage_2_loss:-11.046
step:301/417 avg loss:-11.085, stage_1_loss:-10.988, stage_2_loss:-11.109
Valid Summary | End of Epoch 90 | Time 793.06s | Current time 2025-01-22 19:16:40.153142 |Valid Loss -10.998| 
step:1/1667 avg loss:-12.243, stage_1_loss:-12.250, stage_2_loss:-12.241
step:1/1667 avg loss:-12.749, stage_1_loss:-12.565, stage_2_loss:-12.795
step:1001/1667 avg loss:-12.314, stage_1_loss:-12.169, stage_2_loss:-12.350
step:1001/1667 avg loss:-12.186, stage_1_loss:-12.040, stage_2_loss:-12.222
Train Summary | End of Epoch 91 | Time 4106.65s | Current time 2025-01-22 20:25:10.884493 |Train Loss -12.264| 
step:1/417 avg loss:-13.557, stage_1_loss:-13.490, stage_2_loss:-13.574
step:1/417 avg loss:-9.784, stage_1_loss:-9.616, stage_2_loss:-9.825
step:301/417 avg loss:-11.201, stage_1_loss:-11.085, stage_2_loss:-11.230
step:301/417 avg loss:-11.142, stage_1_loss:-11.034, stage_2_loss:-11.169
Valid Summary | End of Epoch 91 | Time 790.20s | Current time 2025-01-22 20:38:21.083365 |Valid Loss -11.111| 
step:1/1667 avg loss:-12.827, stage_1_loss:-12.588, stage_2_loss:-12.886
step:1/1667 avg loss:-13.592, stage_1_loss:-13.580, stage_2_loss:-13.595
step:1001/1667 avg loss:-12.357, stage_1_loss:-12.217, stage_2_loss:-12.392
step:1001/1667 avg loss:-12.244, stage_1_loss:-12.094, stage_2_loss:-12.282
