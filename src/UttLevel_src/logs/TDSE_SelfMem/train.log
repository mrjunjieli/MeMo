started on logs/TDSE_SelfMem/Teacher+scale+shift_0_1

Namespace(mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', batch_size=6, max_length=6, num_workers=6, epochs=100, effec_batch_size=8, accu_grad=0, C=2, model_name='TDSE_SelfMem', lr=0.001, max_norm=5, log_name='logs/TDSE_SelfMem/Teacher+scale+shift_0_1', use_tensorboard=1, continue_from='FALSE', local_rank=0, sample_rate=16000, win=512, hop_length=128, n_mels=80, shift_range='0,1', Self_enroll_amplitude_scaling='0.1,1', teacher_point=50, num_slots=5, distributed=True, world_size=2, speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800)

Total number of parameters: 23335173 

av_convtasnet(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): TemporalConvNet(
    (layer_norm): ChannelWiseLayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (bottleneck_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (tcn): ModuleList(
      (0-3): 4 x Sequential(
        (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (2): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (3): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (4): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (5): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (6): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (7): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (8): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
      )
    )
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (ve_conv1x1): ModuleList(
      (0-3): 4 x Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
    )
    (mask_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (time_cross_attn): CrossAttention(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
    )
    (linear1): Linear(in_features=256, out_features=1024, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=1024, out_features=256, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (slot_attn): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Start new training
step:1/1667 avg loss:45.046
step:1/1667 avg loss:40.949
step:1001/1667 avg loss:-0.116step:1001/1667 avg loss:-0.099

Train Summary | End of Epoch 1 | Time 2608.81s | Current time 2024-11-26 21:31:35.022438 |Train Loss -0.597| 
step:1/417 avg loss:-0.386
step:1/417 avg loss:-0.270
step:301/417 avg loss:-0.036
step:301/417 avg loss:-0.057
Valid Summary | End of Epoch 1 | Time 401.26s | Current time 2024-11-26 21:38:16.406045 |Valid Loss -0.062| 
Found new best model, dict saved
step:1/1667 avg loss:-1.984
step:1/1667 avg loss:-2.416
step:1001/1667 avg loss:-1.737step:1001/1667 avg loss:-1.759

Train Summary | End of Epoch 2 | Time 2553.58s | Current time 2024-11-26 22:20:51.633876 |Train Loss -1.942| 
step:1/417 avg loss:-0.402
step:1/417 avg loss:-0.222
step:301/417 avg loss:-0.267
step:301/417 avg loss:-0.281
Valid Summary | End of Epoch 2 | Time 393.68s | Current time 2024-11-26 22:27:25.327875 |Valid Loss -0.281| 
Found new best model, dict saved
step:1/1667 avg loss:-2.156
step:1/1667 avg loss:-3.051
step:1001/1667 avg loss:-2.944step:1001/1667 avg loss:-2.950

Train Summary | End of Epoch 3 | Time 2558.40s | Current time 2024-11-26 23:10:04.644627 |Train Loss -3.262| 
step:1/417 avg loss:-2.471
step:1/417 avg loss:-0.243
step:301/417 avg loss:-1.380
step:301/417 avg loss:-1.415
Valid Summary | End of Epoch 3 | Time 391.62s | Current time 2024-11-26 23:16:36.283335 |Valid Loss -1.389| 
Found new best model, dict saved
step:1/1667 avg loss:-3.804
step:1/1667 avg loss:-4.019
step:1001/1667 avg loss:-4.531step:1001/1667 avg loss:-4.527

Train Summary | End of Epoch 4 | Time 2525.49s | Current time 2024-11-26 23:58:42.830087 |Train Loss -4.806| 
step:1/417 avg loss:-3.109
step:1/417 avg loss:-0.911
step:301/417 avg loss:-1.989
step:301/417 avg loss:-1.974
Valid Summary | End of Epoch 4 | Time 390.63s | Current time 2024-11-27 00:05:13.474634 |Valid Loss -1.974| 
Found new best model, dict saved
step:1/1667 avg loss:-5.941
step:1/1667 avg loss:-3.937
step:1001/1667 avg loss:-5.816step:1001/1667 avg loss:-5.806

Train Summary | End of Epoch 5 | Time 2546.19s | Current time 2024-11-27 00:47:40.783902 |Train Loss -6.022| 
step:1/417 avg loss:-4.958
step:1/417 avg loss:-2.522
step:301/417 avg loss:-3.406
step:301/417 avg loss:-3.479
Valid Summary | End of Epoch 5 | Time 389.54s | Current time 2024-11-27 00:54:10.329893 |Valid Loss -3.483| 
Found new best model, dict saved
step:1/1667 avg loss:-6.836
step:1/1667 avg loss:-7.777
step:1001/1667 avg loss:-6.793step:1001/1667 avg loss:-6.774

Train Summary | End of Epoch 6 | Time 2539.99s | Current time 2024-11-27 01:36:32.085227 |Train Loss -6.957| 
step:1/417 avg loss:-5.164
step:1/417 avg loss:-3.176
step:301/417 avg loss:-4.475
step:301/417 avg loss:-4.544
Valid Summary | End of Epoch 6 | Time 390.84s | Current time 2024-11-27 01:43:02.970976 |Valid Loss -4.553| 
Found new best model, dict saved
step:1/1667 avg loss:-6.187
step:1/1667 avg loss:-4.439
step:1001/1667 avg loss:-7.648step:1001/1667 avg loss:-7.614

Train Summary | End of Epoch 7 | Time 2546.09s | Current time 2024-11-27 02:25:30.911960 |Train Loss -7.803| 
step:1/417 avg loss:-7.660
step:1/417 avg loss:-4.410
step:301/417 avg loss:-6.284
step:301/417 avg loss:-6.278
Valid Summary | End of Epoch 7 | Time 388.35s | Current time 2024-11-27 02:31:59.379191 |Valid Loss -6.295| 
Found new best model, dict saved
step:1/1667 avg loss:-7.549
step:1/1667 avg loss:-8.548
step:1001/1667 avg loss:-8.323step:1001/1667 avg loss:-8.361

Train Summary | End of Epoch 8 | Time 2534.62s | Current time 2024-11-27 03:14:15.176034 |Train Loss -8.475| 
step:1/417 avg loss:-4.660
step:1/417 avg loss:-1.194
step:301/417 avg loss:-4.567
step:301/417 avg loss:-4.600
Valid Summary | End of Epoch 8 | Time 399.16s | Current time 2024-11-27 03:20:54.413266 |Valid Loss -4.625| 
step:1/1667 avg loss:-9.030
step:1/1667 avg loss:-7.723
step:1001/1667 avg loss:-8.908step:1001/1667 avg loss:-8.881

Train Summary | End of Epoch 9 | Time 2551.25s | Current time 2024-11-27 04:03:26.460598 |Train Loss -8.972| 
step:1/417 avg loss:-8.187
step:1/417 avg loss:-5.189
step:301/417 avg loss:-7.494
step:301/417 avg loss:-7.466
Valid Summary | End of Epoch 9 | Time 388.05s | Current time 2024-11-27 04:09:54.513881 |Valid Loss -7.484| 
Found new best model, dict saved
step:1/1667 avg loss:-7.693
step:1/1667 avg loss:-8.588
step:1001/1667 avg loss:-9.531
step:1001/1667 avg loss:-9.525
Train Summary | End of Epoch 10 | Time 2544.22s | Current time 2024-11-27 04:52:20.041055 |Train Loss -9.456| 
step:1/417 avg loss:-8.698
step:1/417 avg loss:-7.089
step:301/417 avg loss:-8.326
step:301/417 avg loss:-8.256
Valid Summary | End of Epoch 10 | Time 390.59s | Current time 2024-11-27 04:58:51.068885 |Valid Loss -8.335| 
Found new best model, dict saved
step:1/1667 avg loss:-10.956
step:1/1667 avg loss:-10.032
step:1001/1667 avg loss:-10.043step:1001/1667 avg loss:-10.050

Train Summary | End of Epoch 11 | Time 2547.78s | Current time 2024-11-27 05:41:21.001916 |Train Loss -10.083| 
step:1/417 avg loss:-9.667
step:1/417 avg loss:-6.950
step:301/417 avg loss:-8.898
step:301/417 avg loss:-8.955
Valid Summary | End of Epoch 11 | Time 388.90s | Current time 2024-11-27 05:47:49.983938 |Valid Loss -8.982| 
Found new best model, dict saved
step:1/1667 avg loss:-9.476
step:1/1667 avg loss:-10.719
step:1001/1667 avg loss:-10.309step:1001/1667 avg loss:-10.325

Train Summary | End of Epoch 12 | Time 2540.36s | Current time 2024-11-27 06:30:11.717960 |Train Loss -10.265| 
step:1/417 avg loss:-9.580
step:1/417 avg loss:-7.791
step:301/417 avg loss:-9.224
step:301/417 avg loss:-9.176
Valid Summary | End of Epoch 12 | Time 390.31s | Current time 2024-11-27 06:36:42.109931 |Valid Loss -9.248| 
Found new best model, dict saved
step:1/1667 avg loss:-10.183
step:1/1667 avg loss:-11.112
step:1001/1667 avg loss:-10.500step:1001/1667 avg loss:-10.515

Train Summary | End of Epoch 13 | Time 2538.29s | Current time 2024-11-27 07:19:02.026134 |Train Loss -10.498| 
step:1/417 avg loss:-10.053
step:1/417 avg loss:-8.845
step:301/417 avg loss:-9.248
step:301/417 avg loss:-9.184
Valid Summary | End of Epoch 13 | Time 389.06s | Current time 2024-11-27 07:25:31.158273 |Valid Loss -9.264| 
Found new best model, dict saved
step:1/1667 avg loss:-11.619
step:1/1667 avg loss:-10.727
step:1001/1667 avg loss:-10.935step:1001/1667 avg loss:-10.980

Train Summary | End of Epoch 14 | Time 2546.29s | Current time 2024-11-27 08:07:58.506190 |Train Loss -10.790| 
step:1/417 avg loss:-10.237
step:1/417 avg loss:-8.150
step:301/417 avg loss:-9.133
step:301/417 avg loss:-9.082
Valid Summary | End of Epoch 14 | Time 391.58s | Current time 2024-11-27 08:14:30.597208 |Valid Loss -9.149| 
step:1/1667 avg loss:-11.213
step:1/1667 avg loss:-10.943
step:1001/1667 avg loss:-11.128step:1001/1667 avg loss:-11.109

Train Summary | End of Epoch 15 | Time 2537.12s | Current time 2024-11-27 08:56:49.054758 |Train Loss -11.134| 
step:1/417 avg loss:-10.121
step:1/417 avg loss:-9.139
step:301/417 avg loss:-9.645
step:301/417 avg loss:-9.513
Valid Summary | End of Epoch 15 | Time 400.30s | Current time 2024-11-27 09:03:29.828012 |Valid Loss -9.654| 
Found new best model, dict saved
step:1/1667 avg loss:-10.722
step:1/1667 avg loss:-11.263
step:1001/1667 avg loss:-11.042
step:1001/1667 avg loss:-11.074
Train Summary | End of Epoch 16 | Time 2553.66s | Current time 2024-11-27 09:46:04.536299 |Train Loss -11.128| 
step:1/417 avg loss:-9.922
step:1/417 avg loss:-8.188
step:301/417 avg loss:-9.718
step:301/417 avg loss:-9.633
Valid Summary | End of Epoch 16 | Time 394.01s | Current time 2024-11-27 09:52:38.576678 |Valid Loss -9.723| 
Found new best model, dict saved
step:1/1667 avg loss:-10.265
step:1/1667 avg loss:-11.591
step:1001/1667 avg loss:-11.482step:1001/1667 avg loss:-11.503

Train Summary | End of Epoch 17 | Time 2542.39s | Current time 2024-11-27 10:35:03.335250 |Train Loss -11.301| 
step:1/417 avg loss:-7.256
step:1/417 avg loss:-6.886
step:301/417 avg loss:-7.925
step:301/417 avg loss:-7.872
Valid Summary | End of Epoch 17 | Time 392.40s | Current time 2024-11-27 10:41:35.934070 |Valid Loss -7.963| 
step:1/1667 avg loss:-8.156
step:1/1667 avg loss:-7.818
step:1001/1667 avg loss:-11.244step:1001/1667 avg loss:-11.213

Train Summary | End of Epoch 18 | Time 2585.79s | Current time 2024-11-27 11:24:42.441903 |Train Loss -11.386| 
step:1/417 avg loss:-10.572
step:1/417 avg loss:-8.407
step:301/417 avg loss:-10.146
step:301/417 avg loss:-10.011
Valid Summary | End of Epoch 18 | Time 399.10s | Current time 2024-11-27 11:31:21.986856 |Valid Loss -10.128| 
Found new best model, dict saved
step:1/1667 avg loss:-12.538
step:1/1667 avg loss:-13.055
step:1001/1667 avg loss:-11.716step:1001/1667 avg loss:-11.742

Train Summary | End of Epoch 19 | Time 2582.30s | Current time 2024-11-27 12:14:25.280592 |Train Loss -11.504| 
step:1/417 avg loss:-10.246
step:1/417 avg loss:-7.520
step:301/417 avg loss:-10.286
step:301/417 avg loss:-10.183
Valid Summary | End of Epoch 19 | Time 400.18s | Current time 2024-11-27 12:21:05.558495 |Valid Loss -10.272| 
Found new best model, dict saved
step:1/1667 avg loss:-12.182
step:1/1667 avg loss:-11.524
step:1001/1667 avg loss:-11.513step:1001/1667 avg loss:-11.516

Train Summary | End of Epoch 20 | Time 2586.62s | Current time 2024-11-27 13:04:13.539266 |Train Loss -11.554| 
step:1/417 avg loss:-11.118
step:1/417 avg loss:-9.706
step:301/417 avg loss:-10.357
step:301/417 avg loss:-10.298
Valid Summary | End of Epoch 20 | Time 398.96s | Current time 2024-11-27 13:10:52.549568 |Valid Loss -10.366| 
Found new best model, dict saved
step:1/1667 avg loss:-11.286
step:1/1667 avg loss:-9.966
step:1001/1667 avg loss:-11.682step:1001/1667 avg loss:-11.692

Train Summary | End of Epoch 21 | Time 2576.65s | Current time 2024-11-27 13:53:51.877559 |Train Loss -11.679| 
step:1/417 avg loss:-11.267
step:1/417 avg loss:-9.930
step:301/417 avg loss:-10.605
step:301/417 avg loss:-10.498
Valid Summary | End of Epoch 21 | Time 398.67s | Current time 2024-11-27 14:00:30.861720 |Valid Loss -10.596| 
Found new best model, dict saved
step:1/1667 avg loss:-12.253
step:1/1667 avg loss:-13.076
step:1001/1667 avg loss:-12.109step:1001/1667 avg loss:-12.129

Train Summary | End of Epoch 22 | Time 2569.49s | Current time 2024-11-27 14:43:21.990483 |Train Loss -12.060| 
step:1/417 avg loss:-10.829
step:1/417 avg loss:-7.877
step:301/417 avg loss:-10.087
step:301/417 avg loss:-9.832
Valid Summary | End of Epoch 22 | Time 396.60s | Current time 2024-11-27 14:49:58.633115 |Valid Loss -10.028| 
step:1/1667 avg loss:-11.888
step:1/1667 avg loss:-12.822
step:1001/1667 avg loss:-11.914step:1001/1667 avg loss:-11.921

Train Summary | End of Epoch 23 | Time 2567.02s | Current time 2024-11-27 15:32:46.308051 |Train Loss -11.977| 
step:1/417 avg loss:-10.907
step:1/417 avg loss:-9.833
step:301/417 avg loss:-10.398
step:301/417 avg loss:-10.383
Valid Summary | End of Epoch 23 | Time 409.31s | Current time 2024-11-27 15:39:35.662198 |Valid Loss -10.431| 
step:1/1667 avg loss:-13.686
step:1/1667 avg loss:-12.462
step:1001/1667 avg loss:-12.245step:1001/1667 avg loss:-12.239

Train Summary | End of Epoch 24 | Time 2567.70s | Current time 2024-11-27 16:22:23.855214 |Train Loss -12.193| 
step:1/417 avg loss:-9.767
step:1/417 avg loss:-6.334
step:301/417 avg loss:-10.110
step:301/417 avg loss:-9.904
Valid Summary | End of Epoch 24 | Time 395.82s | Current time 2024-11-27 16:28:59.808511 |Valid Loss -10.064| 
step:1/1667 avg loss:-13.116
step:1/1667 avg loss:-12.355
step:1001/1667 avg loss:-12.087step:1001/1667 avg loss:-12.137

Train Summary | End of Epoch 25 | Time 2566.31s | Current time 2024-11-27 17:11:48.150645 |Train Loss -12.152| 
step:1/417 avg loss:-11.163
step:1/417 avg loss:-10.088
step:301/417 avg loss:-10.323
step:301/417 avg loss:-10.273
Valid Summary | End of Epoch 25 | Time 396.22s | Current time 2024-11-27 17:18:24.382893 |Valid Loss -10.354| 
step:1/1667 avg loss:-12.948
step:1/1667 avg loss:-11.088
step:1001/1667 avg loss:-12.013step:1001/1667 avg loss:-11.999

Train Summary | End of Epoch 26 | Time 2578.21s | Current time 2024-11-27 18:01:23.470728 |Train Loss -12.061| 
step:1/417 avg loss:-8.488
step:1/417 avg loss:-5.147
step:301/417 avg loss:-6.959
step:301/417 avg loss:-6.879
Valid Summary | End of Epoch 26 | Time 399.02s | Current time 2024-11-27 18:08:02.519331 |Valid Loss -7.047| 
step:1/1667 avg loss:-5.852
step:1/1667 avg loss:-6.653
step:1001/1667 avg loss:-11.986step:1001/1667 avg loss:-12.031

Train Summary | End of Epoch 27 | Time 2597.28s | Current time 2024-11-27 18:51:20.175322 |Train Loss -11.702| 
step:1/417 avg loss:-9.688
step:1/417 avg loss:-8.476
step:301/417 avg loss:-9.675
step:301/417 avg loss:-9.681
Valid Summary | End of Epoch 27 | Time 403.75s | Current time 2024-11-27 18:58:03.966233 |Valid Loss -9.716| 
Learning rate adjusted to: 0.000500
step:1/1667 avg loss:-9.178
step:1/1667 avg loss:-9.731
step:1001/1667 avg loss:-12.365
step:1001/1667 avg loss:-12.363
Train Summary | End of Epoch 28 | Time 2613.63s | Current time 2024-11-27 19:41:38.149638 |Train Loss -12.468| 
step:1/417 avg loss:-12.070
step:1/417 avg loss:-7.262
step:301/417 avg loss:-11.183
step:301/417 avg loss:-11.108
Valid Summary | End of Epoch 28 | Time 403.79s | Current time 2024-11-27 19:48:22.067065 |Valid Loss -11.223| 
Found new best model, dict saved
step:1/1667 avg loss:-12.413
step:1/1667 avg loss:-11.883
step:1001/1667 avg loss:-12.773step:1001/1667 avg loss:-12.841

Train Summary | End of Epoch 29 | Time 2602.59s | Current time 2024-11-27 20:31:45.959788 |Train Loss -12.755| 
step:1/417 avg loss:-12.234
step:1/417 avg loss:-7.607
step:301/417 avg loss:-11.394
step:301/417 avg loss:-11.223
Valid Summary | End of Epoch 29 | Time 403.29s | Current time 2024-11-27 20:38:29.260273 |Valid Loss -11.374| 
Found new best model, dict saved
step:1/1667 avg loss:-14.582
step:1/1667 avg loss:-14.078
step:1001/1667 avg loss:-12.917
step:1001/1667 avg loss:-12.942
Train Summary | End of Epoch 30 | Time 2730.25s | Current time 2024-11-27 21:24:01.942326 |Train Loss -12.763| 
step:1/417 avg loss:-12.122
step:1/417 avg loss:-8.948
step:301/417 avg loss:-11.158
step:301/417 avg loss:-11.077
Valid Summary | End of Epoch 30 | Time 397.91s | Current time 2024-11-27 21:30:39.997441 |Valid Loss -11.202| 
step:1/1667 avg loss:-12.585
step:1/1667 avg loss:-12.923
step:1001/1667 avg loss:-12.969step:1001/1667 avg loss:-12.972

Train Summary | End of Epoch 31 | Time 2582.32s | Current time 2024-11-27 22:13:44.531453 |Train Loss -12.950| 
step:1/417 avg loss:-12.305
step:1/417 avg loss:-7.917
step:301/417 avg loss:-11.446
step:301/417 avg loss:-11.315
Valid Summary | End of Epoch 31 | Time 399.12s | Current time 2024-11-27 22:20:23.698386 |Valid Loss -11.450| 
Found new best model, dict saved
step:1/1667 avg loss:-12.784
step:1/1667 avg loss:-13.186
step:1001/1667 avg loss:-13.000step:1001/1667 avg loss:-13.041

Train Summary | End of Epoch 32 | Time 2573.60s | Current time 2024-11-27 23:03:19.174724 |Train Loss -12.966| 
step:1/417 avg loss:-12.160
step:1/417 avg loss:-9.974
step:301/417 avg loss:-11.385
step:301/417 avg loss:-11.217
Valid Summary | End of Epoch 32 | Time 394.62s | Current time 2024-11-27 23:09:54.295248 |Valid Loss -11.363| 
step:1/1667 avg loss:-13.039
step:1/1667 avg loss:-13.704
step:1001/1667 avg loss:-13.078step:1001/1667 avg loss:-13.053

Train Summary | End of Epoch 33 | Time 2563.28s | Current time 2024-11-27 23:52:39.134031 |Train Loss -13.066| 
step:1/417 avg loss:-12.247
step:1/417 avg loss:-8.997
step:301/417 avg loss:-11.383
step:301/417 avg loss:-11.257
Valid Summary | End of Epoch 33 | Time 394.11s | Current time 2024-11-27 23:59:13.722993 |Valid Loss -11.405| 
step:1/1667 avg loss:-12.574
step:1/1667 avg loss:-12.288
step:1001/1667 avg loss:-13.094step:1001/1667 avg loss:-13.140

Train Summary | End of Epoch 34 | Time 2573.18s | Current time 2024-11-28 00:42:07.540149 |Train Loss -13.119| 
step:1/417 avg loss:-12.437
step:1/417 avg loss:-9.651
step:301/417 avg loss:-11.491
step:301/417 avg loss:-11.355
Valid Summary | End of Epoch 34 | Time 395.32s | Current time 2024-11-28 00:48:42.889436 |Valid Loss -11.502| 
Found new best model, dict saved
step:1/1667 avg loss:-12.709
step:1/1667 avg loss:-12.295
step:1001/1667 avg loss:-13.083step:1001/1667 avg loss:-13.109

Train Summary | End of Epoch 35 | Time 2581.33s | Current time 2024-11-28 01:31:45.723174 |Train Loss -13.135| 
step:1/417 avg loss:-12.423
step:1/417 avg loss:-9.756
step:301/417 avg loss:-11.491
step:301/417 avg loss:-11.385
Valid Summary | End of Epoch 35 | Time 395.25s | Current time 2024-11-28 01:38:21.009432 |Valid Loss -11.492| 
step:1/1667 avg loss:-13.004
step:1/1667 avg loss:-14.215
step:1001/1667 avg loss:-13.276step:1001/1667 avg loss:-13.270

Train Summary | End of Epoch 36 | Time 2565.92s | Current time 2024-11-28 02:21:08.236089 |Train Loss -13.222| 
step:1/417 avg loss:-12.375
step:1/417 avg loss:-8.196
step:301/417 avg loss:-11.354
step:301/417 avg loss:-11.309
Valid Summary | End of Epoch 36 | Time 397.32s | Current time 2024-11-28 02:27:45.624630 |Valid Loss -11.399| 
step:1/1667 avg loss:-14.530
step:1/1667 avg loss:-10.092
step:1001/1667 avg loss:-13.228step:1001/1667 avg loss:-13.259

Train Summary | End of Epoch 37 | Time 2571.15s | Current time 2024-11-28 03:10:37.943734 |Train Loss -13.248| 
step:1/417 avg loss:-12.211
step:1/417 avg loss:-10.311
step:301/417 avg loss:-11.416
step:301/417 avg loss:-11.372
Valid Summary | End of Epoch 37 | Time 408.41s | Current time 2024-11-28 03:17:26.523718 |Valid Loss -11.457| 
step:1/1667 avg loss:-14.293
step:1/1667 avg loss:-11.820
step:1001/1667 avg loss:-13.178step:1001/1667 avg loss:-13.228

Train Summary | End of Epoch 38 | Time 2572.31s | Current time 2024-11-28 04:00:19.419564 |Train Loss -13.254| 
step:1/417 avg loss:-12.164
step:1/417 avg loss:-8.655
step:301/417 avg loss:-11.610
step:301/417 avg loss:-11.485
Valid Summary | End of Epoch 38 | Time 396.46s | Current time 2024-11-28 04:06:55.929891 |Valid Loss -11.610| 
Found new best model, dict saved
step:1/1667 avg loss:-13.005
step:1/1667 avg loss:-12.605
step:1001/1667 avg loss:-13.390
step:1001/1667 avg loss:-13.389
Train Summary | End of Epoch 39 | Time 2576.45s | Current time 2024-11-28 04:49:53.532810 |Train Loss -13.341| 
step:1/417 avg loss:-12.375
step:1/417 avg loss:-8.199
step:301/417 avg loss:-11.525
step:301/417 avg loss:-11.362
Valid Summary | End of Epoch 39 | Time 396.37s | Current time 2024-11-28 04:56:29.937847 |Valid Loss -11.521| 
step:1/1667 avg loss:-11.802
step:1/1667 avg loss:-12.543
step:1001/1667 avg loss:-13.376step:1001/1667 avg loss:-13.391

Train Summary | End of Epoch 40 | Time 2563.93s | Current time 2024-11-28 05:39:15.276581 |Train Loss -13.382| 
step:1/417 avg loss:-12.467
step:1/417 avg loss:-8.534
step:301/417 avg loss:-11.432
step:301/417 avg loss:-11.413
Valid Summary | End of Epoch 40 | Time 396.95s | Current time 2024-11-28 05:45:52.248073 |Valid Loss -11.477| 
step:1/1667 avg loss:-14.545
step:1/1667 avg loss:-15.043
step:1001/1667 avg loss:-13.390step:1001/1667 avg loss:-13.432

Train Summary | End of Epoch 41 | Time 2578.01s | Current time 2024-11-28 06:28:52.299943 |Train Loss -13.411| 
step:1/417 avg loss:-12.339
step:1/417 avg loss:-10.863
step:301/417 avg loss:-11.423
step:301/417 avg loss:-11.315
Valid Summary | End of Epoch 41 | Time 397.60s | Current time 2024-11-28 06:35:30.593399 |Valid Loss -11.415| 
step:1/1667 avg loss:-13.231
step:1/1667 avg loss:-13.883
step:1001/1667 avg loss:-13.499
step:1001/1667 avg loss:-13.511
Train Summary | End of Epoch 42 | Time 2573.98s | Current time 2024-11-28 07:18:25.029915 |Train Loss -13.377| 
step:1/417 avg loss:-12.184
step:1/417 avg loss:-8.654
step:301/417 avg loss:-11.660
step:301/417 avg loss:-11.513
Valid Summary | End of Epoch 42 | Time 394.59s | Current time 2024-11-28 07:24:59.778666 |Valid Loss -11.635| 
Found new best model, dict saved
step:1/1667 avg loss:-13.940
step:1/1667 avg loss:-11.948
step:1001/1667 avg loss:-13.541step:1001/1667 avg loss:-13.560

Train Summary | End of Epoch 43 | Time 2585.07s | Current time 2024-11-28 08:08:06.940962 |Train Loss -13.505| 
step:1/417 avg loss:-12.187
step:1/417 avg loss:-10.695
step:301/417 avg loss:-11.110
step:301/417 avg loss:-11.014
Valid Summary | End of Epoch 43 | Time 404.54s | Current time 2024-11-28 08:14:51.862238 |Valid Loss -11.147| 
step:1/1667 avg loss:-11.603
step:1/1667 avg loss:-12.535
step:1001/1667 avg loss:-13.364step:1001/1667 avg loss:-13.414

Train Summary | End of Epoch 44 | Time 2585.21s | Current time 2024-11-28 08:57:58.594217 |Train Loss -13.399| 
step:1/417 avg loss:-12.214
step:1/417 avg loss:-11.970
step:301/417 avg loss:-11.614
step:301/417 avg loss:-11.504
Valid Summary | End of Epoch 44 | Time 412.88s | Current time 2024-11-28 09:04:51.492670 |Valid Loss -11.627| 
step:1/1667 avg loss:-12.857
step:1/1667 avg loss:-12.984
step:1001/1667 avg loss:-13.576step:1001/1667 avg loss:-13.603

Train Summary | End of Epoch 45 | Time 2593.49s | Current time 2024-11-28 09:48:06.149256 |Train Loss -13.573| 
step:1/417 avg loss:-12.154
step:1/417 avg loss:-11.537
step:301/417 avg loss:-11.603
step:301/417 avg loss:-11.581
Valid Summary | End of Epoch 45 | Time 402.19s | Current time 2024-11-28 09:54:48.389262 |Valid Loss -11.651| 
Found new best model, dict saved
step:1/1667 avg loss:-13.135
step:1/1667 avg loss:-14.710
step:1001/1667 avg loss:-13.419step:1001/1667 avg loss:-13.446

Train Summary | End of Epoch 46 | Time 2584.51s | Current time 2024-11-28 10:37:54.641643 |Train Loss -13.454| 
step:1/417 avg loss:-11.890
step:1/417 avg loss:-10.975
step:301/417 avg loss:-11.549
step:301/417 avg loss:-11.542
Valid Summary | End of Epoch 46 | Time 401.46s | Current time 2024-11-28 10:44:36.282564 |Valid Loss -11.606| 
step:1/1667 avg loss:-13.836
step:1/1667 avg loss:-12.794
step:1001/1667 avg loss:-13.605step:1001/1667 avg loss:-13.635

Train Summary | End of Epoch 47 | Time 2594.45s | Current time 2024-11-28 11:27:51.487388 |Train Loss -13.633| 
step:1/417 avg loss:-10.723
step:1/417 avg loss:-9.199
step:301/417 avg loss:-11.075
step:301/417 avg loss:-10.948
Valid Summary | End of Epoch 47 | Time 401.98s | Current time 2024-11-28 11:34:33.569752 |Valid Loss -11.064| 
step:1/1667 avg loss:-13.364
step:1/1667 avg loss:-12.679
step:1001/1667 avg loss:-13.314
step:1001/1667 avg loss:-13.305
Train Summary | End of Epoch 48 | Time 2594.72s | Current time 2024-11-28 12:17:49.095468 |Train Loss -13.430| 
step:1/417 avg loss:-12.417
step:1/417 avg loss:-11.867
step:301/417 avg loss:-11.698
step:301/417 avg loss:-11.530
Valid Summary | End of Epoch 48 | Time 401.47s | Current time 2024-11-28 12:24:31.038000 |Valid Loss -11.655| 
Found new best model, dict saved
step:1/1667 avg loss:-13.775
step:1/1667 avg loss:-14.795
step:1001/1667 avg loss:-13.726step:1001/1667 avg loss:-13.750

Train Summary | End of Epoch 49 | Time 2589.77s | Current time 2024-11-28 13:07:41.937869 |Train Loss -13.711| 
step:1/417 avg loss:-12.058
step:1/417 avg loss:-11.400
step:301/417 avg loss:-11.587
step:301/417 avg loss:-11.443
Valid Summary | End of Epoch 49 | Time 401.37s | Current time 2024-11-28 13:14:23.579497 |Valid Loss -11.575| 
step:1/1667 avg loss:-13.804
step:1/1667 avg loss:-9.713
step:1001/1667 avg loss:-11.702step:1001/1667 avg loss:-11.689

Train Summary | End of Epoch 50 | Time 2589.21s | Current time 2024-11-28 13:57:33.208071 |Train Loss -11.976| 
step:1/417 avg loss:-12.291
step:1/417 avg loss:-9.108
step:301/417 avg loss:-11.498
step:301/417 avg loss:-11.491
Valid Summary | End of Epoch 50 | Time 405.12s | Current time 2024-11-28 14:04:18.334900 |Valid Loss -11.561| 
step:1/1667 avg loss:-13.338
step:1/1667 avg loss:-13.402
step:1001/1667 avg loss:-12.911step:1001/1667 avg loss:-12.952

Train Summary | End of Epoch 51 | Time 2588.08s | Current time 2024-11-28 14:47:28.125814 |Train Loss -12.836| 
step:1/417 avg loss:-12.259
step:1/417 avg loss:-9.997
step:301/417 avg loss:-11.476
step:301/417 avg loss:-11.445
Valid Summary | End of Epoch 51 | Time 408.57s | Current time 2024-11-28 14:54:16.750176 |Valid Loss -11.473| 
step:1/1667 avg loss:-14.591
step:1/1667 avg loss:-11.540
step:1001/1667 avg loss:-13.061step:1001/1667 avg loss:-13.027

Train Summary | End of Epoch 52 | Time 2594.10s | Current time 2024-11-28 15:37:31.460639 |Train Loss -12.995| 
step:1/417 avg loss:-12.178
step:1/417 avg loss:-8.683
step:301/417 avg loss:-11.672
step:301/417 avg loss:-11.555
Valid Summary | End of Epoch 52 | Time 400.48s | Current time 2024-11-28 15:44:11.962693 |Valid Loss -11.638| 
step:1/1667 avg loss:-12.802
step:1/1667 avg loss:-13.253
step:1001/1667 avg loss:-13.244step:1001/1667 avg loss:-13.272

Train Summary | End of Epoch 53 | Time 2591.26s | Current time 2024-11-28 16:27:23.987295 |Train Loss -13.206| 
step:1/417 avg loss:-12.063
step:1/417 avg loss:-8.711
step:301/417 avg loss:-11.598
step:301/417 avg loss:-11.540
Valid Summary | End of Epoch 53 | Time 401.55s | Current time 2024-11-28 16:34:05.609525 |Valid Loss -11.597| 
step:1/1667 avg loss:-12.739
step:1/1667 avg loss:-12.708
step:1001/1667 avg loss:-13.162
step:1001/1667 avg loss:-13.233
Train Summary | End of Epoch 54 | Time 2596.52s | Current time 2024-11-28 17:17:22.859627 |Train Loss -13.210| 
step:1/417 avg loss:-12.347
step:1/417 avg loss:-11.687
step:301/417 avg loss:-11.814
step:301/417 avg loss:-11.705
Valid Summary | End of Epoch 54 | Time 400.61s | Current time 2024-11-28 17:24:03.480200 |Valid Loss -11.788| 
Found new best model, dict saved
step:1/1667 avg loss:-12.542
step:1/1667 avg loss:-13.455
step:1001/1667 avg loss:-13.083step:1001/1667 avg loss:-13.193

Train Summary | End of Epoch 55 | Time 2593.68s | Current time 2024-11-28 18:07:18.599802 |Train Loss -13.217| 
step:1/417 avg loss:-12.082
step:1/417 avg loss:-11.526
step:301/417 avg loss:-11.819
step:301/417 avg loss:-11.723
Valid Summary | End of Epoch 55 | Time 401.16s | Current time 2024-11-28 18:13:59.775925 |Valid Loss -11.835| 
Found new best model, dict saved
step:1/1667 avg loss:-12.546
step:1/1667 avg loss:-14.485
step:1001/1667 avg loss:-13.241step:1001/1667 avg loss:-13.228

Train Summary | End of Epoch 56 | Time 2588.69s | Current time 2024-11-28 18:57:09.870901 |Train Loss -13.236| 
step:1/417 avg loss:-12.243
step:1/417 avg loss:-8.610
step:301/417 avg loss:-11.910
step:301/417 avg loss:-11.756
Valid Summary | End of Epoch 56 | Time 401.07s | Current time 2024-11-28 19:03:51.530706 |Valid Loss -11.873| 
Found new best model, dict saved
step:1/1667 avg loss:-13.255
step:1/1667 avg loss:-13.466
step:1001/1667 avg loss:-13.554step:1001/1667 avg loss:-13.604

Train Summary | End of Epoch 57 | Time 2581.88s | Current time 2024-11-28 19:46:55.671292 |Train Loss -13.518| 
step:1/417 avg loss:-12.366
step:1/417 avg loss:-11.292
step:301/417 avg loss:-11.892
step:301/417 avg loss:-11.686
Valid Summary | End of Epoch 57 | Time 407.65s | Current time 2024-11-28 19:53:43.774914 |Valid Loss -11.792| 
step:1/1667 avg loss:-14.304
step:1/1667 avg loss:-13.086
step:1001/1667 avg loss:-13.532step:1001/1667 avg loss:-13.596

Train Summary | End of Epoch 58 | Time 2601.62s | Current time 2024-11-28 20:37:05.788881 |Train Loss -13.486| 
step:1/417 avg loss:-12.167
step:1/417 avg loss:-11.510
step:301/417 avg loss:-11.405
step:301/417 avg loss:-11.258
Valid Summary | End of Epoch 58 | Time 406.03s | Current time 2024-11-28 20:43:51.921159 |Valid Loss -11.409| 
step:1/1667 avg loss:-15.102
step:1/1667 avg loss:-13.165
step:1001/1667 avg loss:-13.506
step:1001/1667 avg loss:-13.511
Train Summary | End of Epoch 59 | Time 2593.73s | Current time 2024-11-28 21:27:06.738752 |Train Loss -13.481| 
step:1/417 avg loss:-12.438
step:1/417 avg loss:-9.293
step:301/417 avg loss:-11.682
step:301/417 avg loss:-11.639
Valid Summary | End of Epoch 59 | Time 403.46s | Current time 2024-11-28 21:33:50.256089 |Valid Loss -11.692| 
step:1/1667 avg loss:-14.102
step:1/1667 avg loss:-13.870
step:1001/1667 avg loss:-13.529step:1001/1667 avg loss:-13.581

Train Summary | End of Epoch 60 | Time 2587.63s | Current time 2024-11-28 22:16:58.438632 |Train Loss -13.518| 
step:1/417 avg loss:-11.947
step:1/417 avg loss:-11.618
step:301/417 avg loss:-11.655
step:301/417 avg loss:-11.414
Valid Summary | End of Epoch 60 | Time 399.40s | Current time 2024-11-28 22:23:37.988419 |Valid Loss -11.562| 
step:1/1667 avg loss:-13.999
step:1/1667 avg loss:-14.135
step:1001/1667 avg loss:-13.575step:1001/1667 avg loss:-13.592

Train Summary | End of Epoch 61 | Time 2605.14s | Current time 2024-11-28 23:07:05.128341 |Train Loss -13.513| 
step:1/417 avg loss:-12.439
step:1/417 avg loss:-11.571
step:301/417 avg loss:-11.903
step:301/417 avg loss:-11.778
Valid Summary | End of Epoch 61 | Time 411.42s | Current time 2024-11-28 23:13:57.149241 |Valid Loss -11.874| 
Found new best model, dict saved
step:1/1667 avg loss:-14.367
step:1/1667 avg loss:-14.490
step:1001/1667 avg loss:-13.611step:1001/1667 avg loss:-13.580

Train Summary | End of Epoch 62 | Time 2617.57s | Current time 2024-11-28 23:57:37.410163 |Train Loss -13.581| 
step:1/417 avg loss:-12.340
step:1/417 avg loss:-8.144
step:301/417 avg loss:-11.817
step:301/417 avg loss:-11.650
Valid Summary | End of Epoch 62 | Time 401.80s | Current time 2024-11-29 00:04:19.283872 |Valid Loss -11.780| 
step:1/1667 avg loss:-14.100
step:1/1667 avg loss:-13.624
step:1001/1667 avg loss:-13.604step:1001/1667 avg loss:-13.543

Train Summary | End of Epoch 63 | Time 2599.25s | Current time 2024-11-29 00:47:39.681860 |Train Loss -13.559| 
step:1/417 avg loss:-12.245
step:1/417 avg loss:-8.499
step:301/417 avg loss:-11.805
step:301/417 avg loss:-11.635
Valid Summary | End of Epoch 63 | Time 399.39s | Current time 2024-11-29 00:54:19.707833 |Valid Loss -11.771| 
step:1/1667 avg loss:-14.424
step:1/1667 avg loss:-14.361
step:1001/1667 avg loss:-13.808
step:1001/1667 avg loss:-13.834
Train Summary | End of Epoch 64 | Time 2593.96s | Current time 2024-11-29 01:37:34.548217 |Train Loss -13.780| 
step:1/417 avg loss:-12.200
step:1/417 avg loss:-8.749
step:301/417 avg loss:-11.773
step:301/417 avg loss:-11.653
Valid Summary | End of Epoch 64 | Time 402.24s | Current time 2024-11-29 01:44:16.863629 |Valid Loss -11.767| 
step:1/1667 avg loss:-12.086
step:1/1667 avg loss:-13.784
step:1001/1667 avg loss:-13.548step:1001/1667 avg loss:-13.600

Train Summary | End of Epoch 65 | Time 2593.10s | Current time 2024-11-29 02:27:30.399269 |Train Loss -13.581| 
step:1/417 avg loss:-12.299
step:1/417 avg loss:-8.939
step:301/417 avg loss:-11.807
step:301/417 avg loss:-11.603
Valid Summary | End of Epoch 65 | Time 411.15s | Current time 2024-11-29 02:34:21.620300 |Valid Loss -11.761| 
step:1/1667 avg loss:-14.142
step:1/1667 avg loss:-13.300
step:1001/1667 avg loss:-13.614
step:1001/1667 avg loss:-13.643
Train Summary | End of Epoch 66 | Time 2586.48s | Current time 2024-11-29 03:17:28.965240 |Train Loss -13.561| 
step:1/417 avg loss:-11.597
step:1/417 avg loss:-11.498
step:301/417 avg loss:-11.190
step:301/417 avg loss:-11.077
Valid Summary | End of Epoch 66 | Time 399.18s | Current time 2024-11-29 03:24:08.255029 |Valid Loss -11.178| 
step:1/1667 avg loss:-11.938
step:1/1667 avg loss:-13.014
step:1001/1667 avg loss:-13.765step:1001/1667 avg loss:-13.816

Train Summary | End of Epoch 67 | Time 2594.70s | Current time 2024-11-29 04:07:23.329187 |Train Loss -13.786| 
step:1/417 avg loss:-12.484
step:1/417 avg loss:-11.755
step:301/417 avg loss:-11.961
step:301/417 avg loss:-11.754
Valid Summary | End of Epoch 67 | Time 403.67s | Current time 2024-11-29 04:14:08.162941 |Valid Loss -11.884| 
Found new best model, dict saved
step:1/1667 avg loss:-13.065
step:1/1667 avg loss:-13.840
step:1001/1667 avg loss:-13.971step:1001/1667 avg loss:-13.992

Train Summary | End of Epoch 68 | Time 2594.96s | Current time 2024-11-29 04:57:25.284020 |Train Loss -13.899| 
step:1/417 avg loss:-12.199
step:1/417 avg loss:-10.919
step:301/417 avg loss:-11.847
step:301/417 avg loss:-11.703
Valid Summary | End of Epoch 68 | Time 400.98s | Current time 2024-11-29 05:04:06.636382 |Valid Loss -11.823| 
step:1/1667 avg loss:-12.834
step:1/1667 avg loss:-14.195
step:1001/1667 avg loss:-13.907step:1001/1667 avg loss:-13.931

Train Summary | End of Epoch 69 | Time 2595.98s | Current time 2024-11-29 05:47:23.755657 |Train Loss -13.795| 
step:1/417 avg loss:-12.375
step:1/417 avg loss:-11.649
step:301/417 avg loss:-11.834
step:301/417 avg loss:-11.762
Valid Summary | End of Epoch 69 | Time 402.07s | Current time 2024-11-29 05:54:05.850748 |Valid Loss -11.830| 
step:1/1667 avg loss:-13.915
step:1/1667 avg loss:-14.246
step:1001/1667 avg loss:-13.543step:1001/1667 avg loss:-13.569

Train Summary | End of Epoch 70 | Time 2598.44s | Current time 2024-11-29 06:37:24.916423 |Train Loss -13.641| 
step:1/417 avg loss:-12.231
step:1/417 avg loss:-11.699
step:301/417 avg loss:-11.927
step:301/417 avg loss:-11.768
Valid Summary | End of Epoch 70 | Time 400.90s | Current time 2024-11-29 06:44:06.394110 |Valid Loss -11.904| 
Found new best model, dict saved
step:1/1667 avg loss:-14.196
step:1/1667 avg loss:-15.353
step:1001/1667 avg loss:-13.984step:1001/1667 avg loss:-14.046

Train Summary | End of Epoch 71 | Time 2586.12s | Current time 2024-11-29 07:27:15.738617 |Train Loss -13.993| 
step:1/417 avg loss:-12.458
step:1/417 avg loss:-11.946
step:301/417 avg loss:-11.519
step:301/417 avg loss:-11.392
Valid Summary | End of Epoch 71 | Time 405.47s | Current time 2024-11-29 07:34:01.293429 |Valid Loss -11.515| 
step:1/1667 avg loss:-13.455
step:1/1667 avg loss:-14.201
step:1001/1667 avg loss:-13.873
step:1001/1667 avg loss:-13.955
Train Summary | End of Epoch 72 | Time 2604.23s | Current time 2024-11-29 08:17:26.787767 |Train Loss -13.890| 
step:1/417 avg loss:-8.921
step:1/417 avg loss:-8.504
step:301/417 avg loss:-10.258
step:301/417 avg loss:-10.044
Valid Summary | End of Epoch 72 | Time 408.68s | Current time 2024-11-29 08:24:15.490352 |Valid Loss -10.223| 
step:1/1667 avg loss:-11.168
step:1/1667 avg loss:-11.150
step:1001/1667 avg loss:-13.736step:1001/1667 avg loss:-13.756

Train Summary | End of Epoch 73 | Time 2584.23s | Current time 2024-11-29 09:07:20.687978 |Train Loss -13.764| 
step:1/417 avg loss:-12.461
step:1/417 avg loss:-9.041
step:301/417 avg loss:-11.913
step:301/417 avg loss:-11.723
Valid Summary | End of Epoch 73 | Time 399.34s | Current time 2024-11-29 09:14:00.042450 |Valid Loss -11.862| 
step:1/1667 avg loss:-13.903
step:1/1667 avg loss:-15.356
step:1001/1667 avg loss:-13.832step:1001/1667 avg loss:-13.846

Train Summary | End of Epoch 74 | Time 2584.25s | Current time 2024-11-29 09:57:04.792333 |Train Loss -13.827| 
step:1/417 avg loss:-10.903
step:1/417 avg loss:-10.790
step:301/417 avg loss:-11.350
step:301/417 avg loss:-11.264
Valid Summary | End of Epoch 74 | Time 398.78s | Current time 2024-11-29 10:03:43.638611 |Valid Loss -11.379| 
step:1/1667 avg loss:-13.117
step:1/1667 avg loss:-14.693
step:1001/1667 avg loss:-13.992step:1001/1667 avg loss:-14.022

Train Summary | End of Epoch 75 | Time 2589.49s | Current time 2024-11-29 10:46:54.275264 |Train Loss -13.957| 
step:1/417 avg loss:-12.593
step:1/417 avg loss:-10.888
step:301/417 avg loss:-11.945
step:301/417 avg loss:-11.759
Valid Summary | End of Epoch 75 | Time 399.95s | Current time 2024-11-29 10:53:34.308281 |Valid Loss -11.897| 
step:1/1667 avg loss:-12.411
step:1/1667 avg loss:-13.437
step:1001/1667 avg loss:-13.813step:1001/1667 avg loss:-13.876

Train Summary | End of Epoch 76 | Time 2590.36s | Current time 2024-11-29 11:36:45.119946 |Train Loss -13.770| 
step:1/417 avg loss:-11.015
step:1/417 avg loss:-7.376
step:301/417 avg loss:-11.448
step:301/417 avg loss:-11.343
Valid Summary | End of Epoch 76 | Time 400.08s | Current time 2024-11-29 11:43:25.220524 |Valid Loss -11.453| 
Learning rate adjusted to: 0.000250
step:1/1667 avg loss:-14.769
step:1/1667 avg loss:-11.438
step:1001/1667 avg loss:-14.013step:1001/1667 avg loss:-14.060

Train Summary | End of Epoch 77 | Time 2588.66s | Current time 2024-11-29 12:26:35.672048 |Train Loss -14.107| 
step:1/417 avg loss:-12.570
step:1/417 avg loss:-8.513
step:301/417 avg loss:-12.029
step:301/417 avg loss:-11.838
Valid Summary | End of Epoch 77 | Time 400.44s | Current time 2024-11-29 12:33:16.190080 |Valid Loss -12.006| 
Found new best model, dict saved
step:1/1667 avg loss:-14.357
step:1/1667 avg loss:-14.963
step:1001/1667 avg loss:-14.305step:1001/1667 avg loss:-14.348

Train Summary | End of Epoch 78 | Time 2590.83s | Current time 2024-11-29 13:16:29.659965 |Train Loss -14.248| 
step:1/417 avg loss:-11.902
step:1/417 avg loss:-7.407
step:301/417 avg loss:-11.375
step:301/417 avg loss:-11.236
Valid Summary | End of Epoch 78 | Time 393.89s | Current time 2024-11-29 13:23:04.300032 |Valid Loss -11.362| 
step:1/1667 avg loss:-11.220
step:1/1667 avg loss:-14.067
step:1001/1667 avg loss:-14.016step:1001/1667 avg loss:-14.058

Train Summary | End of Epoch 79 | Time 2543.87s | Current time 2024-11-29 14:05:29.814463 |Train Loss -14.138| 
step:1/417 avg loss:-12.282
step:1/417 avg loss:-11.684
step:301/417 avg loss:-11.955
step:301/417 avg loss:-11.850
Valid Summary | End of Epoch 79 | Time 394.32s | Current time 2024-11-29 14:12:04.214539 |Valid Loss -11.949| 
step:1/1667 avg loss:-14.084
step:1/1667 avg loss:-13.483
step:1001/1667 avg loss:-14.371step:1001/1667 avg loss:-14.396

Train Summary | End of Epoch 80 | Time 2562.79s | Current time 2024-11-29 14:54:47.565666 |Train Loss -14.382| 
step:1/417 avg loss:-12.555
step:1/417 avg loss:-8.947
step:301/417 avg loss:-11.996
step:301/417 avg loss:-11.870
Valid Summary | End of Epoch 80 | Time 387.74s | Current time 2024-11-29 15:01:15.682237 |Valid Loss -11.981| 
step:1/1667 avg loss:-13.759
step:1/1667 avg loss:-13.940
step:1001/1667 avg loss:-14.374step:1001/1667 avg loss:-14.464

Train Summary | End of Epoch 81 | Time 2564.40s | Current time 2024-11-29 15:44:01.726388 |Train Loss -14.382| 
step:1/417 avg loss:-12.417
step:1/417 avg loss:-11.922
step:301/417 avg loss:-12.048
step:301/417 avg loss:-11.869
Valid Summary | End of Epoch 81 | Time 390.41s | Current time 2024-11-29 15:50:32.146587 |Valid Loss -12.009| 
Found new best model, dict saved
step:1/1667 avg loss:-15.672
step:1/1667 avg loss:-14.609
step:1001/1667 avg loss:-14.407step:1001/1667 avg loss:-14.418

Train Summary | End of Epoch 82 | Time 2581.86s | Current time 2024-11-29 16:33:37.007444 |Train Loss -14.409| 
step:1/417 avg loss:-12.421
step:1/417 avg loss:-8.397
step:301/417 avg loss:-11.938
step:301/417 avg loss:-11.828
Valid Summary | End of Epoch 82 | Time 388.89s | Current time 2024-11-29 16:40:06.046211 |Valid Loss -11.942| 
step:1/1667 avg loss:-14.302
step:1/1667 avg loss:-14.567
step:1001/1667 avg loss:-14.401step:1001/1667 avg loss:-14.426

Train Summary | End of Epoch 83 | Time 2587.08s | Current time 2024-11-29 17:23:14.258525 |Train Loss -14.417| 
step:1/417 avg loss:-12.273
step:1/417 avg loss:-11.588
step:301/417 avg loss:-12.076
step:301/417 avg loss:-11.857
Valid Summary | End of Epoch 83 | Time 390.84s | Current time 2024-11-29 17:29:45.306308 |Valid Loss -12.012| 
Found new best model, dict saved
step:1/1667 avg loss:-13.978
step:1/1667 avg loss:-15.556
step:1001/1667 avg loss:-14.468step:1001/1667 avg loss:-14.494

Train Summary | End of Epoch 84 | Time 2590.86s | Current time 2024-11-29 18:12:58.831578 |Train Loss -14.453| 
step:1/417 avg loss:-12.374
step:1/417 avg loss:-9.960
step:301/417 avg loss:-11.939
step:301/417 avg loss:-11.820
Valid Summary | End of Epoch 84 | Time 391.48s | Current time 2024-11-29 18:19:30.361725 |Valid Loss -11.913| 
step:1/1667 avg loss:-14.245
step:1/1667 avg loss:-14.089
step:1001/1667 avg loss:-14.459
step:1001/1667 avg loss:-14.530
Train Summary | End of Epoch 85 | Time 2568.03s | Current time 2024-11-29 19:02:19.570728 |Train Loss -14.454| 
step:1/417 avg loss:-12.216
step:1/417 avg loss:-8.430
step:301/417 avg loss:-11.941
step:301/417 avg loss:-11.829
Valid Summary | End of Epoch 85 | Time 408.60s | Current time 2024-11-29 19:09:08.236450 |Valid Loss -11.932| 
step:1/1667 avg loss:-14.619
step:1/1667 avg loss:-14.160
step:1001/1667 avg loss:-14.501step:1001/1667 avg loss:-14.542

Train Summary | End of Epoch 86 | Time 2550.43s | Current time 2024-11-29 19:51:40.211978 |Train Loss -14.488| 
step:1/417 avg loss:-12.543
step:1/417 avg loss:-8.435
step:301/417 avg loss:-11.940
step:301/417 avg loss:-11.834
Valid Summary | End of Epoch 86 | Time 432.59s | Current time 2024-11-29 19:58:53.553138 |Valid Loss -11.940| 
step:1/1667 avg loss:-13.575
step:1/1667 avg loss:-14.594
step:1001/1667 avg loss:-14.562step:1001/1667 avg loss:-14.557

Train Summary | End of Epoch 87 | Time 2551.16s | Current time 2024-11-29 20:41:26.630812 |Train Loss -14.547| 
step:1/417 avg loss:-12.147
step:1/417 avg loss:-9.656
step:301/417 avg loss:-11.960
step:301/417 avg loss:-11.833
Valid Summary | End of Epoch 87 | Time 391.53s | Current time 2024-11-29 20:47:58.624747 |Valid Loss -11.947| 
step:1/1667 avg loss:-15.394
step:1/1667 avg loss:-13.588
step:1001/1667 avg loss:-14.538
step:1001/1667 avg loss:-14.584
Train Summary | End of Epoch 88 | Time 2523.95s | Current time 2024-11-29 21:30:03.770425 |Train Loss -14.561| 
step:1/417 avg loss:-12.042
step:1/417 avg loss:-11.718
step:301/417 avg loss:-11.958
step:301/417 avg loss:-11.780
Valid Summary | End of Epoch 88 | Time 382.87s | Current time 2024-11-29 21:36:27.361111 |Valid Loss -11.915| 
step:1/1667 avg loss:-14.651
step:1/1667 avg loss:-13.094
step:1001/1667 avg loss:-14.589step:1001/1667 avg loss:-14.607

Train Summary | End of Epoch 89 | Time 2533.62s | Current time 2024-11-29 22:18:41.944744 |Train Loss -14.556| 
step:1/417 avg loss:-12.161
step:1/417 avg loss:-8.591
step:301/417 avg loss:-11.977
step:301/417 avg loss:-11.787
Valid Summary | End of Epoch 89 | Time 374.91s | Current time 2024-11-29 22:24:57.216529 |Valid Loss -11.928| 
Learning rate adjusted to: 0.000125
step:1/1667 avg loss:-14.331
step:1/1667 avg loss:-12.862
step:1001/1667 avg loss:-14.670step:1001/1667 avg loss:-14.739

Train Summary | End of Epoch 90 | Time 2564.64s | Current time 2024-11-29 23:07:42.309232 |Train Loss -14.689| 
step:1/417 avg loss:-12.301
step:1/417 avg loss:-8.858
step:301/417 avg loss:-11.962
step:301/417 avg loss:-11.834
Valid Summary | End of Epoch 90 | Time 376.55s | Current time 2024-11-29 23:13:59.244491 |Valid Loss -11.927| 
step:1/1667 avg loss:-14.068
step:1/1667 avg loss:-14.909
step:1001/1667 avg loss:-14.710step:1001/1667 avg loss:-14.760

Train Summary | End of Epoch 91 | Time 2565.73s | Current time 2024-11-29 23:56:46.861106 |Train Loss -14.717| 
step:1/417 avg loss:-12.324
step:1/417 avg loss:-8.442
step:301/417 avg loss:-11.842
step:301/417 avg loss:-11.723
Valid Summary | End of Epoch 91 | Time 379.58s | Current time 2024-11-30 00:03:06.452384 |Valid Loss -11.857| 
step:1/1667 avg loss:-15.828
step:1/1667 avg loss:-14.562
step:1001/1667 avg loss:-14.723step:1001/1667 avg loss:-14.786

Train Summary | End of Epoch 92 | Time 2558.14s | Current time 2024-11-30 00:45:45.278861 |Train Loss -14.744| 
step:1/417 avg loss:-12.292
step:1/417 avg loss:-8.441
step:301/417 avg loss:-11.923
step:301/417 avg loss:-11.704
Valid Summary | End of Epoch 92 | Time 388.71s | Current time 2024-11-30 00:52:14.130140 |Valid Loss -11.875| 
step:1/1667 avg loss:-14.298
step:1/1667 avg loss:-14.404
step:1001/1667 avg loss:-14.749step:1001/1667 avg loss:-14.756

Train Summary | End of Epoch 93 | Time 2545.28s | Current time 2024-11-30 01:34:40.461275 |Train Loss -14.754| 
step:1/417 avg loss:-12.236
step:1/417 avg loss:-10.583
step:301/417 avg loss:-11.957
step:301/417 avg loss:-11.786
Valid Summary | End of Epoch 93 | Time 403.51s | Current time 2024-11-30 01:41:24.426183 |Valid Loss -11.918| 
No improvement for 10 epochs, early stopping.
