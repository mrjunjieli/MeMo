started on logs/TDSE_SpkEmbMem/v_teacher

Namespace(mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', batch_size=3, max_length=5, num_workers=6, epochs=100, effec_batch_size=8, accu_grad=0, C=2, model_name='TDSE_SpkEmbMem', lr=0.001, max_norm=5, log_name='logs/TDSE_SpkEmbMem/v_teacher', use_tensorboard=1, continue_from='FALSE', local_rank=0, sample_rate=16000, win=512, hop_length=128, n_mels=80, shift_range='0,1', Self_enroll_amplitude_scaling='0.1,1', teacher_point=50, num_slots='1,5', loss_weight='28', distributed=True, world_size=3, speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800)

Total number of parameters: 25810661 

av_convtasnet(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): TemporalConvNet(
    (layer_norm): ChannelWiseLayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (bottleneck_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (tcn): ModuleList(
      (0-3): 4 x Sequential(
        (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (2): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (3): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (4): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (5): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (6): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (7): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (8): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
      )
    )
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (ve_conv1x1): ModuleList(
      (0-3): 4 x Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
    )
    (mask_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (preEmphasis): PreEmphasis()
  (spk_encoder): MelSpectrogram(
    (spectrogram): Spectrogram()
    (mel_scale): MelScale()
  )
  (spk_model): ECAPA_TDNN(
    (layer1): Conv1dReluBn(
      (conv): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,))
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (layer2): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer3): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer4): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (conv): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))
    (pool): ASTP(
      (linear1): Conv1d(1536, 128, kernel_size=(1,), stride=(1,))
      (linear2): Conv1d(128, 1536, kernel_size=(1,), stride=(1,))
    )
    (bn): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (linear): Linear(in_features=3072, out_features=256, bias=True)
    (bn2): Identity()
  )
  (self_att): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Start new training
step:1/445 avg loss:39.351, visual_loss:43.889, av_loss:38.216
step:1/445 avg loss:35.004, visual_loss:34.994, av_loss:35.006
step:1/445 avg loss:48.140, visual_loss:48.296, av_loss:48.101
Train Summary | End of Epoch 1 | Time 594.26s | Current time 2024-12-26 17:49:20.184011 |Train Loss 0.914| 
step:1/111 avg loss:-0.312, visual_loss:-0.289, av_loss:-0.318
step:1/111 avg loss:0.308, visual_loss:0.301, av_loss:0.310
step:1/111 avg loss:0.007, visual_loss:-0.098, av_loss:0.033
Valid Summary | End of Epoch 1 | Time 101.76s | Current time 2024-12-26 17:51:01.987396 |Valid Loss 0.268| 
Found new best model, dict saved
step:1/445 avg loss:0.076, visual_loss:0.060, av_loss:0.080
step:1/445 avg loss:-0.407, visual_loss:-0.053, av_loss:-0.496
step:1/445 avg loss:-0.201, visual_loss:-0.012, av_loss:-0.249
Train Summary | End of Epoch 2 | Time 588.29s | Current time 2024-12-26 18:00:53.834049 |Train Loss -0.085| 
step:1/111 avg loss:-0.589, visual_loss:-0.603, av_loss:-0.586
step:1/111 avg loss:0.181, visual_loss:0.079, av_loss:0.206
step:1/111 avg loss:-0.522, visual_loss:-0.654, av_loss:-0.489
Valid Summary | End of Epoch 2 | Time 96.74s | Current time 2024-12-26 18:02:30.592871 |Valid Loss 0.039| 
Found new best model, dict saved
step:1/445 avg loss:-0.381, visual_loss:-0.231, av_loss:-0.419
step:1/445 avg loss:-0.633, visual_loss:-0.373, av_loss:-0.698
step:1/445 avg loss:0.073, visual_loss:0.260, av_loss:0.027
Train Summary | End of Epoch 3 | Time 590.71s | Current time 2024-12-26 18:12:23.936293 |Train Loss -0.434| 
step:1/111 avg loss:-1.074, visual_loss:-1.088, av_loss:-1.071
step:1/111 avg loss:-0.878, visual_loss:-0.883, av_loss:-0.877
step:1/111 avg loss:1.119, visual_loss:0.593, av_loss:1.251
Valid Summary | End of Epoch 3 | Time 100.62s | Current time 2024-12-26 18:14:04.601967 |Valid Loss -0.048| 
Found new best model, dict saved
step:1/445 avg loss:-1.311, visual_loss:-0.921, av_loss:-1.409step:1/445 avg loss:-0.908, visual_loss:-0.502, av_loss:-1.010

step:1/445 avg loss:-1.166, visual_loss:-0.908, av_loss:-1.231
Train Summary | End of Epoch 4 | Time 612.02s | Current time 2024-12-26 18:24:18.210458 |Train Loss -0.832| 
step:1/111 avg loss:-1.191, visual_loss:-1.241, av_loss:-1.178
step:1/111 avg loss:-0.165, visual_loss:-0.336, av_loss:-0.123
step:1/111 avg loss:-0.631, visual_loss:-1.229, av_loss:-0.482
Valid Summary | End of Epoch 4 | Time 103.28s | Current time 2024-12-26 18:26:01.498953 |Valid Loss -0.312| 
Found new best model, dict saved
step:1/445 avg loss:0.006, visual_loss:0.191, av_loss:-0.041
step:1/445 avg loss:-2.461, visual_loss:-2.076, av_loss:-2.557
step:1/445 avg loss:-0.499, visual_loss:-0.654, av_loss:-0.460
Train Summary | End of Epoch 5 | Time 636.98s | Current time 2024-12-26 18:36:39.357687 |Train Loss -1.194| 
step:1/111 avg loss:-0.169, visual_loss:-0.261, av_loss:-0.147
step:1/111 avg loss:-1.564, visual_loss:-2.003, av_loss:-1.454
step:1/111 avg loss:-1.259, visual_loss:-1.495, av_loss:-1.200
Valid Summary | End of Epoch 5 | Time 104.96s | Current time 2024-12-26 18:38:24.370577 |Valid Loss -0.706| 
Found new best model, dict saved
step:1/445 avg loss:-2.025, visual_loss:-2.051, av_loss:-2.019step:1/445 avg loss:-1.623, visual_loss:-1.475, av_loss:-1.659

step:1/445 avg loss:-1.093, visual_loss:-0.508, av_loss:-1.240
Train Summary | End of Epoch 6 | Time 637.63s | Current time 2024-12-26 18:49:04.923186 |Train Loss -1.390| 
step:1/111 avg loss:-0.002, visual_loss:-0.122, av_loss:0.028
step:1/111 avg loss:-2.259, visual_loss:-2.161, av_loss:-2.284
step:1/111 avg loss:-1.619, visual_loss:-1.762, av_loss:-1.584
Valid Summary | End of Epoch 6 | Time 108.20s | Current time 2024-12-26 18:50:53.159246 |Valid Loss -0.766| 
Found new best model, dict saved
step:1/445 avg loss:-1.438, visual_loss:-0.962, av_loss:-1.557
step:1/445 avg loss:-2.533, visual_loss:-2.274, av_loss:-2.598
step:1/445 avg loss:-4.587, visual_loss:-4.208, av_loss:-4.681
Train Summary | End of Epoch 7 | Time 637.84s | Current time 2024-12-26 19:01:33.509332 |Train Loss -1.741| 
step:1/111 avg loss:-3.861, visual_loss:-3.619, av_loss:-3.921
step:1/111 avg loss:0.484, visual_loss:-0.344, av_loss:0.691
step:1/111 avg loss:-2.346, visual_loss:-3.036, av_loss:-2.173
Valid Summary | End of Epoch 7 | Time 109.19s | Current time 2024-12-26 19:03:22.748127 |Valid Loss -1.090| 
Found new best model, dict saved
step:1/445 avg loss:-0.742, visual_loss:-0.106, av_loss:-0.901
step:1/445 avg loss:-2.259, visual_loss:-1.733, av_loss:-2.390
step:1/445 avg loss:-1.645, visual_loss:-1.535, av_loss:-1.672
started on logs/TDSE_SpkEmbMem/v_teacher

Namespace(mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', batch_size=3, max_length=4, num_workers=6, epochs=100, effec_batch_size=8, accu_grad=0, C=2, model_name='TDSE_SpkEmbMem', lr=0.001, max_norm=5, log_name='logs/TDSE_SpkEmbMem/v_teacher', use_tensorboard=1, continue_from='logs/TDSE_SpkEmbMem/v_teacher', local_rank=0, sample_rate=16000, win=512, hop_length=128, n_mels=80, shift_range='0,1', Self_enroll_amplitude_scaling='0.1,1', teacher_point=50, num_slots='1,5', loss_weight='28', distributed=True, world_size=3, speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800)

Total number of parameters: 25810661 

av_convtasnet(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): TemporalConvNet(
    (layer_norm): ChannelWiseLayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (bottleneck_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (tcn): ModuleList(
      (0-3): 4 x Sequential(
        (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (2): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (3): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (4): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (5): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (6): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (7): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (8): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
      )
    )
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (ve_conv1x1): ModuleList(
      (0-3): 4 x Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
    )
    (mask_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (preEmphasis): PreEmphasis()
  (spk_encoder): MelSpectrogram(
    (spectrogram): Spectrogram()
    (mel_scale): MelScale()
  )
  (spk_model): ECAPA_TDNN(
    (layer1): Conv1dReluBn(
      (conv): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,))
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (layer2): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer3): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer4): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (conv): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))
    (pool): ASTP(
      (linear1): Conv1d(1536, 128, kernel_size=(1,), stride=(1,))
      (linear2): Conv1d(128, 1536, kernel_size=(1,), stride=(1,))
    )
    (bn): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (linear): Linear(in_features=3072, out_features=256, bias=True)
    (bn2): Identity()
  )
  (self_att): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 1
step:1/1111 avg loss:-0.675, visual_loss:-0.106, av_loss:-0.817
step:1/1111 avg loss:-0.915, visual_loss:-0.293, av_loss:-1.070step:1/1111 avg loss:-3.164, visual_loss:-2.670, av_loss:-3.288

step:1001/1111 avg loss:-1.980, visual_loss:-1.463, av_loss:-2.109step:1001/1111 avg loss:-1.831, visual_loss:-1.347, av_loss:-1.952
step:1001/1111 avg loss:-1.898, visual_loss:-1.404, av_loss:-2.022

Train Summary | End of Epoch 1 | Time 1414.96s | Current time 2024-12-26 19:31:44.222660 |Train Loss -1.947| 
step:1/278 avg loss:-2.546, visual_loss:-2.063, av_loss:-2.667
step:1/278 avg loss:-0.181, visual_loss:0.356, av_loss:-0.316
step:1/278 avg loss:-2.237, visual_loss:-2.236, av_loss:-2.237
Valid Summary | End of Epoch 1 | Time 257.44s | Current time 2024-12-26 19:36:02.089846 |Valid Loss -1.480| 
Found new best model, dict saved
step:1/1111 avg loss:-2.553, visual_loss:-1.405, av_loss:-2.840step:1/1111 avg loss:-2.878, visual_loss:-0.089, av_loss:-3.575

step:1/1111 avg loss:-3.862, visual_loss:-3.716, av_loss:-3.899
step:1001/1111 avg loss:-2.549, visual_loss:-1.969, av_loss:-2.694step:1001/1111 avg loss:-2.510, visual_loss:-1.960, av_loss:-2.648

step:1001/1111 avg loss:-2.551, visual_loss:-1.923, av_loss:-2.709
Train Summary | End of Epoch 2 | Time 1466.47s | Current time 2024-12-26 20:00:31.365835 |Train Loss -2.547| 
step:1/278 avg loss:0.460, visual_loss:0.100, av_loss:0.550
step:1/278 avg loss:-2.683, visual_loss:-3.267, av_loss:-2.537
step:1/278 avg loss:-4.283, visual_loss:-3.613, av_loss:-4.451
Valid Summary | End of Epoch 2 | Time 275.05s | Current time 2024-12-26 20:05:06.516748 |Valid Loss -1.918| 
Found new best model, dict saved
step:1/1111 avg loss:-2.457, visual_loss:-2.015, av_loss:-2.568step:1/1111 avg loss:-5.260, visual_loss:-5.051, av_loss:-5.313

step:1/1111 avg loss:-1.810, visual_loss:-2.907, av_loss:-1.536
step:1001/1111 avg loss:-3.227, visual_loss:-2.623, av_loss:-3.378step:1001/1111 avg loss:-3.150, visual_loss:-2.546, av_loss:-3.301
step:1001/1111 avg loss:-3.134, visual_loss:-2.542, av_loss:-3.282

Train Summary | End of Epoch 3 | Time 1543.08s | Current time 2024-12-26 20:30:53.243264 |Train Loss -3.174| 
step:1/278 avg loss:-1.754, visual_loss:-0.749, av_loss:-2.005
step:1/278 avg loss:-3.746, visual_loss:-3.858, av_loss:-3.718
step:1/278 avg loss:-4.280, visual_loss:-2.693, av_loss:-4.677
Valid Summary | End of Epoch 3 | Time 282.38s | Current time 2024-12-26 20:35:35.631903 |Valid Loss -2.226| 
Found new best model, dict saved
step:1/1111 avg loss:-1.057, visual_loss:-0.460, av_loss:-1.206
step:1/1111 avg loss:-5.366, visual_loss:-5.321, av_loss:-5.377
step:1/1111 avg loss:-2.849, visual_loss:-2.307, av_loss:-2.985
step:1001/1111 avg loss:-3.735, visual_loss:-3.114, av_loss:-3.890step:1001/1111 avg loss:-3.717, visual_loss:-3.106, av_loss:-3.869

step:1001/1111 avg loss:-3.643, visual_loss:-2.993, av_loss:-3.805
Train Summary | End of Epoch 4 | Time 1553.33s | Current time 2024-12-26 21:01:31.319475 |Train Loss -3.684| 
step:1/278 avg loss:-2.299, visual_loss:-1.961, av_loss:-2.384
step:1/278 avg loss:-2.570, visual_loss:-1.630, av_loss:-2.806
step:1/278 avg loss:-4.390, visual_loss:-4.425, av_loss:-4.381
Valid Summary | End of Epoch 4 | Time 282.62s | Current time 2024-12-26 21:06:14.280035 |Valid Loss -2.539| 
Found new best model, dict saved
step:1/1111 avg loss:-3.971, visual_loss:-2.990, av_loss:-4.216
step:1/1111 avg loss:-4.851, visual_loss:-4.066, av_loss:-5.047
step:1/1111 avg loss:-7.280, visual_loss:-7.047, av_loss:-7.338
step:1001/1111 avg loss:-4.309, visual_loss:-3.709, av_loss:-4.458step:1001/1111 avg loss:-4.339, visual_loss:-3.718, av_loss:-4.494
step:1001/1111 avg loss:-4.256, visual_loss:-3.667, av_loss:-4.403

Train Summary | End of Epoch 5 | Time 1557.00s | Current time 2024-12-26 21:32:13.907085 |Train Loss -4.328| 
step:1/278 avg loss:-2.146, visual_loss:-1.971, av_loss:-2.189
step:1/278 avg loss:2.150, visual_loss:1.501, av_loss:2.312
step:1/278 avg loss:-5.264, visual_loss:-4.216, av_loss:-5.526
Valid Summary | End of Epoch 5 | Time 277.92s | Current time 2024-12-26 21:36:52.015102 |Valid Loss -2.195| 
step:1/1111 avg loss:-0.153, visual_loss:-0.781, av_loss:0.004
step:1/1111 avg loss:-3.478, visual_loss:-2.943, av_loss:-3.611
step:1/1111 avg loss:-5.581, visual_loss:-5.240, av_loss:-5.666
step:1001/1111 avg loss:-4.946, visual_loss:-4.347, av_loss:-5.096step:1001/1111 avg loss:-4.891, visual_loss:-4.346, av_loss:-5.027
step:1001/1111 avg loss:-4.858, visual_loss:-4.291, av_loss:-4.999

Train Summary | End of Epoch 6 | Time 1570.43s | Current time 2024-12-26 22:03:03.463248 |Train Loss -4.917| 
step:1/278 avg loss:-1.816, visual_loss:-1.635, av_loss:-1.861
step:1/278 avg loss:-5.676, visual_loss:-5.722, av_loss:-5.665
step:1/278 avg loss:-5.076, visual_loss:-4.824, av_loss:-5.139
Valid Summary | End of Epoch 6 | Time 282.83s | Current time 2024-12-26 22:07:46.299021 |Valid Loss -3.669| 
Found new best model, dict saved
step:1/1111 avg loss:-5.046, visual_loss:-4.313, av_loss:-5.229
step:1/1111 avg loss:-5.381, visual_loss:-5.280, av_loss:-5.406
step:1/1111 avg loss:-5.675, visual_loss:-5.054, av_loss:-5.830
step:1001/1111 avg loss:-5.432, visual_loss:-4.864, av_loss:-5.574step:1001/1111 avg loss:-5.413, visual_loss:-4.874, av_loss:-5.548

step:1001/1111 avg loss:-5.447, visual_loss:-4.857, av_loss:-5.594
Train Summary | End of Epoch 7 | Time 1565.39s | Current time 2024-12-26 22:33:53.168339 |Train Loss -5.459| 
step:1/278 avg loss:-2.977, visual_loss:-2.052, av_loss:-3.209
step:1/278 avg loss:-5.268, visual_loss:-5.612, av_loss:-5.182
step:1/278 avg loss:-4.530, visual_loss:-4.140, av_loss:-4.627
Valid Summary | End of Epoch 7 | Time 286.71s | Current time 2024-12-26 22:38:39.987032 |Valid Loss -3.889| 
Found new best model, dict saved
step:1/1111 avg loss:-3.420, visual_loss:0.216, av_loss:-4.329
step:1/1111 avg loss:-6.693, visual_loss:-3.336, av_loss:-7.533
step:1/1111 avg loss:-5.582, visual_loss:-2.666, av_loss:-6.311
step:1001/1111 avg loss:-5.857, visual_loss:-5.345, av_loss:-5.985step:1001/1111 avg loss:-5.896, visual_loss:-5.371, av_loss:-6.027

step:1001/1111 avg loss:-5.870, visual_loss:-5.356, av_loss:-5.999
Train Summary | End of Epoch 8 | Time 1565.47s | Current time 2024-12-26 23:04:46.680062 |Train Loss -5.871| 
step:1/278 avg loss:-5.784, visual_loss:-5.390, av_loss:-5.882
step:1/278 avg loss:-2.675, visual_loss:-1.190, av_loss:-3.047
step:1/278 avg loss:-6.487, visual_loss:-5.942, av_loss:-6.623
Valid Summary | End of Epoch 8 | Time 286.99s | Current time 2024-12-26 23:09:33.678724 |Valid Loss -4.408| 
Found new best model, dict saved
step:1/1111 avg loss:-5.254, visual_loss:-5.235, av_loss:-5.258
step:1/1111 avg loss:-6.723, visual_loss:-6.236, av_loss:-6.844
step:1/1111 avg loss:-8.583, visual_loss:-8.489, av_loss:-8.607
step:1001/1111 avg loss:-6.399, visual_loss:-5.967, av_loss:-6.507step:1001/1111 avg loss:-6.402, visual_loss:-5.922, av_loss:-6.522

step:1001/1111 avg loss:-6.289, visual_loss:-5.822, av_loss:-6.406
Train Summary | End of Epoch 9 | Time 1571.71s | Current time 2024-12-26 23:35:46.738890 |Train Loss -6.337| 
step:1/278 avg loss:-5.454, visual_loss:-5.608, av_loss:-5.416
step:1/278 avg loss:-3.308, visual_loss:-2.201, av_loss:-3.585
step:1/278 avg loss:-3.400, visual_loss:-4.205, av_loss:-3.198
Valid Summary | End of Epoch 9 | Time 287.64s | Current time 2024-12-26 23:40:34.418256 |Valid Loss -4.193| 
step:1/1111 avg loss:-5.912, visual_loss:-5.456, av_loss:-6.026
step:1/1111 avg loss:-6.569, visual_loss:-5.755, av_loss:-6.773
step:1/1111 avg loss:-5.801, visual_loss:-4.835, av_loss:-6.043
step:1001/1111 avg loss:-6.656, visual_loss:-6.155, av_loss:-6.782
step:1001/1111 avg loss:-6.696, visual_loss:-6.210, av_loss:-6.818
step:1001/1111 avg loss:-6.490, visual_loss:-5.959, av_loss:-6.622
Train Summary | End of Epoch 10 | Time 1583.93s | Current time 2024-12-27 00:06:58.762499 |Train Loss -6.614| 
step:1/278 avg loss:-6.744, visual_loss:-6.617, av_loss:-6.776
step:1/278 avg loss:-5.056, visual_loss:-3.170, av_loss:-5.528
step:1/278 avg loss:-6.951, visual_loss:-6.995, av_loss:-6.941
Valid Summary | End of Epoch 10 | Time 284.76s | Current time 2024-12-27 00:11:43.547489 |Valid Loss -5.101| 
Found new best model, dict saved
step:1/1111 avg loss:-6.303, visual_loss:-7.206, av_loss:-6.078
step:1/1111 avg loss:-9.233, visual_loss:-8.631, av_loss:-9.384
step:1/1111 avg loss:-5.200, visual_loss:-4.374, av_loss:-5.406
step:1001/1111 avg loss:-7.205, visual_loss:-6.827, av_loss:-7.299step:1001/1111 avg loss:-7.218, visual_loss:-6.808, av_loss:-7.321

step:1001/1111 avg loss:-7.034, visual_loss:-6.630, av_loss:-7.135
Train Summary | End of Epoch 11 | Time 1581.69s | Current time 2024-12-27 00:38:08.198926 |Train Loss -7.180| 
step:1/278 avg loss:-5.202, visual_loss:-6.070, av_loss:-4.985
step:1/278 avg loss:-3.833, visual_loss:-1.469, av_loss:-4.424
step:1/278 avg loss:-4.893, visual_loss:-6.177, av_loss:-4.573
Valid Summary | End of Epoch 11 | Time 285.21s | Current time 2024-12-27 00:42:53.415434 |Valid Loss -4.622| 
step:1/1111 avg loss:-8.513, visual_loss:-8.043, av_loss:-8.631
step:1/1111 avg loss:-6.955, visual_loss:-6.952, av_loss:-6.955
step:1/1111 avg loss:-9.421, visual_loss:-8.981, av_loss:-9.531
step:1001/1111 avg loss:-7.578, visual_loss:-7.191, av_loss:-7.675
step:1001/1111 avg loss:-7.525, visual_loss:-7.124, av_loss:-7.625
step:1001/1111 avg loss:-7.608, visual_loss:-7.194, av_loss:-7.712
Train Summary | End of Epoch 12 | Time 1568.67s | Current time 2024-12-27 01:09:02.400757 |Train Loss -7.503| 
step:1/278 avg loss:-3.162, visual_loss:-2.331, av_loss:-3.370
step:1/278 avg loss:-6.131, visual_loss:-5.403, av_loss:-6.313
step:1/278 avg loss:-7.834, visual_loss:-7.275, av_loss:-7.974
Valid Summary | End of Epoch 12 | Time 286.27s | Current time 2024-12-27 01:13:48.674111 |Valid Loss -4.235| 
step:1/1111 avg loss:-7.120, visual_loss:-5.820, av_loss:-7.445
step:1/1111 avg loss:-7.720, visual_loss:-6.126, av_loss:-8.119
step:1/1111 avg loss:-7.164, visual_loss:-3.727, av_loss:-8.023
step:1001/1111 avg loss:-7.600, visual_loss:-7.180, av_loss:-7.705step:1001/1111 avg loss:-7.622, visual_loss:-7.195, av_loss:-7.728

step:1001/1111 avg loss:-7.470, visual_loss:-6.987, av_loss:-7.591
Train Summary | End of Epoch 13 | Time 1573.31s | Current time 2024-12-27 01:40:02.367112 |Train Loss -7.582| 
step:1/278 avg loss:-5.929, visual_loss:-5.590, av_loss:-6.014
step:1/278 avg loss:-7.419, visual_loss:-7.582, av_loss:-7.378
step:1/278 avg loss:-6.422, visual_loss:-6.923, av_loss:-6.297
Valid Summary | End of Epoch 13 | Time 286.77s | Current time 2024-12-27 01:44:49.147716 |Valid Loss -5.884| 
Found new best model, dict saved
step:1/1111 avg loss:-9.569, visual_loss:-9.515, av_loss:-9.583
step:1/1111 avg loss:-8.262, visual_loss:-7.963, av_loss:-8.337
step:1/1111 avg loss:-7.384, visual_loss:-6.844, av_loss:-7.520
step:1001/1111 avg loss:-8.007, visual_loss:-7.643, av_loss:-8.098step:1001/1111 avg loss:-7.980, visual_loss:-7.619, av_loss:-8.070
step:1001/1111 avg loss:-7.919, visual_loss:-7.548, av_loss:-8.012

Train Summary | End of Epoch 14 | Time 1569.54s | Current time 2024-12-27 02:11:00.416233 |Train Loss -7.916| 
step:1/278 avg loss:-6.875, visual_loss:-7.412, av_loss:-6.741
step:1/278 avg loss:-5.405, visual_loss:-4.982, av_loss:-5.511
step:1/278 avg loss:-8.064, visual_loss:-6.869, av_loss:-8.363
Valid Summary | End of Epoch 14 | Time 289.59s | Current time 2024-12-27 02:15:50.012428 |Valid Loss -5.143| 
step:1/1111 avg loss:-3.751, visual_loss:-4.399, av_loss:-3.588
step:1/1111 avg loss:-5.886, visual_loss:-5.631, av_loss:-5.949
step:1/1111 avg loss:-5.535, visual_loss:-5.493, av_loss:-5.545
step:1001/1111 avg loss:-8.198, visual_loss:-7.821, av_loss:-8.292
step:1001/1111 avg loss:-8.163, visual_loss:-7.835, av_loss:-8.245
step:1001/1111 avg loss:-8.014, visual_loss:-7.624, av_loss:-8.112
Train Summary | End of Epoch 15 | Time 1569.05s | Current time 2024-12-27 02:41:59.976368 |Train Loss -8.123| 
step:1/278 avg loss:-5.708, visual_loss:-4.713, av_loss:-5.956
step:1/278 avg loss:-7.763, visual_loss:-7.179, av_loss:-7.909
step:1/278 avg loss:-4.588, visual_loss:-6.637, av_loss:-4.075
Valid Summary | End of Epoch 15 | Time 286.93s | Current time 2024-12-27 02:46:46.909403 |Valid Loss -5.971| 
Found new best model, dict saved
step:1/1111 avg loss:-5.691, visual_loss:-5.930, av_loss:-5.632
step:1/1111 avg loss:-8.887, visual_loss:-7.696, av_loss:-9.184
step:1/1111 avg loss:-8.548, visual_loss:-8.572, av_loss:-8.542
step:1001/1111 avg loss:-8.124, visual_loss:-7.731, av_loss:-8.222step:1001/1111 avg loss:-8.072, visual_loss:-7.659, av_loss:-8.176

step:1001/1111 avg loss:-7.947, visual_loss:-7.534, av_loss:-8.050
Train Summary | End of Epoch 16 | Time 1568.63s | Current time 2024-12-27 03:12:57.648538 |Train Loss -8.011| 
step:1/278 avg loss:-5.619, visual_loss:-5.513, av_loss:-5.645
step:1/278 avg loss:-6.649, visual_loss:-7.549, av_loss:-6.424
step:1/278 avg loss:-8.220, visual_loss:-8.240, av_loss:-8.216
Valid Summary | End of Epoch 16 | Time 289.06s | Current time 2024-12-27 03:17:46.750226 |Valid Loss -5.825| 
step:1/1111 avg loss:-10.318, visual_loss:-10.271, av_loss:-10.330
step:1/1111 avg loss:-10.278, visual_loss:-10.248, av_loss:-10.286
step:1/1111 avg loss:-9.446, visual_loss:-9.172, av_loss:-9.514
step:1001/1111 avg loss:-8.748, visual_loss:-8.449, av_loss:-8.823
step:1001/1111 avg loss:-8.743, visual_loss:-8.458, av_loss:-8.814
step:1001/1111 avg loss:-8.618, visual_loss:-8.308, av_loss:-8.696
Train Summary | End of Epoch 17 | Time 1570.02s | Current time 2024-12-27 03:43:57.149809 |Train Loss -8.710| 
step:1/278 avg loss:-7.689, visual_loss:-6.322, av_loss:-8.030
step:1/278 avg loss:-6.514, visual_loss:-5.719, av_loss:-6.713
step:1/278 avg loss:-8.904, visual_loss:-8.852, av_loss:-8.917
Valid Summary | End of Epoch 17 | Time 287.45s | Current time 2024-12-27 03:48:44.743273 |Valid Loss -6.010| 
Found new best model, dict saved
step:1/1111 avg loss:-4.974, visual_loss:-5.003, av_loss:-4.967
step:1/1111 avg loss:-10.276, visual_loss:-10.347, av_loss:-10.259
step:1/1111 avg loss:-8.539, visual_loss:-9.175, av_loss:-8.380
step:1001/1111 avg loss:-8.500, visual_loss:-8.200, av_loss:-8.575step:1001/1111 avg loss:-8.621, visual_loss:-8.279, av_loss:-8.707

step:1001/1111 avg loss:-8.351, visual_loss:-7.976, av_loss:-8.445
Train Summary | End of Epoch 18 | Time 1563.14s | Current time 2024-12-27 04:14:50.715683 |Train Loss -8.526| 
step:1/278 avg loss:-8.691, visual_loss:-8.648, av_loss:-8.701
step:1/278 avg loss:-7.064, visual_loss:-5.644, av_loss:-7.418
step:1/278 avg loss:-9.256, visual_loss:-8.881, av_loss:-9.350
Valid Summary | End of Epoch 18 | Time 287.43s | Current time 2024-12-27 04:19:38.146564 |Valid Loss -7.041| 
Found new best model, dict saved
step:1/1111 avg loss:-8.734, visual_loss:-8.611, av_loss:-8.764
step:1/1111 avg loss:-9.964, visual_loss:-9.553, av_loss:-10.067
step:1/1111 avg loss:-10.516, visual_loss:-10.591, av_loss:-10.498
step:1001/1111 avg loss:-8.618, visual_loss:-8.312, av_loss:-8.695
step:1001/1111 avg loss:-8.619, visual_loss:-8.246, av_loss:-8.712
step:1001/1111 avg loss:-8.542, visual_loss:-8.193, av_loss:-8.630
Train Summary | End of Epoch 19 | Time 1564.08s | Current time 2024-12-27 04:45:43.715261 |Train Loss -8.615| 
step:1/278 avg loss:-3.762, visual_loss:-3.506, av_loss:-3.825
step:1/278 avg loss:-8.936, visual_loss:-8.599, av_loss:-9.021
step:1/278 avg loss:-9.463, visual_loss:-9.180, av_loss:-9.533
Valid Summary | End of Epoch 19 | Time 287.91s | Current time 2024-12-27 04:50:31.631851 |Valid Loss -6.330| 
step:1/1111 avg loss:-9.281, visual_loss:-8.654, av_loss:-9.438
step:1/1111 avg loss:-9.068, visual_loss:-8.965, av_loss:-9.094
step:1/1111 avg loss:-7.191, visual_loss:-6.853, av_loss:-7.275
step:1001/1111 avg loss:-9.191, visual_loss:-8.909, av_loss:-9.261step:1001/1111 avg loss:-9.200, visual_loss:-8.914, av_loss:-9.272

step:1001/1111 avg loss:-9.089, visual_loss:-8.786, av_loss:-9.165
Train Summary | End of Epoch 20 | Time 1555.77s | Current time 2024-12-27 05:16:28.142437 |Train Loss -9.130| 
step:1/278 avg loss:-6.667, visual_loss:-5.069, av_loss:-7.067
step:1/278 avg loss:-9.449, visual_loss:-9.049, av_loss:-9.549
step:1/278 avg loss:-9.698, visual_loss:-8.972, av_loss:-9.879
Valid Summary | End of Epoch 20 | Time 288.02s | Current time 2024-12-27 05:21:16.329095 |Valid Loss -6.953| 
step:1/1111 avg loss:-11.798, visual_loss:-11.754, av_loss:-11.809
step:1/1111 avg loss:-6.661, visual_loss:-5.494, av_loss:-6.953
step:1/1111 avg loss:-9.526, visual_loss:-9.303, av_loss:-9.581
step:1001/1111 avg loss:-8.952, visual_loss:-8.645, av_loss:-9.028step:1001/1111 avg loss:-8.971, visual_loss:-8.625, av_loss:-9.057
step:1001/1111 avg loss:-8.844, visual_loss:-8.481, av_loss:-8.935

Train Summary | End of Epoch 21 | Time 1568.43s | Current time 2024-12-27 05:47:26.778505 |Train Loss -8.964| 
step:1/278 avg loss:-6.633, visual_loss:-6.753, av_loss:-6.603
step:1/278 avg loss:-8.719, visual_loss:-9.955, av_loss:-8.410
step:1/278 avg loss:-9.747, visual_loss:-9.449, av_loss:-9.822
Valid Summary | End of Epoch 21 | Time 286.26s | Current time 2024-12-27 05:52:13.096888 |Valid Loss -7.293| 
Found new best model, dict saved
step:1/1111 avg loss:-9.834, visual_loss:-9.770, av_loss:-9.850
step:1/1111 avg loss:-10.984, visual_loss:-10.982, av_loss:-10.984
step:1/1111 avg loss:-9.874, visual_loss:-9.765, av_loss:-9.901
step:1001/1111 avg loss:-9.323, visual_loss:-9.081, av_loss:-9.383step:1001/1111 avg loss:-9.361, visual_loss:-9.054, av_loss:-9.437

step:1001/1111 avg loss:-9.139, visual_loss:-8.825, av_loss:-9.217
Train Summary | End of Epoch 22 | Time 1568.69s | Current time 2024-12-27 06:18:24.318490 |Train Loss -9.284| 
step:1/278 avg loss:-5.872, visual_loss:-6.412, av_loss:-5.737
step:1/278 avg loss:-8.941, visual_loss:-8.980, av_loss:-8.932
step:1/278 avg loss:-9.902, visual_loss:-9.826, av_loss:-9.921
Valid Summary | End of Epoch 22 | Time 288.05s | Current time 2024-12-27 06:23:12.380686 |Valid Loss -7.241| 
step:1/1111 avg loss:-10.177, visual_loss:-10.294, av_loss:-10.148
step:1/1111 avg loss:-9.103, visual_loss:-9.367, av_loss:-9.037
step:1/1111 avg loss:-12.143, visual_loss:-11.988, av_loss:-12.182
step:1001/1111 avg loss:-9.395, visual_loss:-9.137, av_loss:-9.459
step:1001/1111 avg loss:-9.387, visual_loss:-9.137, av_loss:-9.449
step:1001/1111 avg loss:-9.259, visual_loss:-8.957, av_loss:-9.334
Train Summary | End of Epoch 23 | Time 1568.89s | Current time 2024-12-27 06:49:21.923478 |Train Loss -9.307| 
step:1/278 avg loss:-9.037, visual_loss:-8.709, av_loss:-9.119
step:1/278 avg loss:-6.414, visual_loss:-5.340, av_loss:-6.682
step:1/278 avg loss:-1.234, visual_loss:-2.641, av_loss:-0.883
Valid Summary | End of Epoch 23 | Time 287.42s | Current time 2024-12-27 06:54:09.345011 |Valid Loss -6.921| 
step:1/1111 avg loss:-8.373, visual_loss:-2.241, av_loss:-9.907
step:1/1111 avg loss:-8.971, visual_loss:-6.573, av_loss:-9.571
step:1/1111 avg loss:-10.015, visual_loss:-9.842, av_loss:-10.058
step:1001/1111 avg loss:-9.435, visual_loss:-9.147, av_loss:-9.507step:1001/1111 avg loss:-9.462, visual_loss:-9.178, av_loss:-9.533

step:1001/1111 avg loss:-9.348, visual_loss:-9.060, av_loss:-9.420
Train Summary | End of Epoch 24 | Time 1571.77s | Current time 2024-12-27 07:20:21.947682 |Train Loss -9.390| 
step:1/278 avg loss:-7.094, visual_loss:-6.609, av_loss:-7.216
step:1/278 avg loss:-9.104, visual_loss:-9.461, av_loss:-9.015
step:1/278 avg loss:-9.623, visual_loss:-9.110, av_loss:-9.751
Valid Summary | End of Epoch 24 | Time 286.84s | Current time 2024-12-27 07:25:08.802693 |Valid Loss -6.769| 
step:1/1111 avg loss:-9.500, visual_loss:-9.360, av_loss:-9.535
step:1/1111 avg loss:-10.150, visual_loss:-10.257, av_loss:-10.124
step:1/1111 avg loss:-8.461, visual_loss:-7.691, av_loss:-8.653
step:1001/1111 avg loss:-9.545, visual_loss:-9.297, av_loss:-9.607
step:1001/1111 avg loss:-9.536, visual_loss:-9.268, av_loss:-9.603
step:1001/1111 avg loss:-9.450, visual_loss:-9.149, av_loss:-9.525
Train Summary | End of Epoch 25 | Time 1566.11s | Current time 2024-12-27 07:51:15.289737 |Train Loss -9.511| 
step:1/278 avg loss:-7.531, visual_loss:-7.647, av_loss:-7.502
step:1/278 avg loss:-9.457, visual_loss:-9.622, av_loss:-9.415
step:1/278 avg loss:-10.186, visual_loss:-9.974, av_loss:-10.239
Valid Summary | End of Epoch 25 | Time 284.48s | Current time 2024-12-27 07:55:59.771158 |Valid Loss -7.434| 
Found new best model, dict saved
step:1/1111 avg loss:-11.341, visual_loss:-11.159, av_loss:-11.387
step:1/1111 avg loss:-11.060, visual_loss:-10.999, av_loss:-11.075
step:1/1111 avg loss:-10.685, visual_loss:-10.578, av_loss:-10.712
step:1001/1111 avg loss:-9.797, visual_loss:-9.568, av_loss:-9.854step:1001/1111 avg loss:-9.832, visual_loss:-9.585, av_loss:-9.894

step:1001/1111 avg loss:-9.751, visual_loss:-9.505, av_loss:-9.812
Train Summary | End of Epoch 26 | Time 1569.75s | Current time 2024-12-27 08:22:11.082411 |Train Loss -9.799| 
step:1/278 avg loss:-8.113, visual_loss:-8.026, av_loss:-8.135
step:1/278 avg loss:-9.753, visual_loss:-9.872, av_loss:-9.723
step:1/278 avg loss:-6.262, visual_loss:-6.794, av_loss:-6.128
Valid Summary | End of Epoch 26 | Time 286.92s | Current time 2024-12-27 08:26:58.005995 |Valid Loss -7.635| 
Found new best model, dict saved
step:1/1111 avg loss:-12.047, visual_loss:-11.998, av_loss:-12.059
step:1/1111 avg loss:-10.736, visual_loss:-10.562, av_loss:-10.779
step:1/1111 avg loss:-9.070, visual_loss:-9.093, av_loss:-9.065
step:1001/1111 avg loss:-9.943, visual_loss:-9.701, av_loss:-10.004
step:1001/1111 avg loss:-9.757, visual_loss:-9.507, av_loss:-9.820
step:1001/1111 avg loss:-9.973, visual_loss:-9.767, av_loss:-10.025
Train Summary | End of Epoch 27 | Time 1563.50s | Current time 2024-12-27 08:53:02.544589 |Train Loss -9.839| 
step:1/278 avg loss:-4.023, visual_loss:-5.034, av_loss:-3.771
step:1/278 avg loss:-9.276, visual_loss:-9.201, av_loss:-9.295
step:1/278 avg loss:-9.748, visual_loss:-9.462, av_loss:-9.820
Valid Summary | End of Epoch 27 | Time 287.43s | Current time 2024-12-27 08:57:50.025136 |Valid Loss -7.160| 
step:1/1111 avg loss:-11.508, visual_loss:-11.415, av_loss:-11.531
step:1/1111 avg loss:-9.553, visual_loss:-8.672, av_loss:-9.774
step:1/1111 avg loss:-9.066, visual_loss:-8.875, av_loss:-9.114
step:1001/1111 avg loss:-9.905, visual_loss:-9.674, av_loss:-9.962step:1001/1111 avg loss:-9.866, visual_loss:-9.617, av_loss:-9.929
step:1001/1111 avg loss:-9.771, visual_loss:-9.505, av_loss:-9.837

Train Summary | End of Epoch 28 | Time 1579.44s | Current time 2024-12-27 09:24:11.188935 |Train Loss -9.840| 
step:1/278 avg loss:-8.095, visual_loss:-8.164, av_loss:-8.077
step:1/278 avg loss:-9.850, visual_loss:-10.128, av_loss:-9.781
step:1/278 avg loss:-10.338, visual_loss:-10.128, av_loss:-10.391
Valid Summary | End of Epoch 28 | Time 288.44s | Current time 2024-12-27 09:29:00.483654 |Valid Loss -7.839| 
Found new best model, dict saved
step:1/1111 avg loss:-9.106, visual_loss:-8.638, av_loss:-9.223
step:1/1111 avg loss:-11.892, visual_loss:-11.152, av_loss:-12.076
step:1/1111 avg loss:-10.270, visual_loss:-10.114, av_loss:-10.309
started on logs/TDSE_SpkEmbMem/v_teacher

Namespace(mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', batch_size=3, max_length=4, num_workers=6, epochs=100, effec_batch_size=8, accu_grad=0, C=2, model_name='TDSE_SpkEmbMem', lr=0.001, max_norm=5, log_name='logs/TDSE_SpkEmbMem/v_teacher', use_tensorboard=1, continue_from='logs/TDSE_SpkEmbMem/v_teacher', local_rank=0, sample_rate=16000, win=512, hop_length=128, n_mels=80, shift_range='0,1', Self_enroll_amplitude_scaling='0.1,1', teacher_point=50, num_slots='1,5', loss_weight='28', distributed=True, world_size=2, speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800)

Total number of parameters: 25810661 

av_convtasnet(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): TemporalConvNet(
    (layer_norm): ChannelWiseLayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (bottleneck_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (tcn): ModuleList(
      (0-3): 4 x Sequential(
        (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (2): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (3): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (4): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (5): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (6): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (7): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (8): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
      )
    )
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (ve_conv1x1): ModuleList(
      (0-3): 4 x Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
    )
    (mask_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (preEmphasis): PreEmphasis()
  (spk_encoder): MelSpectrogram(
    (spectrogram): Spectrogram()
    (mel_scale): MelScale()
  )
  (spk_model): ECAPA_TDNN(
    (layer1): Conv1dReluBn(
      (conv): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,))
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (layer2): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer3): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer4): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (conv): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))
    (pool): ASTP(
      (linear1): Conv1d(1536, 128, kernel_size=(1,), stride=(1,))
      (linear2): Conv1d(128, 1536, kernel_size=(1,), stride=(1,))
    )
    (bn): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (linear): Linear(in_features=3072, out_features=256, bias=True)
    (bn2): Identity()
  )
  (self_att): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 29
step:1/3334 avg loss:-10.879, visual_loss:-10.545, av_loss:-10.962
step:1/3334 avg loss:-5.497, visual_loss:-5.418, av_loss:-5.516
step:1001/3334 avg loss:-8.464, visual_loss:-7.944, av_loss:-8.594step:1001/3334 avg loss:-8.426, visual_loss:-7.936, av_loss:-8.549

step:2001/3334 avg loss:-8.500, visual_loss:-8.000, av_loss:-8.625step:2001/3334 avg loss:-8.509, visual_loss:-8.039, av_loss:-8.627

step:3001/3334 avg loss:-8.573, visual_loss:-8.086, av_loss:-8.695step:3001/3334 avg loss:-8.591, visual_loss:-8.116, av_loss:-8.709

Train Summary | End of Epoch 29 | Time 3676.02s | Current time 2024-12-27 10:41:26.721885 |Train Loss -8.577| 
step:1/834 avg loss:-7.096, visual_loss:-10.866, av_loss:-6.153
step:1/834 avg loss:-7.794, visual_loss:-8.363, av_loss:-7.652
step:301/834 avg loss:-7.580, visual_loss:-7.632, av_loss:-7.567
step:301/834 avg loss:-7.381, visual_loss:-7.447, av_loss:-7.365
step:601/834 avg loss:-7.488, visual_loss:-7.533, av_loss:-7.477
step:601/834 avg loss:-7.338, visual_loss:-7.489, av_loss:-7.300
Valid Summary | End of Epoch 29 | Time 617.06s | Current time 2024-12-27 10:51:44.364602 |Valid Loss -7.391| 
step:1/3334 avg loss:-10.243, visual_loss:-10.145, av_loss:-10.268
step:1/3334 avg loss:-5.766, visual_loss:-5.609, av_loss:-5.805
step:1001/3334 avg loss:-9.239, visual_loss:-8.810, av_loss:-9.347
step:1001/3334 avg loss:-9.011, visual_loss:-8.567, av_loss:-9.122
step:2001/3334 avg loss:-9.183, visual_loss:-8.785, av_loss:-9.282
step:2001/3334 avg loss:-9.073, visual_loss:-8.606, av_loss:-9.190
step:3001/3334 avg loss:-9.122, visual_loss:-8.699, av_loss:-9.228step:3001/3334 avg loss:-9.012, visual_loss:-8.546, av_loss:-9.128

Train Summary | End of Epoch 30 | Time 3947.34s | Current time 2024-12-27 11:57:32.689167 |Train Loss -9.060| 
step:1/834 avg loss:-10.642, visual_loss:-9.047, av_loss:-11.041
step:1/834 avg loss:-9.098, visual_loss:-9.082, av_loss:-9.102
step:301/834 avg loss:-8.215, visual_loss:-8.209, av_loss:-8.216
step:301/834 avg loss:-8.127, visual_loss:-8.247, av_loss:-8.097
step:601/834 avg loss:-8.152, visual_loss:-8.144, av_loss:-8.153
step:601/834 avg loss:-8.071, visual_loss:-8.242, av_loss:-8.028
Valid Summary | End of Epoch 30 | Time 610.32s | Current time 2024-12-27 12:07:43.158495 |Valid Loss -8.086| 
Found new best model, dict saved
step:1/3334 avg loss:-7.981, visual_loss:-7.739, av_loss:-8.042
step:1/3334 avg loss:-11.323, visual_loss:-10.689, av_loss:-11.481
step:1001/3334 avg loss:-9.535, visual_loss:-9.135, av_loss:-9.635step:1001/3334 avg loss:-9.426, visual_loss:-9.019, av_loss:-9.528

step:2001/3334 avg loss:-9.479, visual_loss:-9.097, av_loss:-9.574
step:2001/3334 avg loss:-9.301, visual_loss:-8.896, av_loss:-9.402
step:3001/3334 avg loss:-9.359, visual_loss:-8.971, av_loss:-9.456
step:3001/3334 avg loss:-9.214, visual_loss:-8.812, av_loss:-9.314
Train Summary | End of Epoch 31 | Time 3962.82s | Current time 2024-12-27 13:13:49.978634 |Train Loss -9.282| 
step:1/834 avg loss:-8.258, visual_loss:-7.930, av_loss:-8.339
step:1/834 avg loss:-11.539, visual_loss:-11.967, av_loss:-11.432
step:301/834 avg loss:-7.592, visual_loss:-7.532, av_loss:-7.607
step:301/834 avg loss:-7.777, visual_loss:-7.725, av_loss:-7.791
step:601/834 avg loss:-7.601, visual_loss:-7.568, av_loss:-7.609
step:601/834 avg loss:-7.702, visual_loss:-7.703, av_loss:-7.701
Valid Summary | End of Epoch 31 | Time 601.11s | Current time 2024-12-27 13:23:51.225738 |Valid Loss -7.581| 
step:1/3334 avg loss:-11.140, visual_loss:-11.061, av_loss:-11.159
step:1/3334 avg loss:-11.375, visual_loss:-11.424, av_loss:-11.362
step:1001/3334 avg loss:-9.142, visual_loss:-8.772, av_loss:-9.234
step:1001/3334 avg loss:-9.066, visual_loss:-8.622, av_loss:-9.177
step:2001/3334 avg loss:-9.423, visual_loss:-9.099, av_loss:-9.504step:2001/3334 avg loss:-9.337, visual_loss:-8.930, av_loss:-9.439

step:3001/3334 avg loss:-9.458, visual_loss:-9.115, av_loss:-9.543step:3001/3334 avg loss:-9.378, visual_loss:-8.977, av_loss:-9.478

Train Summary | End of Epoch 32 | Time 4007.48s | Current time 2024-12-27 14:30:39.730871 |Train Loss -9.451| 
step:1/834 avg loss:-9.162, visual_loss:-8.927, av_loss:-9.221
step:1/834 avg loss:-11.712, visual_loss:-12.578, av_loss:-11.495
step:301/834 avg loss:-8.807, visual_loss:-8.935, av_loss:-8.775
step:301/834 avg loss:-8.368, visual_loss:-8.821, av_loss:-8.255
step:601/834 avg loss:-8.660, visual_loss:-8.882, av_loss:-8.605
step:601/834 avg loss:-8.444, visual_loss:-8.867, av_loss:-8.338
Valid Summary | End of Epoch 32 | Time 598.32s | Current time 2024-12-27 14:40:38.275567 |Valid Loss -8.541| 
Found new best model, dict saved
step:1/3334 avg loss:-9.680, visual_loss:-9.604, av_loss:-9.699
step:1/3334 avg loss:-8.803, visual_loss:-8.465, av_loss:-8.888
step:1001/3334 avg loss:-9.886, visual_loss:-9.596, av_loss:-9.959
step:1001/3334 avg loss:-9.600, visual_loss:-9.172, av_loss:-9.707
step:2001/3334 avg loss:-9.841, visual_loss:-9.535, av_loss:-9.917step:2001/3334 avg loss:-9.602, visual_loss:-9.225, av_loss:-9.696

step:3001/3334 avg loss:-9.752, visual_loss:-9.442, av_loss:-9.829step:3001/3334 avg loss:-9.634, visual_loss:-9.269, av_loss:-9.725

Train Summary | End of Epoch 33 | Time 3432.41s | Current time 2024-12-27 15:37:53.364205 |Train Loss -9.673| 
step:1/834 avg loss:-9.203, visual_loss:-8.246, av_loss:-9.442
step:1/834 avg loss:-11.260, visual_loss:-11.342, av_loss:-11.239
step:301/834 avg loss:-8.407, visual_loss:-8.394, av_loss:-8.411
step:301/834 avg loss:-8.423, visual_loss:-8.445, av_loss:-8.418
step:601/834 avg loss:-8.418, visual_loss:-8.409, av_loss:-8.421
step:601/834 avg loss:-8.368, visual_loss:-8.402, av_loss:-8.360
Valid Summary | End of Epoch 33 | Time 598.13s | Current time 2024-12-27 15:47:52.473536 |Valid Loss -8.370| 
step:1/3334 avg loss:-8.649, visual_loss:-9.032, av_loss:-8.553
step:1/3334 avg loss:-7.310, visual_loss:-7.618, av_loss:-7.233
step:1001/3334 avg loss:-9.933, visual_loss:-9.608, av_loss:-10.015step:1001/3334 avg loss:-9.809, visual_loss:-9.504, av_loss:-9.886

step:2001/3334 avg loss:-9.923, visual_loss:-9.627, av_loss:-9.997step:2001/3334 avg loss:-9.798, visual_loss:-9.477, av_loss:-9.878

step:3001/3334 avg loss:-9.870, visual_loss:-9.553, av_loss:-9.949
step:3001/3334 avg loss:-9.745, visual_loss:-9.417, av_loss:-9.827
Train Summary | End of Epoch 34 | Time 3993.63s | Current time 2024-12-27 16:54:27.417711 |Train Loss -9.813| 
step:1/834 avg loss:-9.722, visual_loss:-8.830, av_loss:-9.946
step:1/834 avg loss:-11.957, visual_loss:-12.254, av_loss:-11.883
step:301/834 avg loss:-8.109, visual_loss:-8.268, av_loss:-8.069
step:301/834 avg loss:-7.860, visual_loss:-8.156, av_loss:-7.786
step:601/834 avg loss:-8.014, visual_loss:-8.184, av_loss:-7.971
step:601/834 avg loss:-7.830, visual_loss:-8.072, av_loss:-7.770
Valid Summary | End of Epoch 34 | Time 599.37s | Current time 2024-12-27 17:04:26.851177 |Valid Loss -7.889| 
step:1/3334 avg loss:-10.115, visual_loss:-7.686, av_loss:-10.723
step:1/3334 avg loss:-7.750, visual_loss:-5.928, av_loss:-8.205
step:1001/3334 avg loss:-9.807, visual_loss:-9.470, av_loss:-9.891step:1001/3334 avg loss:-9.729, visual_loss:-9.409, av_loss:-9.809

step:2001/3334 avg loss:-9.816, visual_loss:-9.492, av_loss:-9.897
step:2001/3334 avg loss:-9.775, visual_loss:-9.446, av_loss:-9.857
step:3001/3334 avg loss:-9.878, visual_loss:-9.570, av_loss:-9.956step:3001/3334 avg loss:-9.825, visual_loss:-9.510, av_loss:-9.903

Train Summary | End of Epoch 35 | Time 3989.78s | Current time 2024-12-27 18:10:57.253653 |Train Loss -9.871| 
step:1/834 avg loss:-9.659, visual_loss:-10.798, av_loss:-9.375
step:1/834 avg loss:-8.362, visual_loss:-12.378, av_loss:-7.358
step:301/834 avg loss:-8.622, visual_loss:-8.745, av_loss:-8.592
step:301/834 avg loss:-8.642, visual_loss:-8.849, av_loss:-8.590
step:601/834 avg loss:-8.463, visual_loss:-8.677, av_loss:-8.410
step:601/834 avg loss:-8.500, visual_loss:-8.790, av_loss:-8.427
Valid Summary | End of Epoch 35 | Time 604.11s | Current time 2024-12-27 18:21:01.810605 |Valid Loss -8.450| 
step:1/3334 avg loss:-9.918, visual_loss:-7.529, av_loss:-10.515
step:1/3334 avg loss:-9.685, visual_loss:-9.473, av_loss:-9.738
step:1001/3334 avg loss:-10.110, visual_loss:-9.828, av_loss:-10.181step:1001/3334 avg loss:-10.142, visual_loss:-9.870, av_loss:-10.210

step:2001/3334 avg loss:-10.134, visual_loss:-9.867, av_loss:-10.201
step:2001/3334 avg loss:-10.151, visual_loss:-9.869, av_loss:-10.222
step:3001/3334 avg loss:-10.168, visual_loss:-9.887, av_loss:-10.238
step:3001/3334 avg loss:-10.137, visual_loss:-9.856, av_loss:-10.207
Train Summary | End of Epoch 36 | Time 3978.92s | Current time 2024-12-27 19:27:21.405158 |Train Loss -10.152| 
step:1/834 avg loss:-9.178, visual_loss:-10.322, av_loss:-8.892
step:1/834 avg loss:-6.219, visual_loss:-6.198, av_loss:-6.224
step:301/834 avg loss:-7.802, visual_loss:-7.892, av_loss:-7.779
step:301/834 avg loss:-7.769, visual_loss:-7.823, av_loss:-7.756
step:601/834 avg loss:-7.889, visual_loss:-7.938, av_loss:-7.876
step:601/834 avg loss:-7.799, visual_loss:-7.869, av_loss:-7.782
Valid Summary | End of Epoch 36 | Time 604.94s | Current time 2024-12-27 19:37:26.496765 |Valid Loss -7.808| 
step:1/3334 avg loss:-10.310, visual_loss:-5.988, av_loss:-11.390
step:1/3334 avg loss:-8.736, visual_loss:-10.340, av_loss:-8.336
step:1001/3334 avg loss:-10.431, visual_loss:-10.170, av_loss:-10.496
step:1001/3334 avg loss:-10.372, visual_loss:-10.112, av_loss:-10.437
step:2001/3334 avg loss:-10.333, visual_loss:-10.090, av_loss:-10.394
step:2001/3334 avg loss:-10.221, visual_loss:-9.928, av_loss:-10.294
step:3001/3334 avg loss:-10.292, visual_loss:-10.039, av_loss:-10.355step:3001/3334 avg loss:-10.137, visual_loss:-9.830, av_loss:-10.213

Train Summary | End of Epoch 37 | Time 3944.59s | Current time 2024-12-27 20:43:11.891788 |Train Loss -10.206| 
step:1/834 avg loss:-10.745, visual_loss:-10.480, av_loss:-10.811
step:1/834 avg loss:-12.699, visual_loss:-12.617, av_loss:-12.719
step:301/834 avg loss:-9.243, visual_loss:-9.250, av_loss:-9.241
step:301/834 avg loss:-9.002, visual_loss:-9.221, av_loss:-8.947
step:601/834 avg loss:-9.155, visual_loss:-9.280, av_loss:-9.123
step:601/834 avg loss:-8.966, visual_loss:-9.181, av_loss:-8.912
Valid Summary | End of Epoch 37 | Time 622.86s | Current time 2024-12-27 20:53:34.800658 |Valid Loss -9.012| 
Found new best model, dict saved
step:1/3334 avg loss:-11.932, visual_loss:-11.910, av_loss:-11.938
step:1/3334 avg loss:-9.912, visual_loss:-9.832, av_loss:-9.932
step:1001/3334 avg loss:-10.468, visual_loss:-10.238, av_loss:-10.526step:1001/3334 avg loss:-10.533, visual_loss:-10.293, av_loss:-10.593

step:2001/3334 avg loss:-10.416, visual_loss:-10.172, av_loss:-10.477
step:2001/3334 avg loss:-10.474, visual_loss:-10.233, av_loss:-10.535
step:3001/3334 avg loss:-10.416, visual_loss:-10.173, av_loss:-10.477
step:3001/3334 avg loss:-10.426, visual_loss:-10.174, av_loss:-10.489
Train Summary | End of Epoch 38 | Time 3959.42s | Current time 2024-12-27 21:59:37.150446 |Train Loss -10.419| 
step:1/834 avg loss:-5.705, visual_loss:-6.141, av_loss:-5.596
step:1/834 avg loss:-12.882, visual_loss:-12.716, av_loss:-12.924
step:301/834 avg loss:-9.253, visual_loss:-9.346, av_loss:-9.230
step:301/834 avg loss:-9.140, visual_loss:-9.391, av_loss:-9.077
step:601/834 avg loss:-9.098, visual_loss:-9.318, av_loss:-9.042
step:601/834 avg loss:-9.069, visual_loss:-9.366, av_loss:-8.995
Valid Summary | End of Epoch 38 | Time 599.01s | Current time 2024-12-27 22:09:36.258145 |Valid Loss -9.049| 
Found new best model, dict saved
step:1/3334 avg loss:-10.057, visual_loss:-9.569, av_loss:-10.179
step:1/3334 avg loss:-9.359, visual_loss:-8.698, av_loss:-9.524
step:1001/3334 avg loss:-10.603, visual_loss:-10.382, av_loss:-10.658step:1001/3334 avg loss:-10.560, visual_loss:-10.300, av_loss:-10.625

step:2001/3334 avg loss:-10.469, visual_loss:-10.236, av_loss:-10.527step:2001/3334 avg loss:-10.394, visual_loss:-10.145, av_loss:-10.457

step:3001/3334 avg loss:-10.455, visual_loss:-10.201, av_loss:-10.518
step:3001/3334 avg loss:-10.365, visual_loss:-10.101, av_loss:-10.431
Train Summary | End of Epoch 39 | Time 3948.41s | Current time 2024-12-27 23:15:25.372612 |Train Loss -10.447| 
step:1/834 avg loss:-8.132, visual_loss:-6.908, av_loss:-8.438
step:1/834 avg loss:-12.308, visual_loss:-12.085, av_loss:-12.364
step:301/834 avg loss:-8.500, visual_loss:-8.461, av_loss:-8.510
step:301/834 avg loss:-8.508, visual_loss:-8.533, av_loss:-8.501
step:601/834 avg loss:-8.433, visual_loss:-8.429, av_loss:-8.434
step:601/834 avg loss:-8.388, visual_loss:-8.468, av_loss:-8.368
Valid Summary | End of Epoch 39 | Time 662.36s | Current time 2024-12-27 23:26:27.870197 |Valid Loss -8.409| 
step:1/3334 avg loss:-7.157, visual_loss:-7.129, av_loss:-7.164
step:1/3334 avg loss:-9.856, visual_loss:-9.808, av_loss:-9.868
step:1001/3334 avg loss:-10.688, visual_loss:-10.474, av_loss:-10.742
step:1001/3334 avg loss:-10.592, visual_loss:-10.351, av_loss:-10.652
step:2001/3334 avg loss:-10.667, visual_loss:-10.433, av_loss:-10.726
step:2001/3334 avg loss:-10.598, visual_loss:-10.359, av_loss:-10.657
step:3001/3334 avg loss:-10.595, visual_loss:-10.353, av_loss:-10.656step:3001/3334 avg loss:-10.505, visual_loss:-10.250, av_loss:-10.568

Train Summary | End of Epoch 40 | Time 3959.11s | Current time 2024-12-28 00:32:28.129713 |Train Loss -10.557| 
step:1/834 avg loss:-10.435, visual_loss:-10.083, av_loss:-10.524
step:1/834 avg loss:-12.843, visual_loss:-13.009, av_loss:-12.801
step:301/834 avg loss:-9.563, visual_loss:-9.607, av_loss:-9.552
step:301/834 avg loss:-9.243, visual_loss:-9.457, av_loss:-9.190
step:601/834 avg loss:-9.452, visual_loss:-9.588, av_loss:-9.418
step:601/834 avg loss:-9.284, visual_loss:-9.495, av_loss:-9.231
Valid Summary | End of Epoch 40 | Time 601.31s | Current time 2024-12-28 00:42:29.473023 |Valid Loss -9.349| 
Found new best model, dict saved
step:1/3334 avg loss:-10.039, visual_loss:-9.774, av_loss:-10.105
step:1/3334 avg loss:-10.980, visual_loss:-10.981, av_loss:-10.980
step:1001/3334 avg loss:-10.544, visual_loss:-10.255, av_loss:-10.616step:1001/3334 avg loss:-10.458, visual_loss:-10.191, av_loss:-10.525

step:2001/3334 avg loss:-10.616, visual_loss:-10.359, av_loss:-10.680step:2001/3334 avg loss:-10.489, visual_loss:-10.225, av_loss:-10.554

step:3001/3334 avg loss:-10.645, visual_loss:-10.386, av_loss:-10.710step:3001/3334 avg loss:-10.520, visual_loss:-10.254, av_loss:-10.587

Train Summary | End of Epoch 41 | Time 3976.75s | Current time 2024-12-28 01:48:49.696521 |Train Loss -10.566| 
step:1/834 avg loss:-10.316, visual_loss:-10.070, av_loss:-10.378
step:1/834 avg loss:-13.017, visual_loss:-13.014, av_loss:-13.018
step:301/834 avg loss:-8.929, visual_loss:-9.158, av_loss:-8.871
step:301/834 avg loss:-8.943, visual_loss:-9.218, av_loss:-8.874
step:601/834 avg loss:-8.934, visual_loss:-9.129, av_loss:-8.886
step:601/834 avg loss:-8.842, visual_loss:-9.151, av_loss:-8.765
Valid Summary | End of Epoch 41 | Time 603.29s | Current time 2024-12-28 01:58:53.063927 |Valid Loss -8.856| 
step:1/3334 avg loss:-11.418, visual_loss:-10.893, av_loss:-11.549
step:1/3334 avg loss:-12.324, visual_loss:-12.341, av_loss:-12.320
step:1001/3334 avg loss:-10.794, visual_loss:-10.596, av_loss:-10.844step:1001/3334 avg loss:-10.837, visual_loss:-10.635, av_loss:-10.888

step:2001/3334 avg loss:-10.859, visual_loss:-10.656, av_loss:-10.909step:2001/3334 avg loss:-10.832, visual_loss:-10.640, av_loss:-10.880

step:3001/3334 avg loss:-10.880, visual_loss:-10.679, av_loss:-10.931step:3001/3334 avg loss:-10.856, visual_loss:-10.649, av_loss:-10.907

Train Summary | End of Epoch 42 | Time 3984.66s | Current time 2024-12-28 03:05:18.449172 |Train Loss -10.846| 
step:1/834 avg loss:-10.419, visual_loss:-9.700, av_loss:-10.599
step:1/834 avg loss:-12.040, visual_loss:-11.730, av_loss:-12.118
step:301/834 avg loss:-8.151, visual_loss:-8.088, av_loss:-8.167
step:301/834 avg loss:-8.124, visual_loss:-8.279, av_loss:-8.086
step:601/834 avg loss:-8.110, visual_loss:-8.087, av_loss:-8.115
step:601/834 avg loss:-7.969, visual_loss:-8.139, av_loss:-7.927
Valid Summary | End of Epoch 42 | Time 601.47s | Current time 2024-12-28 03:15:20.088671 |Valid Loss -8.022| 
step:1/3334 avg loss:-11.381, visual_loss:-11.123, av_loss:-11.446
step:1/3334 avg loss:-10.072, visual_loss:-10.060, av_loss:-10.075
step:1001/3334 avg loss:-10.944, visual_loss:-10.699, av_loss:-11.006
step:1001/3334 avg loss:-10.752, visual_loss:-10.514, av_loss:-10.811
step:2001/3334 avg loss:-10.889, visual_loss:-10.658, av_loss:-10.947
step:2001/3334 avg loss:-10.759, visual_loss:-10.538, av_loss:-10.814
step:3001/3334 avg loss:-10.783, visual_loss:-10.563, av_loss:-10.838step:3001/3334 avg loss:-10.674, visual_loss:-10.451, av_loss:-10.730

Train Summary | End of Epoch 43 | Time 3976.95s | Current time 2024-12-28 04:21:37.750879 |Train Loss -10.726| 
step:1/834 avg loss:-10.889, visual_loss:-10.087, av_loss:-11.090
step:1/834 avg loss:-12.869, visual_loss:-12.831, av_loss:-12.878
step:301/834 avg loss:-9.193, visual_loss:-9.337, av_loss:-9.156
step:301/834 avg loss:-9.168, visual_loss:-9.432, av_loss:-9.102
step:601/834 avg loss:-9.145, visual_loss:-9.289, av_loss:-9.109
step:601/834 avg loss:-9.097, visual_loss:-9.386, av_loss:-9.025
Valid Summary | End of Epoch 43 | Time 601.13s | Current time 2024-12-28 04:31:39.030182 |Valid Loss -9.151| 
step:1/3334 avg loss:-11.958, visual_loss:-11.911, av_loss:-11.970
step:1/3334 avg loss:-10.096, visual_loss:-9.873, av_loss:-10.151
step:1001/3334 avg loss:-11.132, visual_loss:-10.943, av_loss:-11.179
step:1001/3334 avg loss:-11.083, visual_loss:-10.925, av_loss:-11.122
step:2001/3334 avg loss:-10.926, visual_loss:-10.723, av_loss:-10.977step:2001/3334 avg loss:-10.845, visual_loss:-10.636, av_loss:-10.898

step:3001/3334 avg loss:-10.974, visual_loss:-10.779, av_loss:-11.023step:3001/3334 avg loss:-10.882, visual_loss:-10.681, av_loss:-10.933

Train Summary | End of Epoch 44 | Time 3948.71s | Current time 2024-12-28 05:37:28.303702 |Train Loss -10.936| 
step:1/834 avg loss:-10.508, visual_loss:-10.210, av_loss:-10.583
step:1/834 avg loss:-12.782, visual_loss:-12.745, av_loss:-12.791
step:301/834 avg loss:-9.281, visual_loss:-9.347, av_loss:-9.265
step:301/834 avg loss:-9.040, visual_loss:-9.201, av_loss:-9.000
step:601/834 avg loss:-9.106, visual_loss:-9.197, av_loss:-9.083
step:601/834 avg loss:-9.042, visual_loss:-9.198, av_loss:-9.003
Valid Summary | End of Epoch 44 | Time 604.07s | Current time 2024-12-28 05:47:32.378735 |Valid Loss -9.081| 
step:1/3334 avg loss:-9.601, visual_loss:-8.829, av_loss:-9.794
step:1/3334 avg loss:-12.487, visual_loss:-12.416, av_loss:-12.505
step:1001/3334 avg loss:-11.168, visual_loss:-10.996, av_loss:-11.211
step:1001/3334 avg loss:-11.107, visual_loss:-10.919, av_loss:-11.155
step:2001/3334 avg loss:-11.121, visual_loss:-10.942, av_loss:-11.166
step:2001/3334 avg loss:-11.072, visual_loss:-10.879, av_loss:-11.120
step:3001/3334 avg loss:-11.157, visual_loss:-10.978, av_loss:-11.202step:3001/3334 avg loss:-11.091, visual_loss:-10.898, av_loss:-11.140

Train Summary | End of Epoch 45 | Time 3962.72s | Current time 2024-12-28 06:53:35.424380 |Train Loss -11.115| 
step:1/834 avg loss:-10.807, visual_loss:-10.377, av_loss:-10.915
step:1/834 avg loss:-13.237, visual_loss:-13.156, av_loss:-13.258
step:301/834 avg loss:-9.669, visual_loss:-9.951, av_loss:-9.599
step:301/834 avg loss:-9.609, visual_loss:-9.921, av_loss:-9.532
step:601/834 avg loss:-9.606, visual_loss:-9.903, av_loss:-9.531
step:601/834 avg loss:-9.402, visual_loss:-9.777, av_loss:-9.308
Valid Summary | End of Epoch 45 | Time 606.56s | Current time 2024-12-28 07:03:42.033058 |Valid Loss -9.488| 
Found new best model, dict saved
step:1/3334 avg loss:-13.246, visual_loss:-13.223, av_loss:-13.252
step:1/3334 avg loss:-11.278, visual_loss:-11.266, av_loss:-11.281
step:1001/3334 avg loss:-11.120, visual_loss:-10.905, av_loss:-11.174step:1001/3334 avg loss:-10.953, visual_loss:-10.761, av_loss:-11.002

step:2001/3334 avg loss:-11.145, visual_loss:-10.955, av_loss:-11.193step:2001/3334 avg loss:-11.042, visual_loss:-10.847, av_loss:-11.090

step:3001/3334 avg loss:-11.120, visual_loss:-10.925, av_loss:-11.169step:3001/3334 avg loss:-11.052, visual_loss:-10.861, av_loss:-11.100

Train Summary | End of Epoch 46 | Time 3977.12s | Current time 2024-12-28 08:10:00.394532 |Train Loss -11.094| 
step:1/834 avg loss:-9.208, visual_loss:-8.420, av_loss:-9.405
step:1/834 avg loss:-13.136, visual_loss:-13.124, av_loss:-13.138
step:301/834 avg loss:-9.952, visual_loss:-10.033, av_loss:-9.931
step:301/834 avg loss:-9.834, visual_loss:-9.980, av_loss:-9.797
step:601/834 avg loss:-9.875, visual_loss:-9.975, av_loss:-9.850
step:601/834 avg loss:-9.797, visual_loss:-10.023, av_loss:-9.741
Valid Summary | End of Epoch 46 | Time 602.14s | Current time 2024-12-28 08:20:02.541392 |Valid Loss -9.829| 
Found new best model, dict saved
step:1/3334 avg loss:-4.822, visual_loss:-4.901, av_loss:-4.802
step:1/3334 avg loss:-11.296, visual_loss:-11.266, av_loss:-11.303
step:1001/3334 avg loss:-11.228, visual_loss:-11.067, av_loss:-11.269
step:1001/3334 avg loss:-11.246, visual_loss:-11.069, av_loss:-11.290
step:2001/3334 avg loss:-11.122, visual_loss:-10.940, av_loss:-11.168
step:2001/3334 avg loss:-11.046, visual_loss:-10.875, av_loss:-11.088
step:3001/3334 avg loss:-11.120, visual_loss:-10.934, av_loss:-11.166step:3001/3334 avg loss:-11.004, visual_loss:-10.840, av_loss:-11.045

Train Summary | End of Epoch 47 | Time 3949.73s | Current time 2024-12-28 09:25:53.751239 |Train Loss -11.020| 
step:1/834 avg loss:-11.031, visual_loss:-10.673, av_loss:-11.120
step:1/834 avg loss:-13.222, visual_loss:-13.056, av_loss:-13.264
step:301/834 avg loss:-9.675, visual_loss:-9.882, av_loss:-9.623
step:301/834 avg loss:-9.646, visual_loss:-9.863, av_loss:-9.591
step:601/834 avg loss:-9.595, visual_loss:-9.814, av_loss:-9.540
step:601/834 avg loss:-9.521, visual_loss:-9.824, av_loss:-9.445
Valid Summary | End of Epoch 47 | Time 604.06s | Current time 2024-12-28 09:35:57.818642 |Valid Loss -9.564| 
step:1/3334 avg loss:-10.710, visual_loss:-11.391, av_loss:-10.540
step:1/3334 avg loss:-12.841, visual_loss:-12.811, av_loss:-12.848
step:1001/3334 avg loss:-11.411, visual_loss:-11.251, av_loss:-11.452step:1001/3334 avg loss:-11.396, visual_loss:-11.256, av_loss:-11.431

step:2001/3334 avg loss:-11.323, visual_loss:-11.153, av_loss:-11.365
step:2001/3334 avg loss:-11.205, visual_loss:-11.050, av_loss:-11.243
step:3001/3334 avg loss:-11.311, visual_loss:-11.139, av_loss:-11.354step:3001/3334 avg loss:-11.193, visual_loss:-11.046, av_loss:-11.230

Train Summary | End of Epoch 48 | Time 3957.62s | Current time 2024-12-28 10:41:55.858662 |Train Loss -11.232| 
step:1/834 avg loss:-11.434, visual_loss:-11.155, av_loss:-11.504
step:1/834 avg loss:-13.434, visual_loss:-13.409, av_loss:-13.440
step:301/834 avg loss:-9.667, visual_loss:-9.870, av_loss:-9.616
step:301/834 avg loss:-9.436, visual_loss:-9.767, av_loss:-9.353
step:601/834 avg loss:-9.462, visual_loss:-9.766, av_loss:-9.385
step:601/834 avg loss:-9.366, visual_loss:-9.702, av_loss:-9.282
Valid Summary | End of Epoch 48 | Time 601.27s | Current time 2024-12-28 10:51:57.131032 |Valid Loss -9.376| 
step:1/3334 avg loss:-12.804, visual_loss:-12.762, av_loss:-12.815
step:1/3334 avg loss:-11.865, visual_loss:-11.771, av_loss:-11.889
step:1001/3334 avg loss:-11.420, visual_loss:-11.276, av_loss:-11.456
step:1001/3334 avg loss:-11.371, visual_loss:-11.216, av_loss:-11.409
step:2001/3334 avg loss:-11.292, visual_loss:-11.132, av_loss:-11.332
step:2001/3334 avg loss:-11.361, visual_loss:-11.202, av_loss:-11.401
step:3001/3334 avg loss:-11.289, visual_loss:-11.124, av_loss:-11.331
step:3001/3334 avg loss:-11.238, visual_loss:-11.071, av_loss:-11.280
Train Summary | End of Epoch 49 | Time 3956.79s | Current time 2024-12-28 11:57:54.237507 |Train Loss -11.273| 
step:1/834 avg loss:-11.593, visual_loss:-11.272, av_loss:-11.673
step:1/834 avg loss:-13.245, visual_loss:-13.192, av_loss:-13.259
step:301/834 avg loss:-9.633, visual_loss:-9.874, av_loss:-9.573
step:301/834 avg loss:-9.673, visual_loss:-10.016, av_loss:-9.587
step:601/834 avg loss:-9.548, visual_loss:-9.892, av_loss:-9.462
step:601/834 avg loss:-9.546, visual_loss:-9.993, av_loss:-9.435
Valid Summary | End of Epoch 49 | Time 606.55s | Current time 2024-12-28 12:08:00.845348 |Valid Loss -9.563| 
step:1/3334 avg loss:-12.949, visual_loss:-12.920, av_loss:-12.957
step:1/3334 avg loss:-11.995, visual_loss:-10.371, av_loss:-12.401
step:1001/3334 avg loss:-11.428, visual_loss:-11.275, av_loss:-11.466step:1001/3334 avg loss:-11.260, visual_loss:-11.088, av_loss:-11.303

step:2001/3334 avg loss:-11.451, visual_loss:-11.294, av_loss:-11.490
step:2001/3334 avg loss:-11.286, visual_loss:-11.128, av_loss:-11.326
step:3001/3334 avg loss:-11.369, visual_loss:-11.208, av_loss:-11.410step:3001/3334 avg loss:-11.214, visual_loss:-11.051, av_loss:-11.254

Train Summary | End of Epoch 50 | Time 3952.08s | Current time 2024-12-28 13:13:53.318529 |Train Loss -11.323| 
step:1/834 avg loss:-11.780, visual_loss:-11.452, av_loss:-11.862
step:1/834 avg loss:-13.276, visual_loss:-13.244, av_loss:-13.284
step:301/834 avg loss:-10.060, visual_loss:-10.191, av_loss:-10.027
step:301/834 avg loss:-9.929, visual_loss:-10.098, av_loss:-9.887
step:601/834 avg loss:-10.058, visual_loss:-10.187, av_loss:-10.025
step:601/834 avg loss:-9.921, visual_loss:-10.154, av_loss:-9.862
Valid Summary | End of Epoch 50 | Time 627.33s | Current time 2024-12-28 13:24:20.773007 |Valid Loss -9.967| 
Found new best model, dict saved
step:1/3334 avg loss:-11.754, visual_loss:-11.721, av_loss:-11.763
step:1/3334 avg loss:-11.647, visual_loss:-11.513, av_loss:-11.681
step:1001/3334 avg loss:-11.548, visual_loss:-11.396, av_loss:-11.586
step:1001/3334 avg loss:-11.429, visual_loss:-11.276, av_loss:-11.467
step:2001/3334 avg loss:-11.428, visual_loss:-11.281, av_loss:-11.465step:2001/3334 avg loss:-11.329, visual_loss:-11.181, av_loss:-11.366

step:3001/3334 avg loss:-11.474, visual_loss:-11.338, av_loss:-11.508step:3001/3334 avg loss:-11.410, visual_loss:-11.279, av_loss:-11.442

Train Summary | End of Epoch 51 | Time 3964.34s | Current time 2024-12-28 14:30:26.198211 |Train Loss -11.440| 
step:1/834 avg loss:-11.617, visual_loss:-11.484, av_loss:-11.650
step:1/834 avg loss:-13.407, visual_loss:-13.287, av_loss:-13.437
step:301/834 avg loss:-9.661, visual_loss:-9.749, av_loss:-9.639
step:301/834 avg loss:-9.584, visual_loss:-9.831, av_loss:-9.522
step:601/834 avg loss:-9.667, visual_loss:-9.753, av_loss:-9.645
step:601/834 avg loss:-9.488, visual_loss:-9.759, av_loss:-9.420
Valid Summary | End of Epoch 51 | Time 604.92s | Current time 2024-12-28 14:40:31.166389 |Valid Loss -9.570| 
step:1/3334 avg loss:-10.251, visual_loss:-9.959, av_loss:-10.324
step:1/3334 avg loss:-9.794, visual_loss:-9.676, av_loss:-9.823
step:1001/3334 avg loss:-11.465, visual_loss:-11.328, av_loss:-11.499
step:1001/3334 avg loss:-11.376, visual_loss:-11.207, av_loss:-11.419
step:2001/3334 avg loss:-11.525, visual_loss:-11.394, av_loss:-11.557
step:2001/3334 avg loss:-11.472, visual_loss:-11.312, av_loss:-11.512
step:3001/3334 avg loss:-11.539, visual_loss:-11.394, av_loss:-11.576step:3001/3334 avg loss:-11.487, visual_loss:-11.336, av_loss:-11.525

Train Summary | End of Epoch 52 | Time 3957.35s | Current time 2024-12-28 15:46:29.203327 |Train Loss -11.506| 
step:1/834 avg loss:-11.909, visual_loss:-11.907, av_loss:-11.909
step:1/834 avg loss:-13.476, visual_loss:-13.427, av_loss:-13.488
step:301/834 avg loss:-10.005, visual_loss:-10.256, av_loss:-9.943
step:301/834 avg loss:-10.153, visual_loss:-10.435, av_loss:-10.082
step:601/834 avg loss:-10.008, visual_loss:-10.300, av_loss:-9.936
step:601/834 avg loss:-10.106, visual_loss:-10.416, av_loss:-10.029
Valid Summary | End of Epoch 52 | Time 617.78s | Current time 2024-12-28 15:56:47.246996 |Valid Loss -10.050| 
Found new best model, dict saved
step:1/3334 avg loss:-9.335, visual_loss:-9.269, av_loss:-9.352
step:1/3334 avg loss:-10.295, visual_loss:-10.663, av_loss:-10.203
step:1001/3334 avg loss:-11.788, visual_loss:-11.693, av_loss:-11.812
step:1001/3334 avg loss:-11.727, visual_loss:-11.595, av_loss:-11.760
started on logs/TDSE_SpkEmbMem/v_teacher

Namespace(mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', batch_size=6, max_length=6, num_workers=6, epochs=100, effec_batch_size=8, accu_grad=0, C=2, model_name='TDSE_SpkEmbMem', lr=0.001, max_norm=5, log_name='logs/TDSE_SpkEmbMem/v_teacher', use_tensorboard=1, continue_from='logs/TDSE_SpkEmbMem/v_teacher', local_rank=0, sample_rate=16000, win=512, hop_length=128, n_mels=80, shift_range='0,1', Self_enroll_amplitude_scaling='0.1,1', teacher_point=50, num_slots='1,5', loss_weight='46', distributed=True, world_size=2, speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800)

Total number of parameters: 25810661 

av_convtasnet(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): TemporalConvNet(
    (layer_norm): ChannelWiseLayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (bottleneck_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (tcn): ModuleList(
      (0-3): 4 x Sequential(
        (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (2): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (3): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (4): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (5): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (6): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (7): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (8): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
      )
    )
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (ve_conv1x1): ModuleList(
      (0-3): 4 x Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
    )
    (mask_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (preEmphasis): PreEmphasis()
  (spk_encoder): MelSpectrogram(
    (spectrogram): Spectrogram()
    (mel_scale): MelScale()
  )
  (spk_model): ECAPA_TDNN(
    (layer1): Conv1dReluBn(
      (conv): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,))
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (layer2): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer3): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer4): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (conv): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))
    (pool): ASTP(
      (linear1): Conv1d(1536, 128, kernel_size=(1,), stride=(1,))
      (linear2): Conv1d(128, 1536, kernel_size=(1,), stride=(1,))
    )
    (bn): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (linear): Linear(in_features=3072, out_features=256, bias=True)
    (bn2): Identity()
  )
  (self_att): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 53
started on logs/TDSE_SpkEmbMem/v_teacher

Namespace(mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', batch_size=6, max_length=6, num_workers=6, epochs=100, effec_batch_size=8, accu_grad=0, C=2, model_name='TDSE_SpkEmbMem', lr=0.001, max_norm=5, log_name='logs/TDSE_SpkEmbMem/v_teacher', use_tensorboard=1, continue_from='logs/TDSE_SpkEmbMem/v_teacher', local_rank=0, sample_rate=16000, win=512, hop_length=128, n_mels=80, shift_range='0,1', Self_enroll_amplitude_scaling='0.1,1', teacher_point=50, num_slots='1,5', loss_weight='46', distributed=True, world_size=2, speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800)

Total number of parameters: 25810661 

av_convtasnet(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): TemporalConvNet(
    (layer_norm): ChannelWiseLayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (bottleneck_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (tcn): ModuleList(
      (0-3): 4 x Sequential(
        (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (2): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (3): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (4): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (5): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (6): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (7): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (8): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
      )
    )
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (ve_conv1x1): ModuleList(
      (0-3): 4 x Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
    )
    (mask_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (preEmphasis): PreEmphasis()
  (spk_encoder): MelSpectrogram(
    (spectrogram): Spectrogram()
    (mel_scale): MelScale()
  )
  (spk_model): ECAPA_TDNN(
    (layer1): Conv1dReluBn(
      (conv): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,))
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (layer2): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer3): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer4): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (conv): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))
    (pool): ASTP(
      (linear1): Conv1d(1536, 128, kernel_size=(1,), stride=(1,))
      (linear2): Conv1d(128, 1536, kernel_size=(1,), stride=(1,))
    )
    (bn): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (linear): Linear(in_features=3072, out_features=256, bias=True)
    (bn2): Identity()
  )
  (self_att): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 53
step:1/1667 avg loss:-11.392, visual_loss:-11.350, av_loss:-11.419
step:1/1667 avg loss:-11.599, visual_loss:-11.184, av_loss:-11.875
step:1001/1667 avg loss:-11.845, visual_loss:-11.785, av_loss:-11.885step:1001/1667 avg loss:-11.889, visual_loss:-11.813, av_loss:-11.940

Train Summary | End of Epoch 53 | Time 2329.65s | Current time 2024-12-28 16:59:56.340535 |Train Loss -11.867| 
step:1/417 avg loss:-11.536, visual_loss:-11.422, av_loss:-11.613
step:1/417 avg loss:-4.986, visual_loss:-4.637, av_loss:-5.219
step:301/417 avg loss:-10.403, visual_loss:-10.633, av_loss:-10.250
step:301/417 avg loss:-10.366, visual_loss:-10.518, av_loss:-10.265
Valid Summary | End of Epoch 53 | Time 382.10s | Current time 2024-12-28 17:06:18.611137 |Valid Loss -10.461| 
Found new best model, dict saved
step:1/1667 avg loss:-11.763, visual_loss:-11.753, av_loss:-11.769
step:1/1667 avg loss:-11.624, visual_loss:-11.595, av_loss:-11.643
step:1001/1667 avg loss:-11.976, visual_loss:-11.917, av_loss:-12.015
step:1001/1667 avg loss:-11.970, visual_loss:-11.916, av_loss:-12.006
Train Summary | End of Epoch 54 | Time 2519.99s | Current time 2024-12-28 17:48:20.188118 |Train Loss -11.862| 
step:1/417 avg loss:-11.510, visual_loss:-11.265, av_loss:-11.673
step:1/417 avg loss:-7.268, visual_loss:-6.751, av_loss:-7.613
step:301/417 avg loss:-10.226, visual_loss:-10.385, av_loss:-10.120
step:301/417 avg loss:-10.233, visual_loss:-10.341, av_loss:-10.160
Valid Summary | End of Epoch 54 | Time 392.71s | Current time 2024-12-28 17:54:52.978161 |Valid Loss -10.268| 
step:1/1667 avg loss:-10.763, visual_loss:-10.562, av_loss:-10.897
step:1/1667 avg loss:-12.021, visual_loss:-12.117, av_loss:-11.956
step:1001/1667 avg loss:-11.938, visual_loss:-11.890, av_loss:-11.971
step:1001/1667 avg loss:-11.953, visual_loss:-11.906, av_loss:-11.984
Train Summary | End of Epoch 55 | Time 2567.00s | Current time 2024-12-28 18:37:41.582295 |Train Loss -11.941| 
step:1/417 avg loss:-11.486, visual_loss:-11.237, av_loss:-11.651
step:1/417 avg loss:-7.729, visual_loss:-9.756, av_loss:-6.378
step:301/417 avg loss:-10.484, visual_loss:-10.667, av_loss:-10.362
step:301/417 avg loss:-10.329, visual_loss:-10.542, av_loss:-10.186
Valid Summary | End of Epoch 55 | Time 392.53s | Current time 2024-12-28 18:44:14.205322 |Valid Loss -10.426| 
step:1/1667 avg loss:-11.521, visual_loss:-11.520, av_loss:-11.521
step:1/1667 avg loss:-13.249, visual_loss:-13.246, av_loss:-13.250
started on logs/TDSE_SpkEmbMem/v_teacher

Namespace(mix_lst_path='../../data_preparation/mixture_data_list_2mix_with_occludded.csv', visual_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mp4/', mixture_direc='/mntcephfs/lee_dataset/separation/voxceleb2/mixture/', obj_dir='../../data_preparation/Asset/object_image_sr', obj_mask_dir='../../data_preparation/Asset/object_mask_x4', batch_size=6, max_length=6, num_workers=6, epochs=100, effec_batch_size=8, accu_grad=0, C=2, model_name='TDSE_SpkEmbMem', lr=0.001, max_norm=5, log_name='logs/TDSE_SpkEmbMem/v_teacher', use_tensorboard=1, continue_from='logs/TDSE_SpkEmbMem/v_teacher', local_rank=0, sample_rate=16000, win=512, hop_length=128, n_mels=80, shift_range='0,1', Self_enroll_amplitude_scaling='0.1,1', teacher_point=50, num_slots='1,5', loss_weight='28', distributed=True, world_size=2, speaker_dict={'id08108': 0, 'id07077': 1, 'id08249': 2, 'id00699': 3, 'id09016': 4, 'id06010': 5, 'id06017': 6, 'id00015': 7, 'id04526': 8, 'id02179': 9, 'id07955': 10, 'id08592': 11, 'id05480': 12, 'id06645': 13, 'id00979': 14, 'id07447': 15, 'id06496': 16, 'id08457': 17, 'id01355': 18, 'id03918': 19, 'id07970': 20, 'id04015': 21, 'id05010': 22, 'id06843': 23, 'id04219': 24, 'id07293': 25, 'id01264': 26, 'id07744': 27, 'id02455': 28, 'id07741': 29, 'id06702': 30, 'id07108': 31, 'id02102': 32, 'id07136': 33, 'id08745': 34, 'id03824': 35, 'id01457': 36, 'id08834': 37, 'id07613': 38, 'id05241': 39, 'id08241': 40, 'id07560': 41, 'id06070': 42, 'id03258': 43, 'id01764': 44, 'id03312': 45, 'id02339': 46, 'id04081': 47, 'id05913': 48, 'id08053': 49, 'id04523': 50, 'id08335': 51, 'id08546': 52, 'id08376': 53, 'id04705': 54, 'id08065': 55, 'id08041': 56, 'id04199': 57, 'id02667': 58, 'id07672': 59, 'id07357': 60, 'id03493': 61, 'id01681': 62, 'id08066': 63, 'id08704': 64, 'id02294': 65, 'id07837': 66, 'id02593': 67, 'id06362': 68, 'id01857': 69, 'id08853': 70, 'id07982': 71, 'id06203': 72, 'id04586': 73, 'id02239': 74, 'id01057': 75, 'id02343': 76, 'id03269': 77, 'id05057': 78, 'id08647': 79, 'id01056': 80, 'id08904': 81, 'id00078': 82, 'id02578': 83, 'id03942': 84, 'id03696': 85, 'id06005': 86, 'id04738': 87, 'id06617': 88, 'id01111': 89, 'id06988': 90, 'id09032': 91, 'id01840': 92, 'id06798': 93, 'id04684': 94, 'id03748': 95, 'id06239': 96, 'id00282': 97, 'id02281': 98, 'id04961': 99, 'id00943': 100, 'id01054': 101, 'id07158': 102, 'id04534': 103, 'id03502': 104, 'id02987': 105, 'id00568': 106, 'id04615': 107, 'id01648': 108, 'id06597': 109, 'id07670': 110, 'id05066': 111, 'id03445': 112, 'id08739': 113, 'id01694': 114, 'id09232': 115, 'id05710': 116, 'id07719': 117, 'id08377': 118, 'id01916': 119, 'id01570': 120, 'id05978': 121, 'id08714': 122, 'id02318': 123, 'id03994': 124, 'id06948': 125, 'id07165': 126, 'id06663': 127, 'id07164': 128, 'id08042': 129, 'id04518': 130, 'id05875': 131, 'id08569': 132, 'id05856': 133, 'id08317': 134, 'id08243': 135, 'id01286': 136, 'id06038': 137, 'id00262': 138, 'id01504': 139, 'id05935': 140, 'id01677': 141, 'id07276': 142, 'id01658': 143, 'id04221': 144, 'id05293': 145, 'id06771': 146, 'id06586': 147, 'id06323': 148, 'id03992': 149, 'id03713': 150, 'id04968': 151, 'id00076': 152, 'id06364': 153, 'id03240': 154, 'id04134': 155, 'id00187': 156, 'id08348': 157, 'id08568': 158, 'id08917': 159, 'id04257': 160, 'id02733': 161, 'id05573': 162, 'id00996': 163, 'id02438': 164, 'id05777': 165, 'id04715': 166, 'id03939': 167, 'id02214': 168, 'id04320': 169, 'id08064': 170, 'id07272': 171, 'id01225': 172, 'id06588': 173, 'id00272': 174, 'id06445': 175, 'id05907': 176, 'id01040': 177, 'id02297': 178, 'id08343': 179, 'id03199': 180, 'id00350': 181, 'id08122': 182, 'id01163': 183, 'id03522': 184, 'id08080': 185, 'id07708': 186, 'id07844': 187, 'id04988': 188, 'id03923': 189, 'id04086': 190, 'id02299': 191, 'id07800': 192, 'id02457': 193, 'id05496': 194, 'id08837': 195, 'id09033': 196, 'id07161': 197, 'id02260': 198, 'id06584': 199, 'id07349': 200, 'id06351': 201, 'id05288': 202, 'id03215': 203, 'id03334': 204, 'id01758': 205, 'id00978': 206, 'id05851': 207, 'id07351': 208, 'id08362': 209, 'id08518': 210, 'id08928': 211, 'id00062': 212, 'id08658': 213, 'id01193': 214, 'id05453': 215, 'id09051': 216, 'id00708': 217, 'id03267': 218, 'id02477': 219, 'id06507': 220, 'id01262': 221, 'id02249': 222, 'id01750': 223, 'id00741': 224, 'id07956': 225, 'id02361': 226, 'id08933': 227, 'id05832': 228, 'id06866': 229, 'id07181': 230, 'id00876': 231, 'id04537': 232, 'id00620': 233, 'id05591': 234, 'id09147': 235, 'id03430': 236, 'id00086': 237, 'id07010': 238, 'id06696': 239, 'id02116': 240, 'id09192': 241, 'id05060': 242, 'id01387': 243, 'id04538': 244, 'id03201': 245, 'id09171': 246, 'id04752': 247, 'id06360': 248, 'id08781': 249, 'id04064': 250, 'id06289': 251, 'id05664': 252, 'id02008': 253, 'id05186': 254, 'id01265': 255, 'id05998': 256, 'id06725': 257, 'id04855': 258, 'id00967': 259, 'id00513': 260, 'id00816': 261, 'id03727': 262, 'id06958': 263, 'id03755': 264, 'id04780': 265, 'id01010': 266, 'id01904': 267, 'id00942': 268, 'id02067': 269, 'id01411': 270, 'id03896': 271, 'id07623': 272, 'id05506': 273, 'id06415': 274, 'id07434': 275, 'id07856': 276, 'id05693': 277, 'id01806': 278, 'id02156': 279, 'id01713': 280, 'id05454': 281, 'id06212': 282, 'id02534': 283, 'id00806': 284, 'id01678': 285, 'id07578': 286, 'id03014': 287, 'id07145': 288, 'id06250': 289, 'id08653': 290, 'id04372': 291, 'id07236': 292, 'id06582': 293, 'id07750': 294, 'id00060': 295, 'id02015': 296, 'id05257': 297, 'id00692': 298, 'id03371': 299, 'id08289': 300, 'id01160': 301, 'id07809': 302, 'id04618': 303, 'id01906': 304, 'id08694': 305, 'id00888': 306, 'id07618': 307, 'id06986': 308, 'id05171': 309, 'id06717': 310, 'id03414': 311, 'id01746': 312, 'id00664': 313, 'id09010': 314, 'id03885': 315, 'id08993': 316, 'id08501': 317, 'id06131': 318, 'id07776': 319, 'id05165': 320, 'id08047': 321, 'id00709': 322, 'id08738': 323, 'id04358': 324, 'id05265': 325, 'id06411': 326, 'id05699': 327, 'id00891': 328, 'id07686': 329, 'id00990': 330, 'id00673': 331, 'id02495': 332, 'id04125': 333, 'id00084': 334, 'id02943': 335, 'id02980': 336, 'id07703': 337, 'id04349': 338, 'id02888': 339, 'id00159': 340, 'id08079': 341, 'id01218': 342, 'id00787': 343, 'id07178': 344, 'id08239': 345, 'id04409': 346, 'id00378': 347, 'id02873': 348, 'id05260': 349, 'id00417': 350, 'id05666': 351, 'id04284': 352, 'id09225': 353, 'id05691': 354, 'id07834': 355, 'id02075': 356, 'id05049': 357, 'id03983': 358, 'id02392': 359, 'id00258': 360, 'id04419': 361, 'id07642': 362, 'id01006': 363, 'id00385': 364, 'id03728': 365, 'id06479': 366, 'id05153': 367, 'id03028': 368, 'id03147': 369, 'id03328': 370, 'id04194': 371, 'id01859': 372, 'id02341': 373, 'id04508': 374, 'id01354': 375, 'id03118': 376, 'id06946': 377, 'id01556': 378, 'id00043': 379, 'id07535': 380, 'id03429': 381, 'id07128': 382, 'id04247': 383, 'id06114': 384, 'id03227': 385, 'id01614': 386, 'id06424': 387, 'id09269': 388, 'id02157': 389, 'id01768': 390, 'id09088': 391, 'id04742': 392, 'id04903': 393, 'id07531': 394, 'id04757': 395, 'id00680': 396, 'id03313': 397, 'id02721': 398, 'id06109': 399, 'id05738': 400, 'id05632': 401, 'id04617': 402, 'id08604': 403, 'id08573': 404, 'id06477': 405, 'id08236': 406, 'id05272': 407, 'id02760': 408, 'id09052': 409, 'id07360': 410, 'id06551': 411, 'id06414': 412, 'id08742': 413, 'id06252': 414, 'id01776': 415, 'id04842': 416, 'id03810': 417, 'id08968': 418, 'id01060': 419, 'id06196': 420, 'id04117': 421, 'id04184': 422, 'id03460': 423, 'id00191': 424, 'id01448': 425, 'id02311': 426, 'id04198': 427, 'id04458': 428, 'id05986': 429, 'id07078': 430, 'id06792': 431, 'id02817': 432, 'id04417': 433, 'id04997': 434, 'id00149': 435, 'id00462': 436, 'id01209': 437, 'id04974': 438, 'id03786': 439, 'id04644': 440, 'id02053': 441, 'id03556': 442, 'id07851': 443, 'id00944': 444, 'id03705': 445, 'id03288': 446, 'id06183': 447, 'id06432': 448, 'id04564': 449, 'id07887': 450, 'id05494': 451, 'id02653': 452, 'id09091': 453, 'id00529': 454, 'id06121': 455, 'id04229': 456, 'id06248': 457, 'id05930': 458, 'id08885': 459, 'id08953': 460, 'id05189': 461, 'id01798': 462, 'id07400': 463, 'id04655': 464, 'id03750': 465, 'id02039': 466, 'id07415': 467, 'id07974': 468, 'id05133': 469, 'id08860': 470, 'id04108': 471, 'id08334': 472, 'id04188': 473, 'id05741': 474, 'id04017': 475, 'id01659': 476, 'id06870': 477, 'id04179': 478, 'id03675': 479, 'id03733': 480, 'id01206': 481, 'id04274': 482, 'id02166': 483, 'id04160': 484, 'id03532': 485, 'id00202': 486, 'id06336': 487, 'id04486': 488, 'id03754': 489, 'id05054': 490, 'id07421': 491, 'id03247': 492, 'id08146': 493, 'id02289': 494, 'id05027': 495, 'id00168': 496, 'id00184': 497, 'id08529': 498, 'id03917': 499, 'id06944': 500, 'id01960': 501, 'id00774': 502, 'id00903': 503, 'id05554': 504, 'id00332': 505, 'id04814': 506, 'id05261': 507, 'id01100': 508, 'id01523': 509, 'id06222': 510, 'id02463': 511, 'id07556': 512, 'id00773': 513, 'id08660': 514, 'id03088': 515, 'id01317': 516, 'id04667': 517, 'id08999': 518, 'id02948': 519, 'id00788': 520, 'id05585': 521, 'id05926': 522, 'id08802': 523, 'id07624': 524, 'id07179': 525, 'id04333': 526, 'id04930': 527, 'id01539': 528, 'id08981': 529, 'id02351': 530, 'id04554': 531, 'id06062': 532, 'id08936': 533, 'id07489': 534, 'id07625': 535, 'id04404': 536, 'id05289': 537, 'id03866': 538, 'id03325': 539, 'id04195': 540, 'id00029': 541, 'id00823': 542, 'id04254': 543, 'id02017': 544, 'id04588': 545, 'id04677': 546, 'id04423': 547, 'id06952': 548, 'id02389': 549, 'id00413': 550, 'id00176': 551, 'id01101': 552, 'id00352': 553, 'id07299': 554, 'id04223': 555, 'id00976': 556, 'id03821': 557, 'id08462': 558, 'id02735': 559, 'id02739': 560, 'id07727': 561, 'id08160': 562, 'id05696': 563, 'id04488': 564, 'id00592': 565, 'id02598': 566, 'id00650': 567, 'id01754': 568, 'id00980': 569, 'id04559': 570, 'id01690': 571, 'id03351': 572, 'id04511': 573, 'id05771': 574, 'id06650': 575, 'id03768': 576, 'id02981': 577, 'id07405': 578, 'id03652': 579, 'id07186': 580, 'id03706': 581, 'id04146': 582, 'id05622': 583, 'id05401': 584, 'id07332': 585, 'id01617': 586, 'id03331': 587, 'id00266': 588, 'id02573': 589, 'id08147': 590, 'id00524': 591, 'id03274': 592, 'id07497': 593, 'id09210': 594, 'id05737': 595, 'id00494': 596, 'id03871': 597, 'id00020': 598, 'id04628': 599, 'id08919': 600, 'id08097': 601, 'id09049': 602, 'id05541': 603, 'id05286': 604, 'id06937': 605, 'id06347': 606, 'id06874': 607, 'id04784': 608, 'id06949': 609, 'id02062': 610, 'id03264': 611, 'id08049': 612, 'id04408': 613, 'id06900': 614, 'id08261': 615, 'id06140': 616, 'id05477': 617, 'id07060': 618, 'id07726': 619, 'id00353': 620, 'id05160': 621, 'id07418': 622, 'id00064': 623, 'id02724': 624, 'id04265': 625, 'id04097': 626, 'id08599': 627, 'id04133': 628, 'id00902': 629, 'id06262': 630, 'id09230': 631, 'id00469': 632, 'id08536': 633, 'id00395': 634, 'id05853': 635, 'id01931': 636, 'id02004': 637, 'id01280': 638, 'id07429': 639, 'id05916': 640, 'id02889': 641, 'id04163': 642, 'id05492': 643, 'id04654': 644, 'id04839': 645, 'id00965': 646, 'id01489': 647, 'id00558': 648, 'id08804': 649, 'id06529': 650, 'id02306': 651, 'id08311': 652, 'id04712': 653, 'id03525': 654, 'id01511': 655, 'id03336': 656, 'id00459': 657, 'id01908': 658, 'id06847': 659, 'id08983': 660, 'id03989': 661, 'id08556': 662, 'id06875': 663, 'id00185': 664, 'id02021': 665, 'id08240': 666, 'id06723': 667, 'id04869': 668, 'id01501': 669, 'id01235': 670, 'id02340': 671, 'id04582': 672, 'id06676': 673, 'id05841': 674, 'id08702': 675, 'id04517': 676, 'id02346': 677, 'id04660': 678, 'id03947': 679, 'id00724': 680, 'id00829': 681, 'id07256': 682, 'id03796': 683, 'id06541': 684, 'id07080': 685, 'id07500': 686, 'id06378': 687, 'id08323': 688, 'id05757': 689, 'id05723': 690, 'id05576': 691, 'id08278': 692, 'id04873': 693, 'id06226': 694, 'id02891': 695, 'id04856': 696, 'id05739': 697, 'id03804': 698, 'id05893': 699, 'id08002': 700, 'id01769': 701, 'id04334': 702, 'id00192': 703, 'id07043': 704, 'id08533': 705, 'id03298': 706, 'id05209': 707, 'id01689': 708, 'id07130': 709, 'id08937': 710, 'id02538': 711, 'id04031': 712, 'id01239': 713, 'id05434': 714, 'id02112': 715, 'id04782': 716, 'id06552': 717, 'id04509': 718, 'id08414': 719, 'id07344': 720, 'id05644': 721, 'id09186': 722, 'id02087': 723, 'id06355': 724, 'id04907': 725, 'id05685': 726, 'id02267': 727, 'id06138': 728, 'id04216': 729, 'id04529': 730, 'id06368': 731, 'id07046': 732, 'id04530': 733, 'id02032': 734, 'id01121': 735, 'id01927': 736, 'id05256': 737, 'id03757': 738, 'id05104': 739, 'id08600': 740, 'id05039': 741, 'id05983': 742, 'id08351': 743, 'id06016': 744, 'id04427': 745, 'id07534': 746, 'id08756': 747, 'id02979': 748, 'id03945': 749, 'id04128': 750, 'id04222': 751, 'id06673': 752, 'id07050': 753, 'id00255': 754, 'id05471': 755, 'id05141': 756, 'id04027': 757, 'id08525': 758, 'id04266': 759, 'id05989': 760, 'id01827': 761, 'id02270': 762, 'id03242': 763, 'id01223': 764, 'id09048': 765, 'id03207': 766, 'id01098': 767, 'id04224': 768, 'id07175': 769, 'id03877': 770, 'id00755': 771, 'id04151': 772, 'id00022': 773, 'id08022': 774, 'id02384': 775, 'id06474': 776, 'id04716': 777, 'id04560': 778, 'id02040': 779, 'id02437': 780, 'id00911': 781, 'id03931': 782, 'id03527': 783, 'id01315': 784, 'id03820': 785, 'id03352': 786, 'id02150': 787, 'id00752': 788, 'id07980': 789, 'id07341': 790, 'id01639': 791, 'id03304': 792, 'id04589': 793, 'id02252': 794, 'id08477': 795, 'id07954': 796, 'id03907': 797, 'id04300': 798, 'id06905': 799}, speakers=800)

Total number of parameters: 25810661 

av_convtasnet(
  (encoder): Encoder(
    (conv1d_U): Conv1d(1, 256, kernel_size=(40,), stride=(20,), bias=False)
  )
  (separator): TemporalConvNet(
    (layer_norm): ChannelWiseLayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (bottleneck_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (tcn): ModuleList(
      (0-3): 4 x Sequential(
        (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (2): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (3): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (4): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (5): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (6): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (7): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
        (8): TemporalBlock(
          (net): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)
            (1): PReLU(num_parameters=1)
            (2): GlobalLayerNorm()
            (3): DepthwiseSeparableConv(
              (net): Sequential(
                (0): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512, bias=False)
                (1): PReLU(num_parameters=1)
                (2): GlobalLayerNorm()
                (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
              )
            )
          )
        )
      )
    )
    (visual_conv): Sequential(
      (0): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (1): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (2): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (3): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
      (4): VisualConv1D(
        (net): Sequential(
          (0): ReLU()
          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)
          (3): PReLU(num_parameters=1)
          (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
    (ve_conv1x1): ModuleList(
      (0-3): 4 x Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
    )
    (mask_conv1x1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
    (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  )
  (decoder): Decoder(
    (basis_signals): Linear(in_features=256, out_features=40, bias=False)
  )
  (visual_frontend): VisualFrontend(
    (frontend3D): Sequential(
      (0): Conv3d(1, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
      (1): BatchNorm3d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)
    )
    (resnet): ResNet(
      (layer1): ResNetLayer(
        (conv1a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (outbna): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer2): ResNetLayer(
        (conv1a): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer3): ResNetLayer(
        (conv1a): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (layer4): ResNetLayer(
        (conv1a): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1a): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2a): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (outbna): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv1b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1b): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
        (conv2b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (outbnb): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)
      )
      (avgpool): AvgPool2d(kernel_size=(4, 4), stride=(1, 1), padding=0)
    )
  )
  (linear_fusion): Conv1d(512, 256, kernel_size=(1,), stride=(1,), bias=False)
  (preEmphasis): PreEmphasis()
  (spk_encoder): MelSpectrogram(
    (spectrogram): Spectrogram()
    (mel_scale): MelScale()
  )
  (spk_model): ECAPA_TDNN(
    (layer1): Conv1dReluBn(
      (conv): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,))
      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (layer2): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer3): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (layer4): SE_Res2Block(
      (se_res2block): Sequential(
        (0): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): Res2Conv1dReluBn(
          (convs): ModuleList(
            (0-6): 7 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,))
          )
          (bns): ModuleList(
            (0-6): 7 x BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): Conv1dReluBn(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
          (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (3): SE_Connect(
          (linear1): Linear(in_features=256, out_features=128, bias=True)
          (linear2): Linear(in_features=128, out_features=256, bias=True)
        )
      )
    )
    (conv): Conv1d(768, 1536, kernel_size=(1,), stride=(1,))
    (pool): ASTP(
      (linear1): Conv1d(1536, 128, kernel_size=(1,), stride=(1,))
      (linear2): Conv1d(128, 1536, kernel_size=(1,), stride=(1,))
    )
    (bn): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (linear): Linear(in_features=3072, out_features=256, bias=True)
    (bn2): Identity()
  )
  (self_att): CrossAttentionAVG(
    (query_proj): Linear(in_features=256, out_features=256, bias=False)
    (key_proj): Linear(in_features=256, out_features=256, bias=False)
    (value_proj): Linear(in_features=256, out_features=256, bias=False)
  )
)
Resume training from epoch: 56
step:1/1667 avg loss:-13.276, visual_loss:-13.272, av_loss:-13.277
step:1/1667 avg loss:-11.537, visual_loss:-11.500, av_loss:-11.546
step:1001/1667 avg loss:-12.098, visual_loss:-12.035, av_loss:-12.114step:1001/1667 avg loss:-12.123, visual_loss:-12.062, av_loss:-12.138

Train Summary | End of Epoch 56 | Time 2230.51s | Current time 2024-12-28 19:45:42.847974 |Train Loss -12.046| 
step:1/417 avg loss:-11.882, visual_loss:-11.609, av_loss:-11.951
step:1/417 avg loss:-5.795, visual_loss:-5.398, av_loss:-5.894
step:301/417 avg loss:-10.613, visual_loss:-10.867, av_loss:-10.549
step:301/417 avg loss:-10.477, visual_loss:-10.753, av_loss:-10.408
Valid Summary | End of Epoch 56 | Time 373.48s | Current time 2024-12-28 19:51:56.551059 |Valid Loss -10.607| 
Found new best model, dict saved
step:1/1667 avg loss:-11.961, visual_loss:-11.792, av_loss:-12.003
step:1/1667 avg loss:-11.576, visual_loss:-11.564, av_loss:-11.579
step:1001/1667 avg loss:-12.079, visual_loss:-12.016, av_loss:-12.095
step:1001/1667 avg loss:-12.072, visual_loss:-12.016, av_loss:-12.085
Train Summary | End of Epoch 57 | Time 2497.34s | Current time 2024-12-28 20:33:35.596434 |Train Loss -12.072| 
step:1/417 avg loss:-11.831, visual_loss:-11.653, av_loss:-11.876
step:1/417 avg loss:-9.109, visual_loss:-6.287, av_loss:-9.814
step:301/417 avg loss:-10.659, visual_loss:-10.914, av_loss:-10.595
step:301/417 avg loss:-10.502, visual_loss:-10.699, av_loss:-10.452
Valid Summary | End of Epoch 57 | Time 387.09s | Current time 2024-12-28 20:40:02.801435 |Valid Loss -10.626| 
Found new best model, dict saved
step:1/1667 avg loss:-12.824, visual_loss:-12.803, av_loss:-12.830
step:1/1667 avg loss:-11.907, visual_loss:-11.858, av_loss:-11.919
step:1001/1667 avg loss:-12.228, visual_loss:-12.171, av_loss:-12.243step:1001/1667 avg loss:-12.297, visual_loss:-12.228, av_loss:-12.314

Train Summary | End of Epoch 58 | Time 2586.25s | Current time 2024-12-28 21:23:14.777389 |Train Loss -12.152| 
step:1/417 avg loss:-11.225, visual_loss:-11.141, av_loss:-11.246
step:1/417 avg loss:-4.952, visual_loss:-4.813, av_loss:-4.987
step:301/417 avg loss:-10.130, visual_loss:-10.358, av_loss:-10.073
step:301/417 avg loss:-9.978, visual_loss:-10.219, av_loss:-9.918
Valid Summary | End of Epoch 58 | Time 387.12s | Current time 2024-12-28 21:29:41.924287 |Valid Loss -10.126| 
step:1/1667 avg loss:-13.861, visual_loss:-13.855, av_loss:-13.863
step:1/1667 avg loss:-11.488, visual_loss:-11.405, av_loss:-11.508
step:1001/1667 avg loss:-12.304, visual_loss:-12.255, av_loss:-12.317
step:1001/1667 avg loss:-12.286, visual_loss:-12.221, av_loss:-12.302
Train Summary | End of Epoch 59 | Time 2534.17s | Current time 2024-12-28 22:11:57.012588 |Train Loss -12.259| 
step:1/417 avg loss:-11.647, visual_loss:-11.384, av_loss:-11.713
step:1/417 avg loss:-4.897, visual_loss:-5.075, av_loss:-4.853
step:301/417 avg loss:-10.373, visual_loss:-10.648, av_loss:-10.304
step:301/417 avg loss:-10.198, visual_loss:-10.445, av_loss:-10.136
Valid Summary | End of Epoch 59 | Time 389.68s | Current time 2024-12-28 22:18:26.894294 |Valid Loss -10.352| 
step:1/1667 avg loss:-12.965, visual_loss:-12.966, av_loss:-12.965
step:1/1667 avg loss:-12.541, visual_loss:-12.454, av_loss:-12.563
step:1001/1667 avg loss:-12.442, visual_loss:-12.399, av_loss:-12.453
step:1001/1667 avg loss:-12.445, visual_loss:-12.390, av_loss:-12.459
Train Summary | End of Epoch 60 | Time 2566.79s | Current time 2024-12-28 23:01:14.998423 |Train Loss -12.311| 
step:1/417 avg loss:-11.839, visual_loss:-11.727, av_loss:-11.867
step:1/417 avg loss:-4.872, visual_loss:-4.696, av_loss:-4.916
step:301/417 avg loss:-10.515, visual_loss:-10.735, av_loss:-10.461
step:301/417 avg loss:-10.380, visual_loss:-10.603, av_loss:-10.324
Valid Summary | End of Epoch 60 | Time 426.38s | Current time 2024-12-28 23:08:21.397081 |Valid Loss -10.499| 
step:1/1667 avg loss:-12.826, visual_loss:-12.815, av_loss:-12.829
step:1/1667 avg loss:-13.246, visual_loss:-13.099, av_loss:-13.283
step:1001/1667 avg loss:-12.214, visual_loss:-12.154, av_loss:-12.229
step:1001/1667 avg loss:-12.213, visual_loss:-12.145, av_loss:-12.230
Train Summary | End of Epoch 61 | Time 2536.32s | Current time 2024-12-28 23:50:41.309866 |Train Loss -12.222| 
step:1/417 avg loss:-11.746, visual_loss:-11.549, av_loss:-11.795
step:1/417 avg loss:-5.187, visual_loss:-4.866, av_loss:-5.268
step:301/417 avg loss:-10.635, visual_loss:-10.901, av_loss:-10.569
step:301/417 avg loss:-10.523, visual_loss:-10.676, av_loss:-10.485
Valid Summary | End of Epoch 61 | Time 390.37s | Current time 2024-12-28 23:57:11.707563 |Valid Loss -10.661| 
Found new best model, dict saved
step:1/1667 avg loss:-13.243, visual_loss:-13.236, av_loss:-13.244
step:1/1667 avg loss:-13.516, visual_loss:-13.524, av_loss:-13.513
step:1001/1667 avg loss:-12.455, visual_loss:-12.394, av_loss:-12.471step:1001/1667 avg loss:-12.413, visual_loss:-12.340, av_loss:-12.431

Train Summary | End of Epoch 62 | Time 2535.02s | Current time 2024-12-29 00:39:29.860529 |Train Loss -12.451| 
step:1/417 avg loss:-11.720, visual_loss:-11.528, av_loss:-11.768
step:1/417 avg loss:-9.668, visual_loss:-6.997, av_loss:-10.336
step:301/417 avg loss:-10.311, visual_loss:-10.706, av_loss:-10.212
step:301/417 avg loss:-10.263, visual_loss:-10.558, av_loss:-10.189
Valid Summary | End of Epoch 62 | Time 370.93s | Current time 2024-12-29 00:45:40.899505 |Valid Loss -10.316| 
step:1/1667 avg loss:-12.930, visual_loss:-12.829, av_loss:-12.955
step:1/1667 avg loss:-11.917, visual_loss:-11.928, av_loss:-11.914
step:1001/1667 avg loss:-12.346, visual_loss:-12.279, av_loss:-12.363step:1001/1667 avg loss:-12.302, visual_loss:-12.222, av_loss:-12.322

Train Summary | End of Epoch 63 | Time 2522.08s | Current time 2024-12-29 01:27:44.613391 |Train Loss -12.343| 
step:1/417 avg loss:-11.896, visual_loss:-11.691, av_loss:-11.947
step:1/417 avg loss:-10.888, visual_loss:-9.859, av_loss:-11.146
step:301/417 avg loss:-10.687, visual_loss:-10.991, av_loss:-10.611
step:301/417 avg loss:-10.615, visual_loss:-10.852, av_loss:-10.556
Valid Summary | End of Epoch 63 | Time 391.60s | Current time 2024-12-29 01:34:16.868927 |Valid Loss -10.715| 
Found new best model, dict saved
step:1/1667 avg loss:-13.428, visual_loss:-13.429, av_loss:-13.428
step:1/1667 avg loss:-13.486, visual_loss:-13.421, av_loss:-13.502
step:1001/1667 avg loss:-12.654, visual_loss:-12.601, av_loss:-12.667step:1001/1667 avg loss:-12.673, visual_loss:-12.627, av_loss:-12.685

Train Summary | End of Epoch 64 | Time 2590.62s | Current time 2024-12-29 02:17:31.852729 |Train Loss -12.513| 
step:1/417 avg loss:-11.762, visual_loss:-11.520, av_loss:-11.823
step:1/417 avg loss:-9.841, visual_loss:-8.419, av_loss:-10.197
step:301/417 avg loss:-10.268, visual_loss:-10.735, av_loss:-10.151
step:301/417 avg loss:-9.996, visual_loss:-10.466, av_loss:-9.878
Valid Summary | End of Epoch 64 | Time 378.91s | Current time 2024-12-29 02:23:50.807706 |Valid Loss -10.231| 
step:1/1667 avg loss:-10.510, visual_loss:-10.499, av_loss:-10.513
step:1/1667 avg loss:-9.142, visual_loss:-9.489, av_loss:-9.055
step:1001/1667 avg loss:-12.507, visual_loss:-12.453, av_loss:-12.520
step:1001/1667 avg loss:-12.424, visual_loss:-12.356, av_loss:-12.441
Train Summary | End of Epoch 65 | Time 2574.23s | Current time 2024-12-29 03:06:46.692692 |Train Loss -12.533| 
step:1/417 avg loss:-11.770, visual_loss:-11.690, av_loss:-11.790
step:1/417 avg loss:-9.053, visual_loss:-9.197, av_loss:-9.017
step:301/417 avg loss:-10.715, visual_loss:-11.081, av_loss:-10.624
step:301/417 avg loss:-10.589, visual_loss:-10.948, av_loss:-10.499
Valid Summary | End of Epoch 65 | Time 392.85s | Current time 2024-12-29 03:13:19.585126 |Valid Loss -10.720| 
Found new best model, dict saved
step:1/1667 avg loss:-13.348, visual_loss:-13.314, av_loss:-13.357
step:1/1667 avg loss:-12.683, visual_loss:-12.660, av_loss:-12.689
step:1001/1667 avg loss:-12.458, visual_loss:-12.389, av_loss:-12.475step:1001/1667 avg loss:-12.474, visual_loss:-12.410, av_loss:-12.490

Train Summary | End of Epoch 66 | Time 2624.80s | Current time 2024-12-29 03:57:07.453287 |Train Loss -12.417| 
step:1/417 avg loss:-11.601, visual_loss:-11.593, av_loss:-11.603
step:1/417 avg loss:-11.016, visual_loss:-11.118, av_loss:-10.990
step:301/417 avg loss:-10.652, visual_loss:-10.983, av_loss:-10.570
step:301/417 avg loss:-10.528, visual_loss:-10.859, av_loss:-10.445
Valid Summary | End of Epoch 66 | Time 440.41s | Current time 2024-12-29 04:04:28.085227 |Valid Loss -10.654| 
step:1/1667 avg loss:-12.183, visual_loss:-12.181, av_loss:-12.184
step:1/1667 avg loss:-11.086, visual_loss:-11.034, av_loss:-11.099
step:1001/1667 avg loss:-12.710, visual_loss:-12.663, av_loss:-12.722step:1001/1667 avg loss:-12.662, visual_loss:-12.620, av_loss:-12.673

Train Summary | End of Epoch 67 | Time 2578.28s | Current time 2024-12-29 04:47:27.737605 |Train Loss -12.513| 
step:1/417 avg loss:-11.212, visual_loss:-11.213, av_loss:-11.212
step:1/417 avg loss:-6.513, visual_loss:-8.497, av_loss:-6.017
step:301/417 avg loss:-10.626, visual_loss:-10.880, av_loss:-10.562
step:301/417 avg loss:-10.479, visual_loss:-10.760, av_loss:-10.409
Valid Summary | End of Epoch 67 | Time 397.07s | Current time 2024-12-29 04:54:04.916232 |Valid Loss -10.586| 
step:1/1667 avg loss:-11.808, visual_loss:-11.793, av_loss:-11.812
step:1/1667 avg loss:-12.815, visual_loss:-12.855, av_loss:-12.805
step:1001/1667 avg loss:-12.753, visual_loss:-12.709, av_loss:-12.764
step:1001/1667 avg loss:-12.708, visual_loss:-12.661, av_loss:-12.720
Train Summary | End of Epoch 68 | Time 2622.01s | Current time 2024-12-29 05:37:48.652167 |Train Loss -12.692| 
step:1/417 avg loss:-11.868, visual_loss:-11.852, av_loss:-11.872
step:1/417 avg loss:-9.842, visual_loss:-9.114, av_loss:-10.024
step:301/417 avg loss:-10.872, visual_loss:-11.172, av_loss:-10.797
step:301/417 avg loss:-10.715, visual_loss:-11.088, av_loss:-10.622
Valid Summary | End of Epoch 68 | Time 410.69s | Current time 2024-12-29 05:44:39.690498 |Valid Loss -10.851| 
Found new best model, dict saved
step:1/1667 avg loss:-11.982, visual_loss:-12.051, av_loss:-11.965
step:1/1667 avg loss:-12.607, visual_loss:-12.512, av_loss:-12.631
step:1001/1667 avg loss:-12.703, visual_loss:-12.652, av_loss:-12.716step:1001/1667 avg loss:-12.666, visual_loss:-12.615, av_loss:-12.679

Train Summary | End of Epoch 69 | Time 2519.10s | Current time 2024-12-29 06:26:41.035801 |Train Loss -12.623| 
step:1/417 avg loss:-11.866, visual_loss:-11.726, av_loss:-11.901
step:1/417 avg loss:-10.726, visual_loss:-10.623, av_loss:-10.751
step:301/417 avg loss:-10.083, visual_loss:-10.360, av_loss:-10.013
step:301/417 avg loss:-9.966, visual_loss:-10.229, av_loss:-9.900
Valid Summary | End of Epoch 69 | Time 386.02s | Current time 2024-12-29 06:33:07.112975 |Valid Loss -10.093| 
step:1/1667 avg loss:-11.873, visual_loss:-11.852, av_loss:-11.878
step:1/1667 avg loss:-12.974, visual_loss:-12.951, av_loss:-12.980
step:1001/1667 avg loss:-12.735, visual_loss:-12.687, av_loss:-12.747
step:1001/1667 avg loss:-12.689, visual_loss:-12.627, av_loss:-12.704
Train Summary | End of Epoch 70 | Time 2591.61s | Current time 2024-12-29 07:16:21.502353 |Train Loss -12.558| 
step:1/417 avg loss:-11.668, visual_loss:-11.349, av_loss:-11.747
step:1/417 avg loss:-10.143, visual_loss:-10.947, av_loss:-9.942
step:301/417 avg loss:-10.575, visual_loss:-10.829, av_loss:-10.512
step:301/417 avg loss:-10.445, visual_loss:-10.759, av_loss:-10.366
Valid Summary | End of Epoch 70 | Time 411.54s | Current time 2024-12-29 07:23:13.195581 |Valid Loss -10.575| 
step:1/1667 avg loss:-13.060, visual_loss:-13.052, av_loss:-13.062
step:1/1667 avg loss:-14.228, visual_loss:-14.202, av_loss:-14.235
step:1001/1667 avg loss:-12.570, visual_loss:-12.511, av_loss:-12.585step:1001/1667 avg loss:-12.548, visual_loss:-12.486, av_loss:-12.564

Train Summary | End of Epoch 71 | Time 2547.99s | Current time 2024-12-29 08:05:43.387164 |Train Loss -12.640| 
step:1/417 avg loss:-11.982, visual_loss:-11.928, av_loss:-11.995
step:1/417 avg loss:-6.036, visual_loss:-8.203, av_loss:-5.495
step:301/417 avg loss:-11.050, visual_loss:-11.297, av_loss:-10.988
step:301/417 avg loss:-10.904, visual_loss:-11.163, av_loss:-10.839
Valid Summary | End of Epoch 71 | Time 378.70s | Current time 2024-12-29 08:12:02.373518 |Valid Loss -11.016| 
Found new best model, dict saved
step:1/1667 avg loss:-13.330, visual_loss:-13.347, av_loss:-13.326
step:1/1667 avg loss:-13.572, visual_loss:-13.548, av_loss:-13.578
step:1001/1667 avg loss:-12.875, visual_loss:-12.836, av_loss:-12.885
step:1001/1667 avg loss:-12.855, visual_loss:-12.812, av_loss:-12.865
Train Summary | End of Epoch 72 | Time 2478.10s | Current time 2024-12-29 08:53:23.071753 |Train Loss -12.808| 
step:1/417 avg loss:-11.796, visual_loss:-11.778, av_loss:-11.801
step:1/417 avg loss:-9.131, visual_loss:-8.200, av_loss:-9.364
step:301/417 avg loss:-10.494, visual_loss:-10.799, av_loss:-10.418
step:301/417 avg loss:-10.267, visual_loss:-10.687, av_loss:-10.162
Valid Summary | End of Epoch 72 | Time 433.35s | Current time 2024-12-29 09:00:36.516835 |Valid Loss -10.468| 
step:1/1667 avg loss:-11.336, visual_loss:-11.328, av_loss:-11.338
step:1/1667 avg loss:-11.148, visual_loss:-11.108, av_loss:-11.158
step:1001/1667 avg loss:-12.885, visual_loss:-12.841, av_loss:-12.895step:1001/1667 avg loss:-12.875, visual_loss:-12.826, av_loss:-12.887

Train Summary | End of Epoch 73 | Time 2561.56s | Current time 2024-12-29 09:43:19.533441 |Train Loss -12.780| 
step:1/417 avg loss:-11.953, visual_loss:-12.031, av_loss:-11.933
step:1/417 avg loss:-10.975, visual_loss:-10.646, av_loss:-11.058
step:301/417 avg loss:-10.665, visual_loss:-10.990, av_loss:-10.584
step:301/417 avg loss:-10.489, visual_loss:-10.862, av_loss:-10.396
Valid Summary | End of Epoch 73 | Time 378.43s | Current time 2024-12-29 09:49:38.030776 |Valid Loss -10.652| 
step:1/1667 avg loss:-12.945, visual_loss:-12.910, av_loss:-12.954
step:1/1667 avg loss:-14.053, visual_loss:-13.879, av_loss:-14.096
step:1001/1667 avg loss:-12.827, visual_loss:-12.781, av_loss:-12.839step:1001/1667 avg loss:-12.759, visual_loss:-12.697, av_loss:-12.774

Train Summary | End of Epoch 74 | Time 2547.68s | Current time 2024-12-29 10:32:06.532913 |Train Loss -12.751| 
step:1/417 avg loss:-11.878, visual_loss:-11.841, av_loss:-11.887
step:1/417 avg loss:-9.553, visual_loss:-9.799, av_loss:-9.492
step:301/417 avg loss:-10.867, visual_loss:-11.164, av_loss:-10.793
step:301/417 avg loss:-10.727, visual_loss:-11.076, av_loss:-10.640
Valid Summary | End of Epoch 74 | Time 430.82s | Current time 2024-12-29 10:39:17.419677 |Valid Loss -10.869| 
step:1/1667 avg loss:-12.459, visual_loss:-12.428, av_loss:-12.466
step:1/1667 avg loss:-14.057, visual_loss:-13.993, av_loss:-14.073
step:1001/1667 avg loss:-13.076, visual_loss:-13.044, av_loss:-13.084step:1001/1667 avg loss:-13.019, visual_loss:-12.989, av_loss:-13.026

Train Summary | End of Epoch 75 | Time 2611.34s | Current time 2024-12-29 11:22:50.310067 |Train Loss -13.003| 
step:1/417 avg loss:-11.873, visual_loss:-11.788, av_loss:-11.895
step:1/417 avg loss:-6.723, visual_loss:-8.365, av_loss:-6.313
step:301/417 avg loss:-10.871, visual_loss:-11.186, av_loss:-10.793
step:301/417 avg loss:-10.825, visual_loss:-11.150, av_loss:-10.744
Valid Summary | End of Epoch 75 | Time 390.58s | Current time 2024-12-29 11:29:21.258142 |Valid Loss -10.916| 
step:1/1667 avg loss:-11.576, visual_loss:-11.525, av_loss:-11.589
step:1/1667 avg loss:-12.579, visual_loss:-12.548, av_loss:-12.587
step:1001/1667 avg loss:-12.842, visual_loss:-12.799, av_loss:-12.852step:1001/1667 avg loss:-12.831, visual_loss:-12.780, av_loss:-12.844

Train Summary | End of Epoch 76 | Time 2577.42s | Current time 2024-12-29 12:12:19.238641 |Train Loss -12.713| 
step:1/417 avg loss:-12.372, visual_loss:-12.269, av_loss:-12.398
step:1/417 avg loss:-5.757, visual_loss:-8.074, av_loss:-5.178
step:301/417 avg loss:-10.966, visual_loss:-11.156, av_loss:-10.918
step:301/417 avg loss:-10.791, visual_loss:-11.063, av_loss:-10.723
Valid Summary | End of Epoch 76 | Time 393.89s | Current time 2024-12-29 12:18:53.194224 |Valid Loss -10.941| 
step:1/1667 avg loss:-14.289, visual_loss:-14.299, av_loss:-14.287
step:1/1667 avg loss:-11.112, visual_loss:-11.108, av_loss:-11.113
step:1001/1667 avg loss:-12.916, visual_loss:-12.873, av_loss:-12.926step:1001/1667 avg loss:-12.882, visual_loss:-12.836, av_loss:-12.894

Train Summary | End of Epoch 77 | Time 2596.94s | Current time 2024-12-29 13:02:10.700324 |Train Loss -12.741| 
step:1/417 avg loss:-11.957, visual_loss:-11.914, av_loss:-11.967
step:1/417 avg loss:-7.693, visual_loss:-7.924, av_loss:-7.635
step:301/417 avg loss:-10.571, visual_loss:-10.904, av_loss:-10.487
step:301/417 avg loss:-10.546, visual_loss:-10.826, av_loss:-10.476
Valid Summary | End of Epoch 77 | Time 374.67s | Current time 2024-12-29 13:08:25.513900 |Valid Loss -10.658| 
Learning rate adjusted to: 0.000500
step:1/1667 avg loss:-12.864, visual_loss:-12.832, av_loss:-12.872
step:1/1667 avg loss:-13.480, visual_loss:-13.482, av_loss:-13.479
step:1001/1667 avg loss:-13.087, visual_loss:-13.047, av_loss:-13.097
step:1001/1667 avg loss:-13.063, visual_loss:-13.029, av_loss:-13.072
Train Summary | End of Epoch 78 | Time 2529.65s | Current time 2024-12-29 13:50:36.347569 |Train Loss -13.140| 
step:1/417 avg loss:-12.375, visual_loss:-12.280, av_loss:-12.398
step:1/417 avg loss:-8.388, visual_loss:-8.392, av_loss:-8.387
step:301/417 avg loss:-11.051, visual_loss:-11.415, av_loss:-10.960
step:301/417 avg loss:-11.071, visual_loss:-11.331, av_loss:-11.006
Valid Summary | End of Epoch 78 | Time 386.30s | Current time 2024-12-29 13:57:02.837284 |Valid Loss -11.142| 
Found new best model, dict saved
step:1/1667 avg loss:-11.640, visual_loss:-11.630, av_loss:-11.642
step:1/1667 avg loss:-14.142, visual_loss:-14.126, av_loss:-14.146
step:1001/1667 avg loss:-13.378, visual_loss:-13.354, av_loss:-13.384
step:1001/1667 avg loss:-13.372, visual_loss:-13.349, av_loss:-13.378
Train Summary | End of Epoch 79 | Time 2534.62s | Current time 2024-12-29 14:39:19.982228 |Train Loss -13.349| 
step:1/417 avg loss:-12.463, visual_loss:-12.489, av_loss:-12.456
step:1/417 avg loss:-8.324, visual_loss:-8.226, av_loss:-8.349
step:301/417 avg loss:-11.083, visual_loss:-11.421, av_loss:-10.999
step:301/417 avg loss:-10.983, visual_loss:-11.312, av_loss:-10.901
Valid Summary | End of Epoch 79 | Time 380.87s | Current time 2024-12-29 14:45:41.440202 |Valid Loss -11.132| 
step:1/1667 avg loss:-13.084, visual_loss:-13.047, av_loss:-13.093
step:1/1667 avg loss:-12.617, visual_loss:-12.550, av_loss:-12.634
step:1001/1667 avg loss:-13.420, visual_loss:-13.388, av_loss:-13.428
step:1001/1667 avg loss:-13.381, visual_loss:-13.356, av_loss:-13.387
Train Summary | End of Epoch 80 | Time 2496.82s | Current time 2024-12-29 15:27:19.306371 |Train Loss -13.405| 
step:1/417 avg loss:-12.442, visual_loss:-12.413, av_loss:-12.449
step:1/417 avg loss:-8.140, visual_loss:-8.136, av_loss:-8.141
step:301/417 avg loss:-10.991, visual_loss:-11.428, av_loss:-10.881
step:301/417 avg loss:-10.961, visual_loss:-11.291, av_loss:-10.878
Valid Summary | End of Epoch 80 | Time 373.57s | Current time 2024-12-29 15:33:32.936358 |Valid Loss -11.074| 
step:1/1667 avg loss:-12.877, visual_loss:-12.857, av_loss:-12.882
step:1/1667 avg loss:-12.771, visual_loss:-12.739, av_loss:-12.779
step:1001/1667 avg loss:-13.433, visual_loss:-13.401, av_loss:-13.441
step:1001/1667 avg loss:-13.460, visual_loss:-13.424, av_loss:-13.468
Train Summary | End of Epoch 81 | Time 2560.89s | Current time 2024-12-29 16:16:15.368084 |Train Loss -13.458| 
step:1/417 avg loss:-12.349, visual_loss:-12.193, av_loss:-12.388
step:1/417 avg loss:-8.193, visual_loss:-8.000, av_loss:-8.241
step:301/417 avg loss:-11.268, visual_loss:-11.551, av_loss:-11.197
step:301/417 avg loss:-11.095, visual_loss:-11.358, av_loss:-11.029
Valid Summary | End of Epoch 81 | Time 377.61s | Current time 2024-12-29 16:22:32.994774 |Valid Loss -11.253| 
Found new best model, dict saved
step:1/1667 avg loss:-14.791, visual_loss:-14.753, av_loss:-14.800
step:1/1667 avg loss:-13.481, visual_loss:-13.451, av_loss:-13.488
step:1001/1667 avg loss:-13.519, visual_loss:-13.485, av_loss:-13.528step:1001/1667 avg loss:-13.484, visual_loss:-13.451, av_loss:-13.492

Train Summary | End of Epoch 82 | Time 2589.63s | Current time 2024-12-29 17:05:43.288581 |Train Loss -13.471| 
step:1/417 avg loss:-12.400, visual_loss:-12.277, av_loss:-12.431
step:1/417 avg loss:-8.239, visual_loss:-8.122, av_loss:-8.269
step:301/417 avg loss:-11.140, visual_loss:-11.471, av_loss:-11.057
step:301/417 avg loss:-11.061, visual_loss:-11.409, av_loss:-10.974
Valid Summary | End of Epoch 82 | Time 398.48s | Current time 2024-12-29 17:12:21.836530 |Valid Loss -11.185| 
step:1/1667 avg loss:-13.744, visual_loss:-13.713, av_loss:-13.752
step:1/1667 avg loss:-13.798, visual_loss:-13.780, av_loss:-13.802
step:1001/1667 avg loss:-13.629, visual_loss:-13.605, av_loss:-13.635step:1001/1667 avg loss:-13.593, visual_loss:-13.571, av_loss:-13.599

Train Summary | End of Epoch 83 | Time 2564.52s | Current time 2024-12-29 17:55:07.392711 |Train Loss -13.568| 
step:1/417 avg loss:-12.395, visual_loss:-12.351, av_loss:-12.406
step:1/417 avg loss:-8.241, visual_loss:-8.343, av_loss:-8.215
step:301/417 avg loss:-11.161, visual_loss:-11.491, av_loss:-11.079
step:301/417 avg loss:-11.102, visual_loss:-11.411, av_loss:-11.024
Valid Summary | End of Epoch 83 | Time 415.68s | Current time 2024-12-29 18:02:03.648002 |Valid Loss -11.203| 
step:1/1667 avg loss:-13.087, visual_loss:-13.077, av_loss:-13.090
step:1/1667 avg loss:-14.514, visual_loss:-14.487, av_loss:-14.520
step:1001/1667 avg loss:-13.541, visual_loss:-13.517, av_loss:-13.547
step:1001/1667 avg loss:-13.538, visual_loss:-13.510, av_loss:-13.545
Train Summary | End of Epoch 84 | Time 2509.85s | Current time 2024-12-29 18:43:54.630013 |Train Loss -13.546| 
step:1/417 avg loss:-12.401, visual_loss:-12.287, av_loss:-12.430
step:1/417 avg loss:-8.067, visual_loss:-8.055, av_loss:-8.070
step:301/417 avg loss:-11.157, visual_loss:-11.471, av_loss:-11.078
step:301/417 avg loss:-11.079, visual_loss:-11.407, av_loss:-10.997
Valid Summary | End of Epoch 84 | Time 391.32s | Current time 2024-12-29 18:50:26.221819 |Valid Loss -11.206| 
step:1/1667 avg loss:-13.884, visual_loss:-13.854, av_loss:-13.891
step:1/1667 avg loss:-13.544, visual_loss:-13.538, av_loss:-13.546
step:1001/1667 avg loss:-13.660, visual_loss:-13.636, av_loss:-13.666step:1001/1667 avg loss:-13.670, visual_loss:-13.644, av_loss:-13.677

Train Summary | End of Epoch 85 | Time 2531.08s | Current time 2024-12-29 19:32:40.247473 |Train Loss -13.633| 
step:1/417 avg loss:-12.327, visual_loss:-12.214, av_loss:-12.355
step:1/417 avg loss:-7.102, visual_loss:-4.578, av_loss:-7.733
step:301/417 avg loss:-10.968, visual_loss:-11.291, av_loss:-10.887
step:301/417 avg loss:-11.019, visual_loss:-11.331, av_loss:-10.941
Valid Summary | End of Epoch 85 | Time 390.26s | Current time 2024-12-29 19:39:10.625402 |Valid Loss -11.072| 
step:1/1667 avg loss:-13.799, visual_loss:-13.789, av_loss:-13.802
step:1/1667 avg loss:-13.381, visual_loss:-13.363, av_loss:-13.386
step:1001/1667 avg loss:-13.673, visual_loss:-13.643, av_loss:-13.680step:1001/1667 avg loss:-13.655, visual_loss:-13.631, av_loss:-13.661

Train Summary | End of Epoch 86 | Time 2599.46s | Current time 2024-12-29 20:22:30.820363 |Train Loss -13.674| 
step:1/417 avg loss:-12.354, visual_loss:-12.325, av_loss:-12.362
step:1/417 avg loss:-8.142, visual_loss:-8.001, av_loss:-8.178
step:301/417 avg loss:-11.159, visual_loss:-11.519, av_loss:-11.069
step:301/417 avg loss:-11.129, visual_loss:-11.409, av_loss:-11.060
Valid Summary | End of Epoch 86 | Time 393.40s | Current time 2024-12-29 20:29:04.344881 |Valid Loss -11.222| 
step:1/1667 avg loss:-12.659, visual_loss:-12.633, av_loss:-12.665
step:1/1667 avg loss:-13.896, visual_loss:-13.867, av_loss:-13.904
step:1001/1667 avg loss:-13.779, visual_loss:-13.761, av_loss:-13.784step:1001/1667 avg loss:-13.729, visual_loss:-13.707, av_loss:-13.734

Train Summary | End of Epoch 87 | Time 2534.98s | Current time 2024-12-29 21:11:19.965410 |Train Loss -13.736| 
step:1/417 avg loss:-12.521, visual_loss:-12.476, av_loss:-12.532
step:1/417 avg loss:-8.176, visual_loss:-8.041, av_loss:-8.210
step:301/417 avg loss:-11.208, visual_loss:-11.516, av_loss:-11.131
step:301/417 avg loss:-11.174, visual_loss:-11.474, av_loss:-11.099
Valid Summary | End of Epoch 87 | Time 389.43s | Current time 2024-12-29 21:17:49.450819 |Valid Loss -11.278| 
Found new best model, dict saved
step:1/1667 avg loss:-14.698, visual_loss:-14.703, av_loss:-14.697
step:1/1667 avg loss:-12.887, visual_loss:-12.861, av_loss:-12.893
step:1001/1667 avg loss:-13.789, visual_loss:-13.771, av_loss:-13.794step:1001/1667 avg loss:-13.793, visual_loss:-13.773, av_loss:-13.798

Train Summary | End of Epoch 88 | Time 2507.60s | Current time 2024-12-29 21:59:39.584928 |Train Loss -13.735| 
step:1/417 avg loss:-12.079, visual_loss:-11.986, av_loss:-12.102
step:1/417 avg loss:-7.935, visual_loss:-7.970, av_loss:-7.926
step:301/417 avg loss:-10.506, visual_loss:-10.945, av_loss:-10.396
step:301/417 avg loss:-10.512, visual_loss:-10.846, av_loss:-10.428
Valid Summary | End of Epoch 88 | Time 391.95s | Current time 2024-12-29 22:06:11.564164 |Valid Loss -10.595| 
step:1/1667 avg loss:-13.536, visual_loss:-13.490, av_loss:-13.548
step:1/1667 avg loss:-11.221, visual_loss:-11.205, av_loss:-11.225
step:1001/1667 avg loss:-13.701, visual_loss:-13.675, av_loss:-13.708step:1001/1667 avg loss:-13.689, visual_loss:-13.667, av_loss:-13.695

Train Summary | End of Epoch 89 | Time 2484.02s | Current time 2024-12-29 22:47:36.642554 |Train Loss -13.735| 
step:1/417 avg loss:-12.447, visual_loss:-12.394, av_loss:-12.460
step:1/417 avg loss:-8.105, visual_loss:-8.052, av_loss:-8.118
step:301/417 avg loss:-11.237, visual_loss:-11.571, av_loss:-11.154
step:301/417 avg loss:-11.132, visual_loss:-11.471, av_loss:-11.047
Valid Summary | End of Epoch 89 | Time 376.54s | Current time 2024-12-29 22:53:53.223467 |Valid Loss -11.275| 
step:1/1667 avg loss:-13.697, visual_loss:-13.685, av_loss:-13.701
step:1/1667 avg loss:-11.838, visual_loss:-11.826, av_loss:-11.840
step:1001/1667 avg loss:-13.687, visual_loss:-13.657, av_loss:-13.694step:1001/1667 avg loss:-13.678, visual_loss:-13.644, av_loss:-13.687

Train Summary | End of Epoch 90 | Time 2519.93s | Current time 2024-12-29 23:35:53.585231 |Train Loss -13.692| 
step:1/417 avg loss:-12.267, visual_loss:-12.220, av_loss:-12.278
step:1/417 avg loss:-8.200, visual_loss:-8.042, av_loss:-8.240
step:301/417 avg loss:-11.207, visual_loss:-11.591, av_loss:-11.111
step:301/417 avg loss:-11.096, visual_loss:-11.427, av_loss:-11.014
Valid Summary | End of Epoch 90 | Time 409.67s | Current time 2024-12-29 23:42:43.357859 |Valid Loss -11.213| 
step:1/1667 avg loss:-13.239, visual_loss:-13.217, av_loss:-13.244
step:1/1667 avg loss:-14.213, visual_loss:-14.189, av_loss:-14.219
step:1001/1667 avg loss:-13.832, visual_loss:-13.809, av_loss:-13.838
step:1001/1667 avg loss:-13.815, visual_loss:-13.792, av_loss:-13.821
Train Summary | End of Epoch 91 | Time 2564.91s | Current time 2024-12-30 00:25:29.525927 |Train Loss -13.837| 
step:1/417 avg loss:-12.284, visual_loss:-12.178, av_loss:-12.311
step:1/417 avg loss:-8.051, visual_loss:-8.014, av_loss:-8.060
step:301/417 avg loss:-11.239, visual_loss:-11.553, av_loss:-11.161
step:301/417 avg loss:-11.130, visual_loss:-11.392, av_loss:-11.065
Valid Summary | End of Epoch 91 | Time 399.93s | Current time 2024-12-30 00:32:09.488239 |Valid Loss -11.250| 
step:1/1667 avg loss:-14.747, visual_loss:-14.732, av_loss:-14.751
step:1/1667 avg loss:-14.024, visual_loss:-14.007, av_loss:-14.028
step:1001/1667 avg loss:-13.922, visual_loss:-13.904, av_loss:-13.926
step:1001/1667 avg loss:-13.924, visual_loss:-13.904, av_loss:-13.929
Train Summary | End of Epoch 92 | Time 2554.43s | Current time 2024-12-30 01:14:44.488696 |Train Loss -13.905| 
step:1/417 avg loss:-12.418, visual_loss:-12.374, av_loss:-12.430
step:1/417 avg loss:-7.985, visual_loss:-8.083, av_loss:-7.960
step:301/417 avg loss:-11.256, visual_loss:-11.551, av_loss:-11.182
step:301/417 avg loss:-11.184, visual_loss:-11.480, av_loss:-11.110
Valid Summary | End of Epoch 92 | Time 436.89s | Current time 2024-12-30 01:22:01.384455 |Valid Loss -11.283| 
Found new best model, dict saved
step:1/1667 avg loss:-13.824, visual_loss:-13.799, av_loss:-13.830
step:1/1667 avg loss:-13.530, visual_loss:-13.517, av_loss:-13.534
step:1001/1667 avg loss:-13.958, visual_loss:-13.941, av_loss:-13.963step:1001/1667 avg loss:-13.903, visual_loss:-13.882, av_loss:-13.908

Train Summary | End of Epoch 93 | Time 2561.24s | Current time 2024-12-30 02:04:43.505499 |Train Loss -13.928| 
step:1/417 avg loss:-12.240, visual_loss:-12.264, av_loss:-12.234
step:1/417 avg loss:-8.364, visual_loss:-8.602, av_loss:-8.304
step:301/417 avg loss:-11.241, visual_loss:-11.575, av_loss:-11.158
step:301/417 avg loss:-11.212, visual_loss:-11.473, av_loss:-11.147
Valid Summary | End of Epoch 93 | Time 410.34s | Current time 2024-12-30 02:11:33.882934 |Valid Loss -11.308| 
Found new best model, dict saved
step:1/1667 avg loss:-14.304, visual_loss:-14.284, av_loss:-14.309
step:1/1667 avg loss:-14.538, visual_loss:-14.521, av_loss:-14.542
step:1001/1667 avg loss:-13.888, visual_loss:-13.867, av_loss:-13.893
step:1001/1667 avg loss:-13.908, visual_loss:-13.887, av_loss:-13.913
Train Summary | End of Epoch 94 | Time 2588.58s | Current time 2024-12-30 02:54:43.401490 |Train Loss -13.900| 
step:1/417 avg loss:-12.331, visual_loss:-12.277, av_loss:-12.344
step:1/417 avg loss:-8.340, visual_loss:-8.291, av_loss:-8.352
step:301/417 avg loss:-11.167, visual_loss:-11.503, av_loss:-11.083
step:301/417 avg loss:-11.169, visual_loss:-11.491, av_loss:-11.088
Valid Summary | End of Epoch 94 | Time 390.30s | Current time 2024-12-30 03:01:13.715203 |Valid Loss -11.250| 
step:1/1667 avg loss:-12.853, visual_loss:-12.847, av_loss:-12.854
step:1/1667 avg loss:-14.008, visual_loss:-13.978, av_loss:-14.015
step:1001/1667 avg loss:-14.032, visual_loss:-14.014, av_loss:-14.036
step:1001/1667 avg loss:-13.999, visual_loss:-13.981, av_loss:-14.004
Train Summary | End of Epoch 95 | Time 2624.11s | Current time 2024-12-30 03:44:58.148666 |Train Loss -14.001| 
step:1/417 avg loss:-12.505, visual_loss:-12.488, av_loss:-12.510
step:1/417 avg loss:-8.041, visual_loss:-7.967, av_loss:-8.060
step:301/417 avg loss:-11.194, visual_loss:-11.540, av_loss:-11.108
step:301/417 avg loss:-11.105, visual_loss:-11.522, av_loss:-11.001
Valid Summary | End of Epoch 95 | Time 393.34s | Current time 2024-12-30 03:51:31.488479 |Valid Loss -11.230| 
step:1/1667 avg loss:-13.318, visual_loss:-13.315, av_loss:-13.319
step:1/1667 avg loss:-13.939, visual_loss:-13.931, av_loss:-13.942
step:1001/1667 avg loss:-13.969, visual_loss:-13.943, av_loss:-13.975
step:1001/1667 avg loss:-13.960, visual_loss:-13.940, av_loss:-13.966
Train Summary | End of Epoch 96 | Time 2593.97s | Current time 2024-12-30 04:34:54.037136 |Train Loss -13.987| 
step:1/417 avg loss:-12.400, visual_loss:-12.421, av_loss:-12.395
step:1/417 avg loss:-8.178, visual_loss:-8.039, av_loss:-8.213
step:301/417 avg loss:-11.217, visual_loss:-11.540, av_loss:-11.136
step:301/417 avg loss:-11.160, visual_loss:-11.495, av_loss:-11.076
Valid Summary | End of Epoch 96 | Time 383.84s | Current time 2024-12-30 04:41:17.885545 |Valid Loss -11.265| 
step:1/1667 avg loss:-12.922, visual_loss:-12.909, av_loss:-12.926
step:1/1667 avg loss:-15.095, visual_loss:-15.058, av_loss:-15.104
step:1001/1667 avg loss:-13.875, visual_loss:-13.843, av_loss:-13.883
step:1001/1667 avg loss:-13.880, visual_loss:-13.847, av_loss:-13.889
Train Summary | End of Epoch 97 | Time 2572.10s | Current time 2024-12-30 05:24:10.406798 |Train Loss -13.926| 
step:1/417 avg loss:-11.838, visual_loss:-11.886, av_loss:-11.827
step:1/417 avg loss:-5.631, visual_loss:-7.729, av_loss:-5.106
step:301/417 avg loss:-11.009, visual_loss:-11.375, av_loss:-10.917
step:301/417 avg loss:-10.996, visual_loss:-11.285, av_loss:-10.924
Valid Summary | End of Epoch 97 | Time 376.39s | Current time 2024-12-30 05:30:26.799671 |Valid Loss -11.054| 
step:1/1667 avg loss:-13.872, visual_loss:-13.819, av_loss:-13.885
step:1/1667 avg loss:-14.555, visual_loss:-14.553, av_loss:-14.556
step:1001/1667 avg loss:-14.102, visual_loss:-14.084, av_loss:-14.107
step:1001/1667 avg loss:-14.069, visual_loss:-14.052, av_loss:-14.074
Train Summary | End of Epoch 98 | Time 2610.29s | Current time 2024-12-30 06:13:57.372847 |Train Loss -14.089| 
step:1/417 avg loss:-12.371, visual_loss:-12.291, av_loss:-12.390
step:1/417 avg loss:-5.363, visual_loss:-8.105, av_loss:-4.677
step:301/417 avg loss:-11.220, visual_loss:-11.529, av_loss:-11.143
step:301/417 avg loss:-11.088, visual_loss:-11.444, av_loss:-10.999
Valid Summary | End of Epoch 98 | Time 406.97s | Current time 2024-12-30 06:20:44.384356 |Valid Loss -11.221| 
step:1/1667 avg loss:-14.217, visual_loss:-14.191, av_loss:-14.223
step:1/1667 avg loss:-14.335, visual_loss:-14.306, av_loss:-14.342
step:1001/1667 avg loss:-14.064, visual_loss:-14.037, av_loss:-14.071
step:1001/1667 avg loss:-14.009, visual_loss:-13.988, av_loss:-14.015
Train Summary | End of Epoch 99 | Time 2624.66s | Current time 2024-12-30 07:04:29.398893 |Train Loss -14.058| 
step:1/417 avg loss:-12.433, visual_loss:-12.464, av_loss:-12.425
step:1/417 avg loss:-7.911, visual_loss:-7.837, av_loss:-7.930
step:301/417 avg loss:-11.203, visual_loss:-11.551, av_loss:-11.116
step:301/417 avg loss:-11.205, visual_loss:-11.518, av_loss:-11.127
Valid Summary | End of Epoch 99 | Time 396.96s | Current time 2024-12-30 07:11:06.373669 |Valid Loss -11.263| 
Learning rate adjusted to: 0.000250
step:1/1667 avg loss:-13.802, visual_loss:-13.788, av_loss:-13.806
step:1/1667 avg loss:-14.036, visual_loss:-14.010, av_loss:-14.042
step:1001/1667 avg loss:-14.256, visual_loss:-14.240, av_loss:-14.260step:1001/1667 avg loss:-14.225, visual_loss:-14.208, av_loss:-14.230

Train Summary | End of Epoch 100 | Time 2554.67s | Current time 2024-12-30 07:53:41.620862 |Train Loss -14.230| 
step:1/417 avg loss:-12.400, visual_loss:-12.412, av_loss:-12.397
step:1/417 avg loss:-8.026, visual_loss:-7.989, av_loss:-8.036
step:301/417 avg loss:-11.220, visual_loss:-11.587, av_loss:-11.128
step:301/417 avg loss:-11.173, visual_loss:-11.514, av_loss:-11.088
Valid Summary | End of Epoch 100 | Time 401.93s | Current time 2024-12-30 08:00:23.558684 |Valid Loss -11.272| 
